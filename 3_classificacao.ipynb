{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres functions loaded...\n"
     ]
    }
   ],
   "source": [
    "import ipy_table\n",
    "from collections import Counter\n",
    "%run ../aux/postgres.py\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#%run functions/preproc.py\n",
    "#%run functions/classificacao.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gat\n",
      "gat\n",
      "gat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "stemmer = PortugueseStemmer()\n",
    "stemmer.stem(\"dogs\")\n",
    "\n",
    "print(stemmer.stem('gatos'))\n",
    "print(stemmer.stem('gata'))\n",
    "print(stemmer.stem('gato'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('service', 420)\n",
      "('clinical', 172)\n",
      "('personal', 138)\n",
      "('product', 107)\n",
      "('information', 94)\n",
      "('sale', 66)\n",
      "997\n",
      "997\n"
     ]
    }
   ],
   "source": [
    "cmd = (\"SELECT t.text, \"\n",
    "       \"CASE \"\n",
    "       \"WHEN cl.codclassificacao = 230 THEN 226 \"\n",
    "       \"ELSE cl.codclassificacao END, \"\n",
    "       \"CASE \"\n",
    "       \"WHEN cl.codclassificacao = 230 THEN 'service' \"\n",
    "       \"ELSE cl.descricao END \"\n",
    "       \"FROM analisetweet at \"\n",
    "       \"INNER JOIN classificacao cl ON cl.codclassificacao = at.codclassificacao \"\n",
    "       \"INNER JOIN tweet t on at.codtweet = t.codtweet \"\n",
    "       \"WHERE codanaliseamostra = 126 AND at.codclassificacao <> 36 order by random()\")\n",
    "\n",
    "res01 = query(cmd)\n",
    "topics = [x[2] for x in res01]\n",
    "count01 = Counter(topics)\n",
    "\n",
    "tot = 0\n",
    "for x in count01.most_common():\n",
    "    print(x)\n",
    "    tot = tot + x[1]\n",
    "print(tot)\n",
    "\n",
    "tweets = [x[0] for x in res01]\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 226, 229, 226, 224, 224, 226, 229, 226, 223]\n",
      "['Dental Implant machine w motor w reduction 16:1 push contra angle handpiece SET https://t.co/afHj5VEuga https://t.co/ZJOWzvs3dl', 'RT https://t.co/rvhOgYkKw2 Dental Implants are More Efficient than Traditional Dentures and Bridges. Explore details on https://t.co/PXAAhAgdDc #teeth #hornsbydentist #dentisthornsby #Oral‚Ä¶ https://t.co/4GAfBK8nTi', 'Losing #teeth creates the potential for your oral #health to decline quickly‚Ä¶\\n\\nConsider a #dental implants: https://t.co/FCtOAjdmJP\\n\\n#tcmi https://t.co/vUapu9seeq', \"We're on Pinterest!! \\n\\nDid you ever wonder what a dental implant looks like? ...or how Inv... https://t.co/LBqQ6fKLYA https://t.co/9Y8CQpQ538\", 'Preferred Dental Technologies is the First Dental Implant Company to 3D Print Solid Custom ... https://t.co/Um9otgSCSS', 'RT @lintenaju7: Know More about Dental Implants and its Positive Features ‚Äì Medaka Doctor https://t.co/hQhYkBsIQs', 'RT @olivesanders15: Orange County Dentist  provides patients with quality general and cosmetic dentistry such as dental implants, Invisalig‚Ä¶', 'How to Enhance Your Best Smile With Dental Implants,see this post on Cnipo https://t.co/LmcQMV1xLr', \"Tooth loss can happen at any age! Don't let your smile hold you back, ask us about dental implants! https://t.co/1VuY6HTUTt\", 'Dental Implant Market Is Projected To Witness Significant Growth By 2021 | Radiant Insights,Inc https://t.co/LdrY0VCVqB https://t.co/eyiMLaDGtR']\n",
      "997\n",
      "997\n"
     ]
    }
   ],
   "source": [
    "topicsID = [x[1] for x in res01]\n",
    "tweets = [x[0] for x in res01]\n",
    "\n",
    "print(topicsID[:10])\n",
    "\n",
    "print(tweets[:10])\n",
    "print(len(topicsID))\n",
    "print(len(tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr√© Proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "happyFace = ['üòÖ', 'üòÅ', 'üòç', '‚ù§', 'üòÇ', 'üòã', 'üòå', '‚ò∫', 'üòò']\n",
    "sadFace = ['üò™', 'üò≠', 'üò©']\n",
    "neutralFace = ['üòê', 'üòí']\n",
    "scaryFace = ['üò±']\n",
    "\n",
    "def proc_emoji(tweet):\n",
    "    for em in happyFace:\n",
    "        tweet = tweet.replace(em, ' happyFace ')\n",
    "\n",
    "    for em in neutralFace:\n",
    "        tweet = tweet.replace(em, ' neutralFace ')\n",
    "\n",
    "    for em in sadFace:\n",
    "        tweet = tweet.replace(em, ' sadFace ')\n",
    "\n",
    "    for em in scaryFace:\n",
    "        tweet = tweet.replace(em, ' scaryFace ')\n",
    "    return tweet\n",
    "    \n",
    "    \n",
    "def pre_proc(doc_complete):\n",
    "    doc_complete = [ t.lower() for t in doc_complete ]\n",
    "\n",
    "    # Interroga√ß√µes\n",
    "    doc_complete = [re.sub(\"[$]\", \" moneymark \", x) for x in doc_complete ]\n",
    "\n",
    "    # mention removal\n",
    "    doc_complete = [re.sub(\"@\\\\w+\", \" mentionname \", x) for x in doc_complete ]\n",
    "    # Remove URL\n",
    "    doc_complete = [re.sub(\"(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \" urlurl \", x) for x in doc_complete ]\n",
    "    # Remove HTML symbols\n",
    "    doc_complete = [re.sub(\"&[^\\\\s]*;\", \" \", x) for x in doc_complete ]\n",
    "    # Remove numbers\n",
    "    doc_complete = [re.sub(\"[0-9]+\", \" numbers \", x) for x in doc_complete ]\n",
    "    # Interroga√ß√µes\n",
    "    doc_complete = [re.sub(\"[?]\", \" questionmark \", x) for x in doc_complete ]\n",
    "    # Remove Punctuation    \n",
    "    doc_complete = [re.sub(\"[.,\\\\/#!?+$‚Äì\\\"|%-\\\\^&\\\\*;:{}=\\\\-_`~()]\", \" \", x) for x in doc_complete ]\n",
    "    # Remove Emojis\n",
    "    doc_complete = [proc_emoji(x) for x in doc_complete ]\n",
    "    \n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    doc_complete = [pattern.sub(' ', x) for x in doc_complete ]\n",
    "    return doc_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tw: [Dental Implant machine w motor w reduction 16:1 push contra angle handpiece SET https://t.co/afHj5VEuga https://t.co/ZJOWzvs3dl]\n",
      "pp: [dental implant machine w motor w reduction numbers numbers push contra angle handpiece set urlurl ]\n",
      "\n",
      "tw: [RT https://t.co/rvhOgYkKw2 Dental Implants are More Efficient than Traditional Dentures and Bridges. Explore details on https://t.co/PXAAhAgdDc #teeth #hornsbydentist #dentisthornsby #Oral‚Ä¶ https://t.co/4GAfBK8nTi]\n",
      "pp: [rt urlurl ]\n",
      "\n",
      "tw: [Losing #teeth creates the potential for your oral #health to decline quickly‚Ä¶\n",
      "\n",
      "Consider a #dental implants: https://t.co/FCtOAjdmJP\n",
      "\n",
      "#tcmi https://t.co/vUapu9seeq]\n",
      "pp: [losing teeth creates the potential for your oral health to decline quickly consider a dental implants urlurl tcmi urlurl ]\n",
      "\n",
      "tw: [We're on Pinterest!! \n",
      "\n",
      "Did you ever wonder what a dental implant looks like? ...or how Inv... https://t.co/LBqQ6fKLYA https://t.co/9Y8CQpQ538]\n",
      "pp: [we re on pinterest did you ever wonder what a dental implant looks like questionmark or how inv urlurl ]\n",
      "\n",
      "tw: [Preferred Dental Technologies is the First Dental Implant Company to 3D Print Solid Custom ... https://t.co/Um9otgSCSS]\n",
      "pp: [preferred dental technologies is the first dental implant company to numbers d print solid custom urlurl ]\n",
      "\n",
      "tw: [RT @lintenaju7: Know More about Dental Implants and its Positive Features ‚Äì Medaka Doctor https://t.co/hQhYkBsIQs]\n",
      "pp: [rt mentionname know more about dental implants and its positive features medaka doctor urlurl ]\n",
      "\n",
      "tw: [RT @olivesanders15: Orange County Dentist  provides patients with quality general and cosmetic dentistry such as dental implants, Invisalig‚Ä¶]\n",
      "pp: [rt mentionname orange county dentist provides patients with quality general and cosmetic dentistry such as dental implants invisalig ]\n",
      "\n",
      "tw: [How to Enhance Your Best Smile With Dental Implants,see this post on Cnipo https://t.co/LmcQMV1xLr]\n",
      "pp: [how to enhance your best smile with dental implants see this post on cnipo urlurl ]\n",
      "\n",
      "tw: [Tooth loss can happen at any age! Don't let your smile hold you back, ask us about dental implants! https://t.co/1VuY6HTUTt]\n",
      "pp: [tooth loss can happen at any age don t let your smile hold you back ask us about dental implants urlurl ]\n",
      "\n",
      "tw: [Dental Implant Market Is Projected To Witness Significant Growth By 2021 | Radiant Insights,Inc https://t.co/LdrY0VCVqB https://t.co/eyiMLaDGtR]\n",
      "pp: [dental implant market is projected to witness significant growth by numbers radiant insights inc urlurl ]\n",
      "\n",
      "tw: [Reviewed Dental implants costs in Mohali by Dentia Speciality on LibraryThing https://t.co/w6oQI0oSD5]\n",
      "pp: [reviewed dental implants costs in mohali by dentia speciality on librarything urlurl ]\n",
      "\n",
      "tw: [Top Oral Health Advice To Help Keep Your Teeth Healthy Dental Implants In¬†Mexico https://t.co/BNlsmME7d0]\n",
      "pp: [top oral health advice to help keep your teeth healthy dental implants in mexico urlurl ]\n",
      "\n",
      "tw: [Stay in style and keep your smile with dental implants! The closest artificial product to your own tooth! https://t.co/ZogB6u1x5w]\n",
      "pp: [stay in style and keep your smile with dental implants the closest artificial product to your own tooth urlurl ]\n",
      "\n",
      "tw: [Btu Dental Implant System + LED Endo Motor Handpiece + Apex Locator Surgery Kit https://t.co/k3vW36pTjC]\n",
      "pp: [btu dental implant system led endo motor handpiece apex locator surgery kit urlurl ]\n",
      "\n",
      "tw: [RT @IanNeedleman: Prof. Sculean: @PanDental2016 @BSPerio Importance of achieving good periodontal health B4 placing dental implants https:/‚Ä¶]\n",
      "pp: [rt mentionname prof sculean mentionname mentionname importance of achieving good periodontal health b numbers placing dental implants https ]\n",
      "\n",
      "tw: [With dental implants, Dr. DiFazio can replace missing or severely damaged teeth so you can have your smile back. https://t.co/RblKyKobc9]\n",
      "pp: [with dental implants dr difazio can replace missing or severely damaged teeth so you can have your smile back urlurl ]\n",
      "\n",
      "tw: [Dentzz Reviews &amp; Dentzz Dental Implant reviews https://t.co/lP1R1wFrll #dentzz #reviews https://t.co/UrQo58XDHS]\n",
      "pp: [dentzz reviews dentzz dental implant reviews urlurl ]\n",
      "\n",
      "tw: [Five things you need to know about dental implants in Hampshire https://t.co/rvd91Kq3zx https://t.co/EvNirrDXTs]\n",
      "pp: [five things you need to know about dental implants in hampshire urlurl ]\n",
      "\n",
      "tw: [Wish me luck. I'm going to do dental implant tomorrow. I'm afraid.]\n",
      "pp: [wish me luck i m going to do dental implant tomorrow i m afraid ]\n",
      "\n",
      "tw: [Searching for cheap dental implants? Acapulco - a hidden dental tourism destination. https://t.co/2tgg8IQGWh]\n",
      "pp: [searching for cheap dental implants questionmark acapulco a hidden dental tourism destination urlurl ]\n",
      "\n",
      "tw: [Don't forget it's the last day to order @3MOralCare MDI Mini Dental implants.  For more ideas on how to transition those cases call us today]\n",
      "pp: [don t forget it s the last day to order mentionname mdi mini dental implants for more ideas on how to transition those cases call us today]\n",
      "\n",
      "tw: [Dental implants are the most effective alternative for missing teeth, with over a 98% success rate. https://t.co/IezYuMW1Ns]\n",
      "pp: [dental implants are the most effective alternative for missing teeth with over a numbers success rate urlurl ]\n",
      "\n",
      "tw: [Dental Implants Professionals Provides High-Quality Dental Implants for Patients Who Are Missing Teeth https://t.co/ZFtWRKDEt1 https://t.co/GCzxrAM0xj]\n",
      "pp: [dental implants professionals provides high quality dental implants for patients who are missing teeth urlurl ]\n",
      "\n",
      "tw: [I look at the mentions of a dental implants ad I was served and wow it did not disappoint https://t.co/UOFJIw7FF1]\n",
      "pp: [i look at the mentions of a dental implants ad i was served and wow it did not disappoint urlurl ]\n",
      "\n",
      "tw: [#Dental Implants by Cosmetic Dentistry Experts  #article 129731 @Myartsubmit @Netdatabiz]\n",
      "pp: [ dental implants by cosmetic dentistry experts article numbers mentionname mentionname ]\n",
      "\n",
      "tw: [We have a range of dental implant services whether it be just one tooth missing or a few https://t.co/VKyt65QQpy #DentalImplant #Cheshire #Chester #teeth https://t.co/sTAbDny8Fk]\n",
      "pp: [we have a range of dental implant services whether it be just one tooth missing or a few urlurl ]\n",
      "\n",
      "tw: [10x Dental Implant DSI Lock Attachment Abutment Set + Caps + Housing + Ring 3mm https://t.co/CKN0hb4RHs]\n",
      "pp: [ numbers x dental implant dsi lock attachment abutment set caps housing ring numbers mm urlurl ]\n",
      "\n",
      "tw: [#DentalImplantCost How to choose the right dental implant. Read Blog: https://t.co/CTNV2qhbE1]\n",
      "pp: [ dentalimplantcost how to choose the right dental implant read blog urlurl ]\n",
      "\n",
      "tw: [#dental implant supplies viet nam vacations]\n",
      "pp: [ dental implant supplies viet nam vacations]\n",
      "\n",
      "tw: [Considering dental implants? Celebrate Dental offers the affordable, professional service to help you feel... https://t.co/uPNP9Sr1yx]\n",
      "pp: [considering dental implants questionmark celebrate dental offers the affordable professional service to help you feel urlurl ]\n",
      "\n",
      "tw: [How dental implant works #dentist #dentistry #surgery #smile #NewYork https://t.co/4arhzTir0d]\n",
      "pp: [how dental implant works dentist dentistry surgery smile newyork urlurl ]\n",
      "\n",
      "tw: [Dental Implants Market to enjoy ‚Äòexplosive ‚Äògrowth to 2022 https://t.co/xJsFUxwAHh]\n",
      "pp: [dental implants market to enjoy explosive growth to numbers urlurl ]\n",
      "\n",
      "tw: [Dental Implants 101 https://t.co/Js4c2eNYCo https://t.co/B3I3coAyo8]\n",
      "pp: [dental implants numbers urlurl ]\n",
      "\n",
      "tw: [RT @jenniferjmyers2: Lake Jackson Dentist offers general dentistry including dental implants, braces and more https://t.co/E2gUWQJuUH]\n",
      "pp: [rt mentionname lake jackson dentist offers general dentistry including dental implants braces and more urlurl ]\n",
      "\n",
      "tw: [RT @NottsDentist: Cost Of ALL ON 4 Dental Implants | All-On-4¬†Clinic https://t.co/pK5aQYu3Wy]\n",
      "pp: [rt mentionname cost of all on numbers dental implants all on numbers clinic urlurl ]\n",
      "\n",
      "tw: [Dental Implants Restore Your Smile - What you need to know about dental implants If you want to restore your sm... https://t.co/cnEB66y5Md]\n",
      "pp: [dental implants restore your smile what you need to know about dental implants if you want to restore your sm urlurl ]\n",
      "\n",
      "tw: [RT @WestEnd_Dental: Replace a single missing tooth with a dental implant from just ¬£88 a month with 0% APR interest free finance. https://t‚Ä¶]\n",
      "pp: [rt mentionname replace a single missing tooth with a dental implant from just numbers a month with numbers apr interest free finance https t ]\n",
      "\n",
      "tw: [RT @PerioNews: Dental implants are secured into the jaw, function like natural teeth &amp; allow you to confidently smile, speak, &amp; ea‚Ä¶ ]\n",
      "pp: [rt mentionname dental implants are secured into the jaw function like natural teeth allow you to confidently smile speak ea ]\n",
      "\n",
      "tw: [Check. A #Chinese #Robot Has Performed the World's First Automated Dental Implant https://t.co/wIwcFt9PoD #tech #digital #business https://t.co/9okM5o9lBB]\n",
      "pp: [check a chinese robot has performed the world s first automated dental implant urlurl ]\n",
      "\n",
      "tw: [Dental Implants Anatomy Closeup Model (Health/Medicine)\n",
      "https://t.co/zC0wBeir9s]\n",
      "pp: [dental implants anatomy closeup model health medicine urlurl ]\n",
      "\n",
      "tw: [Dental Implants Tools Kit\n",
      "Whatsapp: 0092-3116960642\n",
      "https://t.co/B1IiSZG3V2 https://t.co/tIdu3Ixi2U]\n",
      "pp: [dental implants tools kit whatsapp numbers numbers urlurl ]\n",
      "\n",
      "tw: [Your permanent solution for missing teeth that fit your active style. For more information on dental implants,... https://t.co/kd0em9NEi7]\n",
      "pp: [your permanent solution for missing teeth that fit your active style for more information on dental implants urlurl ]\n",
      "\n",
      "tw: [‚ÄúAll-On-4¬Æ Dental Implants vs. Dentures and Traditional Implants‚Äù https://t.co/acOo5d1Bxd #dentalimplants #willowpassdentalcare #allon4 https://t.co/9fo8CtGE8l]\n",
      "pp: [ all on numbers dental implants vs dentures and traditional implants urlurl ]\n",
      "\n",
      "tw: [In today‚Äôs blog, we learn about the interesting history of the dental implant. Read on to find out more! https://t.co/sqn3eetzSe]\n",
      "pp: [in today s blog we learn about the interesting history of the dental implant read on to find out more urlurl ]\n",
      "\n",
      "tw: [#price for dental implants serenity bahamas real estate]\n",
      "pp: [ price for dental implants serenity bahamas real estate]\n",
      "\n",
      "tw: [RT @WillowPasDental: Dentures or Dental Implants: What is Best for Me? Willow Pass Dental Care https://t.co/n8SlV1xlCF #Dentist #dentalcare‚Ä¶]\n",
      "pp: [rt mentionname dentures or dental implants what is best for me questionmark willow pass dental care urlurl ]\n",
      "\n",
      "tw: [Do you have dental implants? Do you smoke? Read about the effect that smoking has on dental implants! https://t.co/eg9YgK2K8y]\n",
      "pp: [do you have dental implants questionmark do you smoke questionmark read about the effect that smoking has on dental implants urlurl ]\n",
      "\n",
      "tw: [How much do you truly know about dental implants? Head over to our blog to find out the truth: https://t.co/hYOKPfZHAg https://t.co/cBKBNfgRlI]\n",
      "pp: [how much do you truly know about dental implants questionmark head over to our blog to find out the truth urlurl ]\n",
      "\n",
      "tw: [RT @SandvikMaterial: #Sandvik 316LVM medical wire is used by our customers for dental implants https://t.co/AHV5GKQ1tY https://t.co/Y2g5fnO‚Ä¶]\n",
      "pp: [rt mentionname sandvik numbers lvm medical wire is used by our customers for dental implants urlurl ]\n",
      "\n",
      "tw: [Robot successfully completes dental implant in human. #futurist https://t.co/qDEi3teAWf]\n",
      "pp: [robot successfully completes dental implant in human futurist urlurl ]\n",
      "\n",
      "tw: [#southwest portland or how much does dental implants cost per tooth]\n",
      "pp: [ southwest portland or how much does dental implants cost per tooth]\n",
      "\n",
      "tw: [I liked a @YouTube video https://t.co/VP8ImKVtFT I'M GETTING DENTAL IMPLANTS]\n",
      "pp: [i liked a mentionname video urlurl ]\n",
      "\n",
      "tw: [Curious about the history of dental implants? What about the circumstances in which you may need to seek dental... https://t.co/cE2EeZQB4j]\n",
      "pp: [curious about the history of dental implants questionmark what about the circumstances in which you may need to seek dental urlurl ]\n",
      "\n",
      "tw: [10*NSK Dental implant reduction 20:1 low speed Contra Angle Handpiece MAX SG20 X https://t.co/CmpMPiTpPx https://t.co/bVQeDkj5Yn]\n",
      "pp: [ numbers nsk dental implant reduction numbers numbers low speed contra angle handpiece max sg numbers x urlurl ]\n",
      "\n",
      "tw: [RT @marlonphillips: What should dental implants cost in your city?]\n",
      "pp: [rt mentionname what should dental implants cost in your city questionmark ]\n",
      "\n",
      "tw: [RT @Post_planner: https://t.co/QjbynoMYaW  Dental Implant Technology That You Can Trust]\n",
      "pp: [rt mentionname urlurl ]\n",
      "\n",
      "tw: [3 Ways to Take Care of Your Dental Implants https://t.co/SFKoRkptem]\n",
      "pp: [ numbers ways to take care of your dental implants urlurl ]\n",
      "\n",
      "tw: [Forum Post : ‚ÄãI still haven‚Äôt got my implant in!: The dentist that is supposed to be doing my dental implant ... https://t.co/lSOs9HXqX6]\n",
      "pp: [forum post i still haven t got my implant in the dentist that is supposed to be doing my dental implant urlurl ]\n",
      "\n",
      "tw: [#total workforce management services cost of clear choice dental implants]\n",
      "pp: [ total workforce management services cost of clear choice dental implants]\n",
      "\n",
      "tw: [RT @ukbestdentist: Full Set Dental Implants in The Blythe #Complete #Teeth #Implants #The #Blythe https://t.co/t7Qc2PiRjI]\n",
      "pp: [rt mentionname full set dental implants in the blythe complete teeth implants the blythe urlurl ]\n",
      "\n",
      "tw: [What is a dental implant? Our latest infographic on our blog has the answer: https://t.co/SbEVh3ofeq https://t.co/hHtHQTMvj0]\n",
      "pp: [what is a dental implant questionmark our latest infographic on our blog has the answer urlurl ]\n",
      "\n",
      "tw: [Hiring Now: Project Manager -  Dental Implants in Freiburg, Germany https://t.co/FXh7S7Z4UY #job]\n",
      "pp: [hiring now project manager dental implants in freiburg germany urlurl ]\n",
      "\n",
      "tw: [RT @Learn_Things: This is how a dental implant are installed https://t.co/wqAy4ziz0z]\n",
      "pp: [rt mentionname this is how a dental implant are installed urlurl ]\n",
      "\n",
      "tw: [Taking A Look At Dental Implants Treatment - https://t.co/znlDgcciXX]\n",
      "pp: [taking a look at dental implants treatment urlurl ]\n",
      "\n",
      "tw: [#giorgio armani luminous silk foundation reviews dental implant procedure photos]\n",
      "pp: [ giorgio armani luminous silk foundation reviews dental implant procedure photos]\n",
      "\n",
      "tw: [Dental implants - All on 4 Sandusky, OH: https://t.co/G2APDuQ5sM via @YouTube]\n",
      "pp: [dental implants all on numbers sandusky oh urlurl ]\n",
      "\n",
      "tw: [@PaidGames2 This might help. DDS. Doctor Botbol. Toronto. The Dental Implant Specialist. https://t.co/yh94wRhBo7 Excellent choice. Hope it works Bonus. Love life!]\n",
      "pp: [ mentionname this might help dds doctor botbol toronto the dental implant specialist urlurl ]\n",
      "\n",
      "tw: [RT @TimeToBecome1: Follow These Steps to Find an Experienced Dentist in Your Area - Pisa Dental Implants https://t.co/sV2ukTaDSz]\n",
      "pp: [rt mentionname follow these steps to find an experienced dentist in your area pisa dental implants urlurl ]\n",
      "\n",
      "tw: [San Francisco Dental Implant Center Announces Updated All-on-four‚Ä¶ https://t.co/5YIwmY4lM6]\n",
      "pp: [san francisco dental implant center announces updated all on four urlurl ]\n",
      "\n",
      "tw: [I need Dental Implants https://t.co/7YtbrprnSE]\n",
      "pp: [i need dental implants urlurl ]\n",
      "\n",
      "tw: [#dental implants and bone loss automated industrial machinery]\n",
      "pp: [ dental implants and bone loss automated industrial machinery]\n",
      "\n",
      "tw: [#A.ScottGrivasIii,Dds Durable Smile Rehabilitation: Dental Implants in Sacramento. Read Blog: https://t.co/gFowyJeMRW]\n",
      "pp: [ a scottgrivasiii dds durable smile rehabilitation dental implants in sacramento read blog urlurl ]\n",
      "\n",
      "tw: [20x Dental Implant Implants LAMINA¬Æ Spiral Self-Drilling Compatible Internal Hex https://t.co/u3tm9ErOeN https://t.co/aEuGTfMcir]\n",
      "pp: [ numbers x dental implant implants lamina spiral self drilling compatible internal hex urlurl ]\n",
      "\n",
      "tw: [Learning The Cost Of Full Mouth Dental Implants https://t.co/ZVjQ9brs7r #healthy #health #digitalhealth https://t.co/p5P2wLnahy]\n",
      "pp: [learning the cost of full mouth dental implants urlurl ]\n",
      "\n",
      "tw: [Just Added: Published: Dental Implants Austin Cost\n",
      "https://t.co/EOPHgLpzer]\n",
      "pp: [just added published dental implants austin cost urlurl ]\n",
      "\n",
      "tw: [Dental implants are one of many solutions for replacing missing teeth. We provide exceptional dental services.\n",
      "https://t.co/G787EeUMZ3]\n",
      "pp: [dental implants are one of many solutions for replacing missing teeth we provide exceptional dental services urlurl ]\n",
      "\n",
      "tw: [Learn all about dental implants right here:\n",
      "https://t.co/z78SvOF1t2 https://t.co/IFpn4VDSIi]\n",
      "pp: [learn all about dental implants right here urlurl ]\n",
      "\n",
      "tw: [Follow T.I.D.E. online and stay current with dental implant trends! https://t.co/0bE0gfinC2, https://t.co/9aViSIaryx https://t.co/mZ3J98VRjh]\n",
      "pp: [follow t i d e online and stay current with dental implant trends urlurl ]\n",
      "\n",
      "tw: [RT @dentacoin: Dear community, we are excited to announce to you that Dentacoin is now accepted means of payment for dental implants made b‚Ä¶]\n",
      "pp: [rt mentionname dear community we are excited to announce to you that dentacoin is now accepted means of payment for dental implants made b ]\n",
      "\n",
      "tw: [RT @williswood92: Dental Implants Torrance CA | Redondo Beach | South Bay Dentists https://t.co/KsexpKoJl2]\n",
      "pp: [rt mentionname dental implants torrance ca redondo beach south bay dentists urlurl ]\n",
      "\n",
      "tw: [#travel Affordable Dental Implants https://t.co/MlYmyfEoZF]\n",
      "pp: [ travel affordable dental implants urlurl ]\n",
      "\n",
      "tw: [RT @reportbuyer: #Dental implants are expensive, but permanent solution to #tooth loss https://t.co/YIZbz1WIKz #teeth #dentist‚Ä¶ ]\n",
      "pp: [rt mentionname dental implants are expensive but permanent solution to tooth loss urlurl ]\n",
      "\n",
      "tw: [@goodsearch1 I have one dental implant and love it I wish all of mine were. They are that awesome]\n",
      "pp: [ mentionname i have one dental implant and love it i wish all of mine were they are that awesome]\n",
      "\n",
      "tw: [Improve your quality of life with dental implants, the permanent solution to a missing tooth or teeth. Contact us today on 01773 830560 for more information.\n",
      "https://t.co/KBQwPjrKti‚Ä¶ #dentalimplants #straumann https://t.co/8V4vWnIvb8]\n",
      "pp: [improve your quality of life with dental implants the permanent solution to a missing tooth or teeth contact us today on numbers numbers for more information urlurl ]\n",
      "\n",
      "tw: [Affordable vs. Expensive Dental Implants. Why is Pricing So Different?\n",
      "https://t.co/SR3LUPTt31]\n",
      "pp: [affordable vs expensive dental implants why is pricing so different questionmark urlurl ]\n",
      "\n",
      "tw: [Botched dental implants 'leave woman in agony for almost two years' https://t.co/timCR5h4iG]\n",
      "pp: [botched dental implants leave woman in agony for almost two years urlurl ]\n",
      "\n",
      "tw: [New Year, New Smile! Get FREE Teeth Whitening when receiving multiple dental implants! Call our office to learn... https://t.co/q8DM97pZXH]\n",
      "pp: [new year new smile get free teeth whitening when receiving multiple dental implants call our office to learn urlurl ]\n",
      "\n",
      "tw: [Tiffin service to dental implants: All tried to beat system, swap old notes https://t.co/GWu1ojGlzQ via @IndianExpress]\n",
      "pp: [tiffin service to dental implants all tried to beat system swap old notes urlurl ]\n",
      "\n",
      "tw: [San Francisco Dental Implant Center Announces Availability of Trefoil Dental Implants for Bay Area Patients https://t.co/GJheqi6D1N]\n",
      "pp: [san francisco dental implant center announces availability of trefoil dental implants for bay area patients urlurl ]\n",
      "\n",
      "tw: [#clear choice dental implants costs banner stands for trade shows]\n",
      "pp: [ clear choice dental implants costs banner stands for trade shows]\n",
      "\n",
      "tw: [How to Look After Dental Implants:\n",
      "\n",
      "https://t.co/UlF53gMDrt]\n",
      "pp: [how to look after dental implants urlurl ]\n",
      "\n",
      "tw: [Implant restoration: Our dental implants can restore missing teeth. Call us to see if they're right for you.¬† https://t.co/CAaIiPJifH]\n",
      "pp: [implant restoration our dental implants can restore missing teeth call us to see if they re right for you urlurl ]\n",
      "\n",
      "tw: [Dental implant with slow-release drug reservoir reduces infection risk https://t.co/jtYmBnM2zo https://t.co/XsoakqwmCt]\n",
      "pp: [dental implant with slow release drug reservoir reduces infection risk urlurl ]\n",
      "\n",
      "tw: [Dental Implants vs Bridges. Which ones would you prefer? https://t.co/qOWKkoU8B5]\n",
      "pp: [dental implants vs bridges which ones would you prefer questionmark urlurl ]\n",
      "\n",
      "tw: [RT @CRDentalimplant: SAVE MONEY ON YOUR DENTAL IMPLANTS IN COSTA RICA: https://t.co/d0iqJd6Iia https://t.co/knBhCmbZcx]\n",
      "pp: [rt mentionname save money on your dental implants in costa rica urlurl ]\n",
      "\n",
      "tw: [dentistry dental implant restoration surgery tools \n",
      "https://t.co/Q6GtxnPkp3]\n",
      "pp: [dentistry dental implant restoration surgery tools urlurl ]\n",
      "\n",
      "tw: [RT @ucleastman: CPD: Learn how to perform mucogingival therapy around teeth and dental implants, book now for March https://t.co/ujJON514ub]\n",
      "pp: [rt mentionname cpd learn how to perform mucogingival therapy around teeth and dental implants book now for march urlurl ]\n",
      "\n",
      "tw: [RT @Kristen7765: Dental Implants Seattle - Anoosh Afifi Comprehensive Dentistry https://t.co/zfcW6Uod7g]\n",
      "pp: [rt mentionname dental implants seattle anoosh afifi comprehensive dentistry urlurl ]\n",
      "\n",
      "tw: [#dental implant prices informal custody agreement]\n",
      "pp: [ dental implant prices informal custody agreement]\n",
      "\n",
      "tw: [RT @EurekaMag: Evaluation of an experimental periodontal ligament for dental implants https://t.co/fTYGM8fZ2l #EurekaMag https://t.co/IZObY‚Ä¶]\n",
      "pp: [rt mentionname evaluation of an experimental periodontal ligament for dental implants urlurl ]\n",
      "\n",
      "tw: [Fed up with uncomfortable dentures! Find out more about dental implants with a no obligation consultation #Leicester]\n",
      "pp: [fed up with uncomfortable dentures find out more about dental implants with a no obligation consultation leicester]\n",
      "\n",
      "tw: [@TheRickWilson Kind of slurry there from the Weenie Cheeto. Perhaps the dental implants have also come undone]\n",
      "pp: [ mentionname kind of slurry there from the weenie cheeto perhaps the dental implants have also come undone]\n",
      "\n",
      "tw: [Teeth make all the difference! You can look 10 years younger with our dental implant treatments! Contact us now! https://t.co/oUKxrA64hj https://t.co/rpvpwEWKny]\n",
      "pp: [teeth make all the difference you can look numbers years younger with our dental implant treatments contact us now urlurl ]\n",
      "\n",
      "tw: [Missing teeth? Considering Dental Implants? https://t.co/4GJmEgwUS0 #Dentist #Stirling https://t.co/mHL1snKmA0]\n",
      "pp: [missing teeth questionmark considering dental implants questionmark urlurl ]\n",
      "\n",
      "tw: [@JanetMagee6 @realDonaldTrump Best dental implant ... EVER!!]\n",
      "pp: [ mentionname mentionname best dental implant ever ]\n",
      "\n",
      "tw: [#CreekviewDental A Step by Step Guide to Dental Implants. Read Blog: https://t.co/025Z3b6WQK]\n",
      "pp: [ creekviewdental a step by step guide to dental implants read blog urlurl ]\n",
      "\n",
      "tw: [#iti dental implant miracas online shopping site]\n",
      "pp: [ iti dental implant miracas online shopping site]\n",
      "\n",
      "tw: [In this video you can get the answer for your question-Dental Implants Cost in #Mexico https://t.co/PhLg5f5Z2f #dental]\n",
      "pp: [in this video you can get the answer for your question dental implants cost in mexico urlurl ]\n",
      "\n",
      "tw: [AZDENT Dental Implant Motor System LED Surgical Brushless Handpiece https://t.co/rgsDt2UF8a https://t.co/84NSsVvcYF]\n",
      "pp: [azdent dental implant motor system led surgical brushless handpiece urlurl ]\n",
      "\n",
      "tw: [Dr. Ira Langstein is lecturing on novel dental implant techniques. https://t.co/vH1qjPv3sk]\n",
      "pp: [dr ira langstein is lecturing on novel dental implant techniques urlurl ]\n",
      "\n",
      "tw: [#dental implant procedure video corporate advertising jobs]\n",
      "pp: [ dental implant procedure video corporate advertising jobs]\n",
      "\n",
      "tw: [Dental Implants-We offer Dental Implants/ Braces from Rs.15999 (EMI'S Available)..Please contact us: +91-9988556934 https://t.co/IJQa0vyGTf https://t.co/QUPr3JOCn7]\n",
      "pp: [dental implants we offer dental implants braces from rs numbers emi s available please contact us numbers numbers urlurl ]\n",
      "\n",
      "tw: [We can restore that classic smile of yours back to its original state with dental implants. Call us today to... https://t.co/pwXcRr7LgK]\n",
      "pp: [we can restore that classic smile of yours back to its original state with dental implants call us today to urlurl ]\n",
      "\n",
      "tw: [RT @NellieFawcett: Dental Implants - Columbus Ohio Dentist | Upper Arlington OH Dentist https://t.co/9lkmQDwl9K]\n",
      "pp: [rt mentionname dental implants columbus ohio dentist upper arlington oh dentist urlurl ]\n",
      "\n",
      "tw: [@whisperandmoan it's from a broken nose, dental implants &amp; jaw reconstruction, he's 26 yrs older FFS &amp; at least 25 pounds heavier so STFU]\n",
      "pp: [ mentionname it s from a broken nose dental implants jaw reconstruction he s numbers yrs older ffs at least numbers pounds heavier so stfu]\n",
      "\n",
      "tw: [If you're missing a tooth, we can help. We've been placing dental implants for over 20 years #dentalimplants #smile https://t.co/KKTkx0lLMK https://t.co/liQeBuvEji]\n",
      "pp: [if you re missing a tooth we can help we ve been placing dental implants for over numbers years dentalimplants smile urlurl ]\n",
      "\n",
      "tw: [Harrell Dental Implant Center is Tops in Charlotte https://t.co/7u8nVFNctH]\n",
      "pp: [harrell dental implant center is tops in charlotte urlurl ]\n",
      "\n",
      "tw: [#average cost dental implants per tooth monty python slot machine]\n",
      "pp: [ average cost dental implants per tooth monty python slot machine]\n",
      "\n",
      "tw: [RT @DrGorczyca: Dental Implants Look and Feel Like Real Teeth https://t.co/rKlpqLe8Ux #dental #dentalimplants #implants #teeth #antioch htt‚Ä¶]\n",
      "pp: [rt mentionname dental implants look and feel like real teeth urlurl ]\n",
      "\n",
      "tw: [A dental implant not only restores the function of your teeth, it restores the beauty of your smile! https://t.co/zYntL86rLe]\n",
      "pp: [a dental implant not only restores the function of your teeth it restores the beauty of your smile urlurl ]\n",
      "\n",
      "tw: [We added a new FAQ to our website: How long do dental implants last? https://t.co/5UQlpfZJQ9]\n",
      "pp: [we added a new faq to our website how long do dental implants last questionmark urlurl ]\n",
      "\n",
      "tw: [Global Dental Implants and Biomaterials Market Size, Share, Growth, Trends, Analysis and Forecast to 2022: Re... https://t.co/91MCQ2nORs]\n",
      "pp: [global dental implants and biomaterials market size share growth trends analysis and forecast to numbers re urlurl ]\n",
      "\n",
      "tw: [As professional private #London dentists we offer predictable permanent treatment solutions using dental implants, to replace missing teeth.]\n",
      "pp: [as professional private london dentists we offer predictable permanent treatment solutions using dental implants to replace missing teeth ]\n",
      "\n",
      "tw: [@AnthonyCumiaxyz would dental implants be better?]\n",
      "pp: [ mentionname would dental implants be better questionmark ]\n",
      "\n",
      "tw: [Dental Implants In Nassau County NY Can Restore Your Smile | Dental Health Advice https://t.co/HotGctI1Tc]\n",
      "pp: [dental implants in nassau county ny can restore your smile dental health advice urlurl ]\n",
      "\n",
      "tw: [Are you interested in dental implants but your dentist doesn‚Äôt do them? Ask for a referral to our specialist team #Tonbridge]\n",
      "pp: [are you interested in dental implants but your dentist doesn t do them questionmark ask for a referral to our specialist team tonbridge]\n",
      "\n",
      "tw: [I just uploaded ‚ÄúDental Implants Dentist Yuba City CA‚Äù to #Vimeo: https://t.co/rcybbVjYZp]\n",
      "pp: [i just uploaded dental implants dentist yuba city ca to vimeo urlurl ]\n",
      "\n",
      "tw: [Tooth Implant Sydney Offers Affordable and Comprehensive Dental Implant Services https://t.co/LfptBbpL66 https://t.co/nStSLfHj1w]\n",
      "pp: [tooth implant sydney offers affordable and comprehensive dental implant services urlurl ]\n",
      "\n",
      "tw: [Dillon's Dental Implant Surgery https://t.co/day9jkpK2C]\n",
      "pp: [dillon s dental implant surgery urlurl ]\n",
      "\n",
      "tw: [Fire at dental implant unit near #Airport https://t.co/9aArwPnlAM]\n",
      "pp: [fire at dental implant unit near airport urlurl ]\n",
      "\n",
      "tw: [Dental implants are a great option to replace back teeth so you can chew.  This is what it looks‚Ä¶ https://t.co/yb2vZ3Exzq]\n",
      "pp: [dental implants are a great option to replace back teeth so you can chew this is what it looks urlurl ]\n",
      "\n",
      "tw: [Free informational booklet on DENTAL IMPLANT THERAPY\n",
      "\n",
      "Created from the most common questions I hear regarding... https://t.co/bIdjA9C1VF]\n",
      "pp: [free informational booklet on dental implant therapy created from the most common questions i hear regarding urlurl ]\n",
      "\n",
      "tw: [Dental surgeons develop revolutionary method to speed up dental implant procedures\n",
      "       \n",
      "‚Ä¶ https://t.co/UYAwzYvcrM]\n",
      "pp: [dental surgeons develop revolutionary method to speed up dental implant procedures urlurl ]\n",
      "\n",
      "tw: [Dr. Farahmand‚Äôs Comprehensive Interdisciplinary Study Club with colleagues discussing a complex dental implants case. https://t.co/svDcJBV1t4]\n",
      "pp: [dr farahmand s comprehensive interdisciplinary study club with colleagues discussing a complex dental implants case urlurl ]\n",
      "\n",
      "tw: [Believe it or not, dental implants date all the way back to the Ancient Mayans in 600 AD. They hammered pieces of shell into the jaw to replace missing teeth. https://t.co/P2txEolyoE]\n",
      "pp: [believe it or not dental implants date all the way back to the ancient mayans in numbers ad they hammered pieces of shell into the jaw to replace missing teeth urlurl ]\n",
      "\n",
      "tw: [#treadmill best home average cost of a dental implant]\n",
      "pp: [ treadmill best home average cost of a dental implant]\n",
      "\n",
      "tw: [#SeidenDental Dental Implants in Levittown. Read Blog: https://t.co/QFXYfILALL]\n",
      "pp: [ seidendental dental implants in levittown read blog urlurl ]\n",
      "\n",
      "tw: [RT @allenasamrxtg54: Facts You‚Äôll Need To Know About Dental Implants - Happy Rock Dental https://t.co/Bzx1eq4twP]\n",
      "pp: [rt mentionname facts you ll need to know about dental implants happy rock dental urlurl ]\n",
      "\n",
      "tw: [If I can do it right after godawful dental implant surgery, you can do it https://t.co/mFUlIgP2tl]\n",
      "pp: [if i can do it right after godawful dental implant surgery you can do it urlurl ]\n",
      "\n",
      "tw: [#dentistry news: Bay Area Dental Implant Information Page Announced by San Francisco Dental Implant Center https://t.co/1BdaX4PwyT]\n",
      "pp: [ dentistry news bay area dental implant information page announced by san francisco dental implant center urlurl ]\n",
      "\n",
      "tw: [Dr. Agravat Same Day Dental Implants Clinic Ahmedabad India Celebrates 18 Years Of Implant Dentistry &amp; Launching‚Ä¶ https://t.co/t4Vhp7HpZt https://t.co/dT2t5fdOlD]\n",
      "pp: [dr agravat same day dental implants clinic ahmedabad india celebrates numbers years of implant dentistry launching urlurl ]\n",
      "\n",
      "tw: [Learn more about a dental implant in Boca Raton and how Dr McCauley can assist figuring out your insurance coverage https://t.co/aGGQp8jmRf]\n",
      "pp: [learn more about a dental implant in boca raton and how dr mccauley can assist figuring out your insurance coverage urlurl ]\n",
      "\n",
      "tw: [What Do Dental Implants Cost And Are They Really Good Value? https://t.co/abWU7xfxOL]\n",
      "pp: [what do dental implants cost and are they really good value questionmark urlurl ]\n",
      "\n",
      "tw: [Putting together the RIGHT sales plan for the Dental Implant Industry! https://t.co/artDdGZfGW #dentalimplant #sales #healthcare https://t.co/6wo9uvt0rx]\n",
      "pp: [putting together the right sales plan for the dental implant industry urlurl ]\n",
      "\n",
      "tw: [My @Quora post: Global Dental Implants Market Research Report 2016 https://t.co/zixuxeWImQ]\n",
      "pp: [my mentionname post global dental implants market research report numbers urlurl ]\n",
      "\n",
      "tw: [Punjab to provide free dental implants to needy and poor https://t.co/Z2ZMaMJeWS #dentalsupplies]\n",
      "pp: [punjab to provide free dental implants to needy and poor urlurl ]\n",
      "\n",
      "tw: [RT @Facts_andTruth: What all is Done in a Dental Implants Procedure? ‚Äì Learn Some of the Advantages | Booost https://t.co/GFUTBO4vno]\n",
      "pp: [rt mentionname what all is done in a dental implants procedure questionmark learn some of the advantages booost urlurl ]\n",
      "\n",
      "tw: [Care for Your New Dental Implants \n",
      "https://t.co/Od9XHZrjdk #teeth #health #smile #dentist]\n",
      "pp: [care for your new dental implants urlurl ]\n",
      "\n",
      "tw: [Dental implants torque wrench Nobel Biocare compatible premium 4mm square hole https://t.co/YYxPKv0SKx https://t.co/eNUt24ZuK4]\n",
      "pp: [dental implants torque wrench nobel biocare compatible premium numbers mm square hole urlurl ]\n",
      "\n",
      "tw: [Things to Avoid After a Dental Implant Surgery https://t.co/P9BDwanAuB]\n",
      "pp: [things to avoid after a dental implant surgery urlurl ]\n",
      "\n",
      "tw: [Happy St. Patrick's Day from First Dental Implant Center! https://t.co/7P2VGK5NYw]\n",
      "pp: [happy st patrick s day from first dental implant center urlurl ]\n",
      "\n",
      "tw: [#dental implants nc do deer like corn syrup https://t.co/G3jdIBgN68]\n",
      "pp: [ dental implants nc do deer like corn syrup urlurl ]\n",
      "\n",
      "tw: [Dental implants are an investment for your future confidence. Be the most confident you, schedule your dental implants consultation today! https://t.co/Icsqubplri]\n",
      "pp: [dental implants are an investment for your future confidence be the most confident you schedule your dental implants consultation today urlurl ]\n",
      "\n",
      "tw: [Dr. Steven Van Scoyoc Now Features X-Guide‚Ñ¢ Technology to Improve Placement of Dental Implants in Southern Pines, NC https://t.co/8criKawo4w]\n",
      "pp: [dr steven van scoyoc now features x guide technology to improve placement of dental implants in southern pines nc urlurl ]\n",
      "\n",
      "tw: [here's what new dental implants should cost: ‚Äî wh- https://t.co/F7oqGeQiD3]\n",
      "pp: [here s what new dental implants should cost wh urlurl ]\n",
      "\n",
      "tw: [5 key takeaways for dental implant imaging https://t.co/G4vZ98U0dM]\n",
      "pp: [ numbers key takeaways for dental implant imaging urlurl ]\n",
      "\n",
      "tw: [Find the Best Dentist in Croatia for Dental implants through https://t.co/6Z6oD2f75L . https://t.co/7FuzjS16Md #WriteUpCafe]\n",
      "pp: [find the best dentist in croatia for dental implants through urlurl ]\n",
      "\n",
      "tw: [RT @TheDailyLifer: Here's What New Dental Implants Should Cost https://t.co/N8rUJa4V3w]\n",
      "pp: [rt mentionname here s what new dental implants should cost urlurl ]\n",
      "\n",
      "tw: [Call today for your free Dental Implant Consultation or second opinion. We guarantee to beat the competition or... https://t.co/MqSQdxJ7oq]\n",
      "pp: [call today for your free dental implant consultation or second opinion we guarantee to beat the competition or urlurl ]\n",
      "\n",
      "tw: [Dentist, Darryl D. Burke, DDS, on Staff at Burke Dental Implants and General Dentistry, to‚Ä¶ https://t.co/BniNwg1cgp https://t.co/e8FnHaWCvn]\n",
      "pp: [dentist darryl d burke dds on staff at burke dental implants and general dentistry to urlurl ]\n",
      "\n",
      "tw: [Laser Treatment Saves Ailing and Failing Dental Implants https://t.co/L7oYYaOxSX 2017 https://t.co/CsCkq9la62]\n",
      "pp: [laser treatment saves ailing and failing dental implants urlurl ]\n",
      "\n",
      "tw: [Don't be ashamed of your smile! Ask us about dental implants! https://t.co/zyl9vDaKMu]\n",
      "pp: [don t be ashamed of your smile ask us about dental implants urlurl ]\n",
      "\n",
      "tw: [Factors affecting the possibility to detect buccal bone condition around dental implants using cone beam computed ‚Ä¶ https://t.co/YRVVy9CPHo]\n",
      "pp: [factors affecting the possibility to detect buccal bone condition around dental implants using cone beam computed urlurl ]\n",
      "\n",
      "tw: [RT @AMDCUAE: Dental implants look and feel like your own teeth. \n",
      "+971 4 374 8428\n",
      "#Topdentist #Dubai #OralHealth #MySmile\n",
      "#ÿØÿ®Ÿä‚Ä¶ ]\n",
      "pp: [rt mentionname dental implants look and feel like your own teeth numbers numbers numbers numbers topdentist dubai oralhealth mysmile ÿØÿ®Ÿä ]\n",
      "\n",
      "tw: [Dr Lampee explains permanent dental implants on AM Northwest: https://t.co/Y7LPk6Ud5j via @YouTube]\n",
      "pp: [dr lampee explains permanent dental implants on am northwest urlurl ]\n",
      "\n",
      "tw: [Advantages of Dental Implants | https://t.co/f14a9Huteo https://t.co/0QLJH99chJ]\n",
      "pp: [advantages of dental implants urlurl ]\n",
      "\n",
      "tw: [Attend a public seminar about dental implants on Tuesday, 15 August at the Dr Agravat Cosmetic Dental Clinic in Bodakdev, Ahmedabad. https://t.co/3Yr3rQSUdm]\n",
      "pp: [attend a public seminar about dental implants on tuesday numbers august at the dr agravat cosmetic dental clinic in bodakdev ahmedabad urlurl ]\n",
      "\n",
      "tw: [Survival of dental implants placed in sites of previously failed implants https://t.co/uft3Mxwc30 https://t.co/SH27Mvdhgp]\n",
      "pp: [survival of dental implants placed in sites of previously failed implants urlurl ]\n",
      "\n",
      "tw: [A simple Dental Implant case by Dr Martin J Cribbin of @CMDental_  a two tooth space filled by a two unit implant supported bridge. One very happy smiling patient in time for the Christmas season. https://t.co/IQvYwGVqY5]\n",
      "pp: [a simple dental implant case by dr martin j cribbin of mentionname a two tooth space filled by a two unit implant supported bridge one very happy smiling patient in time for the christmas season urlurl ]\n",
      "\n",
      "tw: [RT @london_dentists: #Your #Smile #Is #Priceless‚Ä¶.!!\n",
      "Depending on #technology hungary dental provide a  #service of #3D dental implants. ht‚Ä¶]\n",
      "pp: [rt mentionname your smile is priceless depending on technology hungary dental provide a service of numbers d dental implants ht ]\n",
      "\n",
      "tw: [For great evils, there are great solutions ... And ours are dental implants!\n",
      "Learn more about the treatments we offer, as well as how we can make you happy!\n",
      "https://t.co/oiZCViWchf https://t.co/eLqgT3jcBr]\n",
      "pp: [for great evils there are great solutions and ours are dental implants learn more about the treatments we offer as well as how we can make you happy urlurl ]\n",
      "\n",
      "tw: [#price for tile roof dental implant bridge cost]\n",
      "pp: [ price for tile roof dental implant bridge cost]\n",
      "\n",
      "tw: [Seeing my oral surgeon for continued dental implant care. (@ Dr Boland in FL) https://t.co/KJinbf0vdS]\n",
      "pp: [seeing my oral surgeon for continued dental implant care dr boland in fl urlurl ]\n",
      "\n",
      "tw: [For those with missing teeth, dental implants are the best alternative to your natural teeth &amp; function just as well https://t.co/0Qk7IZM4yS]\n",
      "pp: [for those with missing teeth dental implants are the best alternative to your natural teeth function just as well urlurl ]\n",
      "\n",
      "tw: [@LikeABeeToHoney Congratulations on the dental implants]\n",
      "pp: [ mentionname congratulations on the dental implants]\n",
      "\n",
      "tw: [RT @TheBestWebDaily: The importance of dental implant insurance and what you should consider before taking the plunge and getting one...\n",
      "\n",
      "h‚Ä¶]\n",
      "pp: [rt mentionname the importance of dental implant insurance and what you should consider before taking the plunge and getting one h ]\n",
      "\n",
      "tw: [Lost An Adult Tooth? Keep Calm And Get A Dental Implant https://t.co/3QfWjlMtiE #Chantilly #Dental https://t.co/hVVedbJixE]\n",
      "pp: [lost an adult tooth questionmark keep calm and get a dental implant urlurl ]\n",
      "\n",
      "tw: [#www dental implant what-if analysis excel tools]\n",
      "pp: [ www dental implant what if analysis excel tools]\n",
      "\n",
      "tw: [#front teeth dental implants massage northgate seattle]\n",
      "pp: [ front teeth dental implants massage northgate seattle]\n",
      "\n",
      "tw: [Have #Dental Implants? Follow These Tips to Keep Them Healthy and Happy - https://t.co/NlZ8YmZzgk https://t.co/CC0kRkqn5K]\n",
      "pp: [have dental implants questionmark follow these tips to keep them healthy and happy urlurl ]\n",
      "\n",
      "tw: [#Breaking_News With new nanocoating, we may now have more successful dental implants https://t.co/nJAcrjfLG6 via #Indilens https://t.co/AlWcQsndkh]\n",
      "pp: [ breaking news with new nanocoating we may now have more successful dental implants urlurl ]\n",
      "\n",
      "tw: [20 Kits of Dental Implant + Straight Abutment + Healing Cap + Transfer + Analog https://t.co/DngJa1lnDa https://t.co/iGJs5W2oMZ]\n",
      "pp: [ numbers kits of dental implant straight abutment healing cap transfer analog urlurl ]\n",
      "\n",
      "tw: [Dental Implants Missing teeth not only troubles yo..For more info visit... https://t.co/MlxYLiHqzz]\n",
      "pp: [dental implants missing teeth not only troubles yo for more info visit urlurl ]\n",
      "\n",
      "tw: [What are the benefits of dental implant?\n",
      "\n",
      "Click Here ‚¨áÔ∏è\n",
      "https://t.co/9iap7WcrZq\n",
      "\n",
      "#NewsToday #INFORMATION #teethwhitening #teeth #healthcare #Health #HealthForAll #oralhealth #Medical #dental #dentalcare #dentalhealth #Smile #USA #treatment #Hialeah https://t.co/24ORLLZuyI]\n",
      "pp: [what are the benefits of dental implant questionmark click here urlurl newstoday information teethwhitening teeth healthcare health healthforall oralhealth medical dental dentalcare dentalhealth smile usa treatment hialeah urlurl ]\n",
      "\n",
      "tw: [RT @ErmaWaters20: Why Dental Implants Are Better Than Dentures | Chandler Dentist https://t.co/RxaP02T1CD]\n",
      "pp: [rt mentionname why dental implants are better than dentures chandler dentist urlurl ]\n",
      "\n",
      "tw: [What Should I Expect The Cost Of Full Mouth Dental Implants To Be? https://t.co/aGoV93fxwy]\n",
      "pp: [what should i expect the cost of full mouth dental implants to be questionmark urlurl ]\n",
      "\n",
      "tw: [If I had $5,000 I'd get [dental implants so I could have a social life once again..]! ü§ë #GetItAll @Postmates]\n",
      "pp: [if i had moneymark numbers numbers i d get dental implants so i could have a social life once again getitall mentionname ]\n",
      "\n",
      "tw: [Your Complete Guide To Dental Implants In Short Hills NJ | Article Goal https://t.co/kwOya5RUXa]\n",
      "pp: [your complete guide to dental implants in short hills nj article goal urlurl ]\n",
      "\n",
      "tw: [#Dental Implants at Crown House https://t.co/0SFiJXtq0S #dentist #Egham #Surrey https://t.co/nWnt9wE9aT]\n",
      "pp: [ dental implants at crown house urlurl ]\n",
      "\n",
      "tw: [Are you in the market for a restored smile? Ask us about dental implants! https://t.co/nUZIr6tNEc]\n",
      "pp: [are you in the market for a restored smile questionmark ask us about dental implants urlurl ]\n",
      "\n",
      "tw: [Don‚Äôt Let Diabetes Stop You from Having Dental Implants https://t.co/b8M3iZE0zV https://t.co/GHBybdDhs9]\n",
      "pp: [don t let diabetes stop you from having dental implants urlurl ]\n",
      "\n",
      "tw: [RT @3dersorg: Singapore researchers invented a 3D printed scaffold to grow bone for dental implants\n",
      "https://t.co/LstUNjI0jQ‚Ä¶ ]\n",
      "pp: [rt mentionname singapore researchers invented a numbers d printed scaffold to grow bone for dental implants urlurl ]\n",
      "\n",
      "tw: [Patient Who Had Two Front Teeth Replaced With Dental Implants In¬†Nottingham https://t.co/ijqXHwumyU]\n",
      "pp: [patient who had two front teeth replaced with dental implants in nottingham urlurl ]\n",
      "\n",
      "tw: [RT @Dentistry_bio: Russia Dental Implants Market Outlook to 2025 Report Updated 01092018 Prices from USD $4246: Russia Dental Implants Mark‚Ä¶]\n",
      "pp: [rt mentionname russia dental implants market outlook to numbers report updated numbers prices from usd moneymark numbers russia dental implants mark ]\n",
      "\n",
      "tw: [Survival of dental implants in patients with Down syndrome: A case series https://t.co/0tha9Or2E0]\n",
      "pp: [survival of dental implants in patients with down syndrome a case series urlurl ]\n",
      "\n",
      "tw: [RT @DentalImplantCR: Questions about dental implants, rates and more? I¬¥m here to help you, let me know your questions and concerns. I will‚Ä¶]\n",
      "pp: [rt mentionname questions about dental implants rates and more questionmark i m here to help you let me know your questions and concerns i will ]\n",
      "\n",
      "tw: [#Dental Implants in Cave Creek\n",
      "You can‚Äôt stop smiling once you receive the dental implants in Cave Creek from our top-rated Cosmetic dentist https://t.co/E6Jz7038Nx]\n",
      "pp: [ dental implants in cave creek you can t stop smiling once you receive the dental implants in cave creek from our top rated cosmetic dentist urlurl ]\n",
      "\n",
      "tw: [Tips On Choosing A Good Specialist In Dental Implants In West Los Angeles https://t.co/jDxNWVyes9]\n",
      "pp: [tips on choosing a good specialist in dental implants in west los angeles urlurl ]\n",
      "\n",
      "tw: [100 Straight Anatomic Abutment, For Dental Implant Internal Hex https://t.co/SHIU74eRiD https://t.co/ajIDrrmown]\n",
      "pp: [ numbers straight anatomic abutment for dental implant internal hex urlurl ]\n",
      "\n",
      "tw: [RT @john_pasqual: Dental implants and wisdom tooth extraction are two of the focus areas of Dr. John Pasqual‚Äôs practice. https://t.co/i16TI‚Ä¶]\n",
      "pp: [rt mentionname dental implants and wisdom tooth extraction are two of the focus areas of dr john pasqual s practice urlurl ]\n",
      "\n",
      "tw: [RT @UnaBlake01: Dental Implants Dublin - For The Best Prices, Call 01-2603741 https://t.co/vcuZi3se6H]\n",
      "pp: [rt mentionname dental implants dublin for the best prices call numbers numbers urlurl ]\n",
      "\n",
      "tw: [How should you replace your missing tooth? See why dental implants are one of the most popular restoration options https://t.co/RRbKVHXk9c]\n",
      "pp: [how should you replace your missing tooth questionmark see why dental implants are one of the most popular restoration options urlurl ]\n",
      "\n",
      "tw: [Will I Need a Bone Graft for Dental Implants? https://t.co/q9QKqbGFwn #dentist]\n",
      "pp: [will i need a bone graft for dental implants questionmark urlurl ]\n",
      "\n",
      "tw: [Replacement of Missing Molar tooth with Dental Implants-Dental Clinic in Chennai https://t.co/ENopPSeXy4]\n",
      "pp: [replacement of missing molar tooth with dental implants dental clinic in chennai urlurl ]\n",
      "\n",
      "tw: [Dental Implants Wilmslow | Dental Implant Manchest https://t.co/44BAM8YMI0]\n",
      "pp: [dental implants wilmslow dental implant manchest urlurl ]\n",
      "\n",
      "tw: [TeethXpress Full Arch Dental Implants Course https://t.co/EDaWZnLu6B https://t.co/mCAgKV0Tbg]\n",
      "pp: [teethxpress full arch dental implants course urlurl ]\n",
      "\n",
      "tw: [Hair transplants in Istanbul \n",
      "Cosmetic operations in Turkey\n",
      "Nose Surgery\n",
      "Liposuction\n",
      "Dental implants\n",
      "Hollywood Smile\n",
      "Gastric toning\n",
      "‚òéÔ∏è00905372601174\n",
      "‚òéÔ∏è00905444111330\n",
      "WhatsApp+Viber+Call https://t.co/lGGmYN6bHu]\n",
      "pp: [hair transplants in istanbul cosmetic operations in turkey nose surgery liposuction dental implants hollywood smile gastric toning numbers numbers whatsapp viber call urlurl ]\n",
      "\n",
      "tw: [See how dental implants can transform the way you feel about your smile: https://t.co/LhiXLOjNe7]\n",
      "pp: [see how dental implants can transform the way you feel about your smile urlurl ]\n",
      "\n",
      "tw: [Best Dental Implant Sugar Land TX - Patient Review\n",
      "\n",
      "Advanced Dentistry\n",
      "9920 U.S. Highway 90-A #100-C, Sugar Land, TX 77478\n",
      "(281) 494-5600\n",
      "https://t.co/LhocpucLpC https://t.co/3Y7HHgtFya]\n",
      "pp: [best dental implant sugar land tx patient review advanced dentistry numbers u s highway numbers a numbers c sugar land tx numbers numbers numbers numbers urlurl ]\n",
      "\n",
      "tw: [$ALSGD announce  dental implant placement https://t.co/2vSEwFqxHj  #WSJ #nytimes #reuters #bloomberg #thestreet #jimmyfallon #forbes #nasdaq #chicago #digitalmarkiting #ihub #newyork  #social #networking #business  #cnn #bet #foxnews https://t.co/rd3YOJFVeP]\n",
      "pp: [ moneymark alsgd announce dental implant placement urlurl ]\n",
      "\n",
      "tw: [Dental implants can help you maintain a healthy diet and improve your overall health! Contact Dr. DID to learn more about the benefits of dental implants. #Dr.DID https://t.co/UUM8jPAMdj]\n",
      "pp: [dental implants can help you maintain a healthy diet and improve your overall health contact dr did to learn more about the benefits of dental implants dr did urlurl ]\n",
      "\n",
      "tw: [RT @HasletDentist: Dental implants are almost structurally identical to natural teeth, making them a great option for replacements: https:/‚Ä¶]\n",
      "pp: [rt mentionname dental implants are almost structurally identical to natural teeth making them a great option for replacements https ]\n",
      "\n",
      "tw: [How many of you may need a dental implant? Because you don't come on vacation to Cancun to solve the problem and save a lot of money and also have fun with their family.\n",
      "\n",
      "#vozdental‚Ä¶ https://t.co/67ZAgLKArC]\n",
      "pp: [how many of you may need a dental implant questionmark because you don t come on vacation to cancun to solve the problem and save a lot of money and also have fun with their family vozdental urlurl ]\n",
      "\n",
      "tw: [RT @KianorShah: Fulcrum Implant \n",
      "Strongest Dental Implant in the World - Coming‚Ä¶ https://t.co/0aAUlfwIfq]\n",
      "pp: [rt mentionname fulcrum implant strongest dental implant in the world coming urlurl ]\n",
      "\n",
      "tw: [MIS dental implant 3mm complete prosthetic kit, h 4mm, WP https://t.co/6kyCjB8WIw]\n",
      "pp: [mis dental implant numbers mm complete prosthetic kit h numbers mm wp urlurl ]\n",
      "\n",
      "tw: [RT @FOSInstitute: ‚ÄúA New Hope for Some Patients Seeking Dental Implants‚Äù https://t.co/1hVkeAaUsz #DentalImplants #fosi‚Ä¶ ]\n",
      "pp: [rt mentionname a new hope for some patients seeking dental implants urlurl ]\n",
      "\n",
      "tw: [Dental Implants Diploma \n",
      "Sana'a 2017-2018 \n",
      "Module 2 : from 12th to 14th July 2017 \n",
      ". \n",
      "Speakers‚Ä¶ https://t.co/wsouH6CAi5]\n",
      "pp: [dental implants diploma sana a numbers numbers module numbers from numbers th to numbers th july numbers speakers urlurl ]\n",
      "\n",
      "tw: [I just added \"Dental Implants Dentist Middlebury CT\" to Joy K. Lunan, DDS on @Vimeo: https://t.co/FRQB5iWugF]\n",
      "pp: [i just added dental implants dentist middlebury ct to joy k lunan dds on mentionname urlurl ]\n",
      "\n",
      "tw: [DENTAL IMPLANTS GURGAON | BEST DENTAL CLINIC | BES..For more info visit... https://t.co/JwsYjO50Z4 https://t.co/6i1pyQtkNw]\n",
      "pp: [dental implants gurgaon best dental clinic bes for more info visit urlurl ]\n",
      "\n",
      "tw: [What is the true cost of dental implants in Puyallup? And why is it important to choose an implant dentist? Read now: https://t.co/eR8IWOGAaU https://t.co/eR8IWOGAaU]\n",
      "pp: [what is the true cost of dental implants in puyallup questionmark and why is it important to choose an implant dentist questionmark read now urlurl ]\n",
      "\n",
      "tw: [From planning to placement, CAD/CAM technology can make a big difference in your dental implant experience. Learn... https://t.co/1wH6zgShmV]\n",
      "pp: [from planning to placement cad cam technology can make a big difference in your dental implant experience learn urlurl ]\n",
      "\n",
      "tw: [#dental implants covered by insurance honda pedals]\n",
      "pp: [ dental implants covered by insurance honda pedals]\n",
      "\n",
      "tw: [Dr. Stephen DeLoach, Tooth Restoration and Dental Implants Expert, Chosen for Upcoming Book https://t.co/oI7XHFGwmp https://t.co/tQ7VAo6BLs]\n",
      "pp: [dr stephen deloach tooth restoration and dental implants expert chosen for upcoming book urlurl ]\n",
      "\n",
      "tw: [Dental implants consist of a titanium post and a dental crown attached to its end:\n",
      "\n",
      "https://t.co/cFLEqdCFd6]\n",
      "pp: [dental implants consist of a titanium post and a dental crown attached to its end urlurl ]\n",
      "\n",
      "tw: [Dental Implants Market Will Witness Enhanced Demand In Tooth (or teeth) Replacement Till 2024: Grand View Research,‚Ä¶ https://t.co/A2xeXiTBzo]\n",
      "pp: [dental implants market will witness enhanced demand in tooth or teeth replacement till numbers grand view research urlurl ]\n",
      "\n",
      "tw: [San Francisco Dental Implant Center updated its page,  issues may require a skilled oral surgeon. Jaw surgery, - https://t.co/VBiQSs1aFo]\n",
      "pp: [san francisco dental implant center updated its page issues may require a skilled oral surgeon jaw surgery urlurl ]\n",
      "\n",
      "tw: [Interested in dental implants and looking for a specialist practice for the treatment? We can help #Kent]\n",
      "pp: [interested in dental implants and looking for a specialist practice for the treatment questionmark we can help kent]\n",
      "\n",
      "tw: [@eugene_greeley_ Excellent source. Dentist of dental implants. In North York. https://t.co/FE6DMBZ08R Good results. Hope it helps Some words‚Ä¶ Take it easy.]\n",
      "pp: [ mentionname excellent source dentist of dental implants in north york urlurl ]\n",
      "\n",
      "tw: [Cosmetic Dentist Delray Beach, Boynton Beach, FL: Dental Implant Interview https://t.co/2GogRlJbzQ https://t.co/JtjFIQxODp]\n",
      "pp: [cosmetic dentist delray beach boynton beach fl dental implant interview urlurl ]\n",
      "\n",
      "tw: [#Dental implants - SWOT your marketing for better results: \n",
      "\n",
      "https://t.co/w8aWXk2SQr]\n",
      "pp: [ dental implants swot your marketing for better results urlurl ]\n",
      "\n",
      "tw: [Think a dental implant may be right for you? Read all about them here! https://t.co/hhEw2eXFqD]\n",
      "pp: [think a dental implant may be right for you questionmark read all about them here urlurl ]\n",
      "\n",
      "tw: [Dental implants are the best option for replacing missing teeth and we offer them at our practice. Contact us if you have any questions! https://t.co/3KlgSK2HbA]\n",
      "pp: [dental implants are the best option for replacing missing teeth and we offer them at our practice contact us if you have any questions urlurl ]\n",
      "\n",
      "tw: [Receive a Complimentary Dental Implant report by completing a short form at https://t.co/lQs09L2av0]\n",
      "pp: [receive a complimentary dental implant report by completing a short form at urlurl ]\n",
      "\n",
      "tw: [Dental Implant Drills Kit External Irrigation 8 PCS https://t.co/TaYh6UCDY0]\n",
      "pp: [dental implant drills kit external irrigation numbers pcs urlurl ]\n",
      "\n",
      "tw: [Dental implant restorations are designed to last a lifetime. https://t.co/ApfxPoBg9E]\n",
      "pp: [dental implant restorations are designed to last a lifetime urlurl ]\n",
      "\n",
      "tw: [In order to get a dental implant, you need a fully developed jaw. https://t.co/eZVCHk65Q0]\n",
      "pp: [in order to get a dental implant you need a fully developed jaw urlurl ]\n",
      "\n",
      "tw: [The Benefits of Dental Implants ... https://t.co/bAPJVDYqVN  #dentalimplants https://t.co/F6BWQ0NJTf]\n",
      "pp: [the benefits of dental implants urlurl ]\n",
      "\n",
      "tw: [Did you know that all our dental implant consultations are free?  Find out more..  https://t.co/onTA2313Gj\n",
      "#dentalimplants #freeconsultation https://t.co/FMIZtUdHkC]\n",
      "pp: [did you know that all our dental implant consultations are free questionmark find out more urlurl dentalimplants freeconsultation urlurl ]\n",
      "\n",
      "tw: [RT @SHARKSURVIVOR1: @debilu2 @thehill I‚Äôd like it to pay for my dental implants, my eye care, my office visits. Sex is a choice. Prevent yo‚Ä¶]\n",
      "pp: [rt mentionname mentionname mentionname i d like it to pay for my dental implants my eye care my office visits sex is a choice prevent yo ]\n",
      "\n",
      "tw: [Saw this ad today. Trying to imagine how painful it must be to get a $125 dental implant. https://t.co/u7KUajK81U]\n",
      "pp: [saw this ad today trying to imagine how painful it must be to get a moneymark numbers dental implant urlurl ]\n",
      "\n",
      "tw: [RT @drloev: What's involved in getting a dental implant?\n",
      "#EdwardLLoevDMD\n",
      "#DentistSanFranciscoCA\n",
      "#BestDentist https://t.co/oY8mT7DYFD]\n",
      "pp: [rt mentionname what s involved in getting a dental implant questionmark edwardlloevdmd dentistsanfranciscoca bestdentist urlurl ]\n",
      "\n",
      "tw: [#Win a $38,000 #G4ImplantSolution Dental Implant Smile Makeover #4Implants | Enter Here &gt;&gt;&gt;&gt; https://t.co/VMW0bU49aV https://t.co/Am4A0sIlJj]\n",
      "pp: [ win a moneymark numbers numbers g numbers implantsolution dental implant smile makeover numbers implants enter here urlurl ]\n",
      "\n",
      "tw: [RT @Shrympp: Revolutionary Discovery: Goodbye Dental Implants, Grow Your Own Teeth In Just 9 Weeks! https://t.co/9LDW7AzW9l]\n",
      "pp: [rt mentionname revolutionary discovery goodbye dental implants grow your own teeth in just numbers weeks urlurl ]\n",
      "\n",
      "tw: [Dr. Jaqueline Subka Now Offers Computer Guided Dental Implants in Thousand Oaks, CA https://t.co/3XYpIwTmSt]\n",
      "pp: [dr jaqueline subka now offers computer guided dental implants in thousand oaks ca urlurl ]\n",
      "\n",
      "tw: [Dental implants can help you live a richer, fuller life as you get older!\n",
      "https://t.co/Pd40AvevPl]\n",
      "pp: [dental implants can help you live a richer fuller life as you get older urlurl ]\n",
      "\n",
      "tw: [Click-Tite Dentures Dental Implants Dentures Dublin dentures a new smile - https://t.co/B6X6xLm9HW]\n",
      "pp: [click tite dentures dental implants dentures dublin dentures a new smile urlurl ]\n",
      "\n",
      "tw: [Benefits Offered by Dental Implant Services in Ann Arbor, MI - https://t.co/EA4NwslLqy]\n",
      "pp: [benefits offered by dental implant services in ann arbor mi urlurl ]\n",
      "\n",
      "tw: [Affordable Dental Implants Alton IL - YouTube https://t.co/Gis0Vvhn5W]\n",
      "pp: [affordable dental implants alton il youtube urlurl ]\n",
      "\n",
      "tw: [Straumann Dental Implant SCS Driver Torque free ship https://t.co/kizTdpMIk8 https://t.co/bYD166lAcy]\n",
      "pp: [straumann dental implant scs driver torque free ship urlurl ]\n",
      "\n",
      "tw: [Global Dental Implants Market 2018-2025 | Analysis by Major Manufacturers, Technology, Trends, Growth, Insights, Analysis and Future Forecast Report https://t.co/8AGtIdDwAN]\n",
      "pp: [global dental implants market numbers numbers analysis by major manufacturers technology trends growth insights analysis and future forecast report urlurl ]\n",
      "\n",
      "tw: [Dental Implants Market Is Anticipated To Register A Healthy CAGR Of Around 7.7% From 2016 To 2024: Grand View Research, Inc. https://t.co/9cLxW23cGd https://t.co/3jdVdswTF7]\n",
      "pp: [dental implants market is anticipated to register a healthy cagr of around numbers numbers from numbers to numbers grand view research inc urlurl ]\n",
      "\n",
      "tw: [RT @evlazareva61384: Mini Dental Implant Information Dayton, OH #dental #implants #information https://t.co/YaiyUKOEd2]\n",
      "pp: [rt mentionname mini dental implant information dayton oh dental implants information urlurl ]\n",
      "\n",
      "tw: [#how to afford dental implants forensic science articles]\n",
      "pp: [ how to afford dental implants forensic science articles]\n",
      "\n",
      "tw: [Save 42% on the Dental Implant and Crown Treatment, London Suitable for ages 18+ Redeemable‚Ä¶ https://t.co/81kvBJK1Js]\n",
      "pp: [save numbers on the dental implant and crown treatment london suitable for ages numbers redeemable urlurl ]\n",
      "\n",
      "tw: [RT @MrG_Picks: https://t.co/GCdEnrUq5k Dental Implants Specialists New York New Jersey NY, NJ, Manhattan, NYC, Nutley, Long Island, Queens]\n",
      "pp: [rt mentionname urlurl ]\n",
      "\n",
      "tw: [The Dental Implant Team at PermaDontics Welcomes Prosthodontist Dr. Ben Javid https://t.co/RTH2ar6B8N]\n",
      "pp: [the dental implant team at permadontics welcomes prosthodontist dr ben javid urlurl ]\n",
      "\n",
      "tw: [Dental Implant 101: All You Need To Know About Tooth Implants\n",
      "https://t.co/AJ7cnSNwzb]\n",
      "pp: [dental implant numbers all you need to know about tooth implants urlurl ]\n",
      "\n",
      "tw: [Out of 478 local residents Rebecca Hillman is the recipient of the Alexandria Oral Surgery &amp; Dental Implant Center‚Äôs new Smile Again Program. Hillman will receive life changing dental surgery costing close to $50,000! https://t.co/tS6Y83G3ql]\n",
      "pp: [out of numbers local residents rebecca hillman is the recipient of the alexandria oral surgery dental implant center s new smile again program hillman will receive life changing dental surgery costing close to moneymark numbers numbers urlurl ]\n",
      "\n",
      "tw: [I liked a @YouTube video https://t.co/JqyVyt4dKA full dental implants | FULL MOUTH REHABILITATION WITH IMPLANT]\n",
      "pp: [i liked a mentionname video urlurl ]\n",
      "\n",
      "tw: [I always pictured trump more of a dental implant guy but lmaoo, get some fixodent.]\n",
      "pp: [i always pictured trump more of a dental implant guy but lmaoo get some fixodent ]\n",
      "\n",
      "tw: [RT @imoyse: Dental Implants Explained [Infographic] https://t.co/L2bVsLbGiJ https://t.co/bZej9SpS2h]\n",
      "pp: [rt mentionname dental implants explained infographic urlurl ]\n",
      "\n",
      "tw: [RT @ClearChoice: Why not get all the information about dental implants? Schedule a free consultation to see if they are right for you https‚Ä¶]\n",
      "pp: [rt mentionname why not get all the information about dental implants questionmark schedule a free consultation to see if they are right for you https ]\n",
      "\n",
      "tw: [RT @HowardFarran: 69% of adults ages 35-44 have lost at least one permanent tooth. A dental implant can help to restore your smile! https:/‚Ä¶]\n",
      "pp: [rt mentionname numbers of adults ages numbers numbers have lost at least one permanent tooth a dental implant can help to restore your smile https ]\n",
      "\n",
      "tw: [Greeley Mini Dental Implants Review 970-785-8009 Affordable Dental Implants Greeley CO https://t.co/OD8MYo902p]\n",
      "pp: [greeley mini dental implants review numbers numbers numbers affordable dental implants greeley co urlurl ]\n",
      "\n",
      "tw: [AAOMS Subcommittee Honored for Advancing Dental Implant Education ‚Äì Markets Insider https://t.co/zURi33BnTh]\n",
      "pp: [aaoms subcommittee honored for advancing dental implant education markets insider urlurl ]\n",
      "\n",
      "tw: [Dental implants done right first time https://t.co/Tsco19pbf0]\n",
      "pp: [dental implants done right first time urlurl ]\n",
      "\n",
      "tw: [Smile Again With Dental Implants From PermaDontics https://t.co/86tSyB2TH6]\n",
      "pp: [smile again with dental implants from permadontics urlurl ]\n",
      "\n",
      "tw: [Get you dental implants from the experts at https://t.co/6VL9nmylLC \n",
      "#DentalCare #MichiganDentist]\n",
      "pp: [get you dental implants from the experts at urlurl dentalcare michigandentist]\n",
      "\n",
      "tw: [Caring for Dental Implants: What‚Äôs Different? https://t.co/8iniIuE7w2 #atowndental https://t.co/tOjSq2jXlt]\n",
      "pp: [caring for dental implants what s different questionmark urlurl ]\n",
      "\n",
      "tw: [You need a dental implant, but you are reluctant to get started. We understand that there is some hesitation and life gets in the way. However, here are a list of reasons as to why you... https://t.co/h403K8UnHc]\n",
      "pp: [you need a dental implant but you are reluctant to get started we understand that there is some hesitation and life gets in the way however here are a list of reasons as to why you urlurl ]\n",
      "\n",
      "tw: [Porter Ranch Dentists Dental Implants Cosmetic Dentistry Services Launched via Latest press ... https://t.co/lxPlk6uVpQ]\n",
      "pp: [porter ranch dentists dental implants cosmetic dentistry services launched via latest press urlurl ]\n",
      "\n",
      "tw: [Cosmetic Dentist Delray Beach, Boynton Beach, FL, Interviewed on Dental Implant Solutions https://t.co/EJ4EcC3x7o https://t.co/SsncufGJPm]\n",
      "pp: [cosmetic dentist delray beach boynton beach fl interviewed on dental implant solutions urlurl ]\n",
      "\n",
      "tw: [RT @NDentalAlliance: RT:@BogerDental: You need to know the truth about dental implants. https://t.co/oCsNLolyN6\n",
      "\n",
      "#mn #dentists]\n",
      "pp: [rt mentionname rt mentionname you need to know the truth about dental implants urlurl mn dentists]\n",
      "\n",
      "tw: [RT&gt;@ArtLabDentistry: A Picture Perfect Smile With All-On-4¬Æ Dental Implants https://t.co/EqojGpKS81 #prosthodontist #LAdentist #smile https://t.co/y5palTq6E5]\n",
      "pp: [rt mentionname a picture perfect smile with all on numbers dental implants urlurl ]\n",
      "\n",
      "tw: [Smoking is not good for any part of the body, but what effect does it have on dental implant success? Find out here! https://t.co/SjyQ8QLoMZ]\n",
      "pp: [smoking is not good for any part of the body but what effect does it have on dental implant success questionmark find out here urlurl ]\n",
      "\n",
      "tw: [Basal Implants, SwissDesign.\n",
      "Advantages of the Immediate Loading Basal Dental Implants ‚Äì prosthesis is fixed... https://t.co/Ectl3xhsS2]\n",
      "pp: [basal implants swissdesign advantages of the immediate loading basal dental implants prosthesis is fixed urlurl ]\n",
      "\n",
      "tw: [THIS IS PROBABLY SO PAINFUL...\n",
      "\n",
      "But I need a few dental implants to compliment my eight restorations... haha. https://t.co/Y9Frtvzn67]\n",
      "pp: [this is probably so painful but i need a few dental implants to compliment my eight restorations haha urlurl ]\n",
      "\n",
      "tw: [RT @MrG_Picks: https://t.co/bFrYTHpLPp  Dental Implant Technology That You Can Trust]\n",
      "pp: [rt mentionname urlurl ]\n",
      "\n",
      "tw: [What Is All On 4 Dental Implants?\n",
      "All-on-4 dental implants are specifically designed for people in need of full upper and/or lower restorations to treat failing teeth or replace removable dentures.\n",
      "https://t.co/jMfxQfxwZp]\n",
      "pp: [what is all on numbers dental implants questionmark all on numbers dental implants are specifically designed for people in need of full upper and or lower restorations to treat failing teeth or replace removable dentures urlurl ]\n",
      "\n",
      "tw: [Dental Implants and Bridges in Safdarjung Enclave Delhi..For more info visit https://t.co/bahhhCWvII https://t.co/DF8zIBD9sH]\n",
      "pp: [dental implants and bridges in safdarjung enclave delhi for more info visit urlurl ]\n",
      "\n",
      "tw: [Stem Cell Dental Implants Grow New Teeth In 2 Months! https://t.co/rUXj9Wmx4n]\n",
      "pp: [stem cell dental implants grow new teeth in numbers months urlurl ]\n",
      "\n",
      "tw: [RT @__YNA__: Webinar about: (Avoiding and managing dental implant complications) by Sclar, Thursday Oct 26\n",
      "Registration at:‚Ä¶ ]\n",
      "pp: [rt mentionname webinar about avoiding and managing dental implant complications by sclar thursday oct numbers registration at ]\n",
      "\n",
      "tw: [\"The Revolutionary All-On-4 Dental Implant\" https://t.co/MmKUfLxBVa  #dentalimplants #fosi https://t.co/kB6g2MBoyP]\n",
      "pp: [ the revolutionary all on numbers dental implant urlurl ]\n",
      "\n",
      "tw: [Have you heard of mini implants? Theyre affordable dental implants! Learn more: https://t.co/jPBQCk2Sct]\n",
      "pp: [have you heard of mini implants questionmark theyre affordable dental implants learn more urlurl ]\n",
      "\n",
      "tw: [Premium Dental Implants Las Vegas https://t.co/w8JHTLtzzV #teethinaday]\n",
      "pp: [premium dental implants las vegas urlurl ]\n",
      "\n",
      "tw: [Time for Dr. Burress.\n",
      "Problems with 10yr old crown/dental implant. Ughü§ëüí∏ https://t.co/RFMWeKWgdu]\n",
      "pp: [time for dr burress problems with numbers yr old crown dental implant ugh urlurl ]\n",
      "\n",
      "tw: [The reviews are in and our Dental Implant Seminars are a hit! Don't miss out on these informative seminars that... https://t.co/W5Cdqld4jz]\n",
      "pp: [the reviews are in and our dental implant seminars are a hit don t miss out on these informative seminars that urlurl ]\n",
      "\n",
      "tw: [Dental Implants and Prosthetics #Cosmetic Dentistry #Cosmetic Dentist #Dental Implants https://t.co/kow410CmjJ https://t.co/qb3W6NU1Ml]\n",
      "pp: [dental implants and prosthetics cosmetic dentistry cosmetic dentist dental implants urlurl ]\n",
      "\n",
      "tw: [Facts About Dental Implants in Nassau County NY - https://t.co/pQ99hSvcVn https://t.co/pQ99hSvcVn]\n",
      "pp: [facts about dental implants in nassau county ny urlurl ]\n",
      "\n",
      "tw: [Two swiss firms partner to develop ceramic components for dental implant systems https://t.co/LG6XcuDnnw]\n",
      "pp: [two swiss firms partner to develop ceramic components for dental implant systems urlurl ]\n",
      "\n",
      "tw: [Registration Open 20162017 The New Dental Implant Continuum A 5 Module Comprehensive University B: Thu,‚Ä¶ https://t.co/qN9FapKVsn #Mesquite]\n",
      "pp: [registration open numbers the new dental implant continuum a numbers module comprehensive university b thu urlurl ]\n",
      "\n",
      "tw: [X 100 Dental spiral kit of 5 parts - All in one for dental implant hex abutment  https://t.co/opPjYktCJq https://t.co/VndnDt6Ojl]\n",
      "pp: [x numbers dental spiral kit of numbers parts all in one for dental implant hex abutment urlurl ]\n",
      "\n",
      "tw: [Whats involved in getting a dental implant?\n",
      "\n",
      "Dental implants are increasingly popular as a way to replace missing or damaged teeth. https://t.co/8s9lqzwvws\n",
      "\n",
      "#MichaelDMeshnickDDSPC #DentistWappingersFalls #ImplantDentistNY #dentalimplants]\n",
      "pp: [whats involved in getting a dental implant questionmark dental implants are increasingly popular as a way to replace missing or damaged teeth urlurl michaeldmeshnickddspc dentistwappingersfalls implantdentistny dentalimplants]\n",
      "\n",
      "tw: [Caring Monterey, CA dental implant dentist¬†¬† ¬†¬†https://t.co/5VXKvbmufK]\n",
      "pp: [caring monterey ca dental implant dentist urlurl ]\n",
      "\n",
      "tw: [Affordable Dental Implants: #affordable dental insurance\n",
      "#Affordable Dental Implants Without the N... https://t.co/3Bc9a70X03 #insurance]\n",
      "pp: [affordable dental implants affordable dental insurance affordable dental implants without the n urlurl ]\n",
      "\n",
      "tw: [So far today I've gotten emails about dental implants and senior living apartments... I get it. I'm old.]\n",
      "pp: [so far today i ve gotten emails about dental implants and senior living apartments i get it i m old ]\n",
      "\n",
      "tw: [RT @DowneyDentistry: Dental implants can greatly improve your smile! Here's a Free Guide About All on 4 Dental Implants‚Ä¶ ]\n",
      "pp: [rt mentionname dental implants can greatly improve your smile here s a free guide about all on numbers dental implants ]\n",
      "\n",
      "tw: [Dental Implant FAQs Dental Implants West San Jose Interested in implants? For years, dental implants have swept... https://t.co/ZI9tFoJIK0]\n",
      "pp: [dental implant faqs dental implants west san jose interested in implants questionmark for years dental implants have swept urlurl ]\n",
      "\n",
      "tw: [RT @villanovadental: Dental implants are an excellent option to replace missing teeth, providing several advantages over other treatments‚Äîh‚Ä¶]\n",
      "pp: [rt mentionname dental implants are an excellent option to replace missing teeth providing several advantages over other treatments h ]\n",
      "\n",
      "tw: [Teeth In a day dental implant testimonial | same day dental Implants | dr rajat sachdeva: https://t.co/1qrVdRWgBZ via @YouTube]\n",
      "pp: [teeth in a day dental implant testimonial same day dental implants dr rajat sachdeva urlurl ]\n",
      "\n",
      "tw: [Allow The Dentist Near Herald Square To Perform Dental Implants https://t.co/nwPeag0U8k]\n",
      "pp: [allow the dentist near herald square to perform dental implants urlurl ]\n",
      "\n",
      "tw: [#how much does it cost for dental implants storage hilo]\n",
      "pp: [ how much does it cost for dental implants storage hilo]\n",
      "\n",
      "tw: [Cleaning is essential even for those who have dental implants, as it helps to maintain the healthy gums. https://t.co/oiZCViEASF https://t.co/HebURhwgRz]\n",
      "pp: [cleaning is essential even for those who have dental implants as it helps to maintain the healthy gums urlurl ]\n",
      "\n",
      "tw: [Global Dental Implant Market Analysis, Share, Trends and Forecast by 2022- Market Research Report 2017 https://t.co/de3cKQAYow https://t.co/UnEzf2OKcx]\n",
      "pp: [global dental implant market analysis share trends and forecast by numbers market research report numbers urlurl ]\n",
      "\n",
      "tw: [Dental Implant Market Is Growing With The Technical Advancement, Dental Implants Industry to Project CAGR of more than 7% https://t.co/JWmywpTmbH https://t.co/PXyd1iOzLO]\n",
      "pp: [dental implant market is growing with the technical advancement dental implants industry to project cagr of more than numbers urlurl ]\n",
      "\n",
      "tw: [#air dust cleaning how much do one day dental implants cost]\n",
      "pp: [ air dust cleaning how much do one day dental implants cost]\n",
      "\n",
      "tw: [Why choosing dental implants could be the best decision you have ever made visit https://t.co/h3ZvOOwwsn or CALL 01501 731 711 https://t.co/j0W6S7jxX1]\n",
      "pp: [why choosing dental implants could be the best decision you have ever made visit urlurl ]\n",
      "\n",
      "tw: [Missing teeth? Have you considered #Dental Implants https://t.co/KKTkx0lLMK #cornwall #wadebridge #smile https://t.co/xIeWKsTGp2]\n",
      "pp: [missing teeth questionmark have you considered dental implants urlurl ]\n",
      "\n",
      "tw: [Replace Missing Teeth or Secure Dental Devices With Dental Implants Cedar City UT - https://t.co/NG8JNKZIte https://t.co/NG8JNKZIte]\n",
      "pp: [replace missing teeth or secure dental devices with dental implants cedar city ut urlurl ]\n",
      "\n",
      "tw: [US Dental Implant System Surgical Brushless Motor W 20:1 Contra Angle Handpiece https://t.co/pTW9jz0H2t]\n",
      "pp: [us dental implant system surgical brushless motor w numbers numbers contra angle handpiece urlurl ]\n",
      "\n",
      "tw: [Learn about bone regeneration and find out why it is important in dental implant procedures.  #DentalImplants #ImplantDentist #FullMouth\n",
      "https://t.co/opSj2rNDDc]\n",
      "pp: [learn about bone regeneration and find out why it is important in dental implant procedures dentalimplants implantdentist fullmouth urlurl ]\n",
      "\n",
      "tw: [RT @fcollective: There have been more than 50,000 dental implants created by @Formlabs-printed surgical guides. https://t.co/InAhPCiIBl #Pr‚Ä¶]\n",
      "pp: [rt mentionname there have been more than numbers numbers dental implants created by mentionname printed surgical guides urlurl ]\n",
      "\n",
      "tw: [Cieplak Dental Excellence Invests in Imaging Technology for Reliable Dental Implant Treatment in La Plata, MD https://t.co/O4MllAZnxh]\n",
      "pp: [cieplak dental excellence invests in imaging technology for reliable dental implant treatment in la plata md urlurl ]\n",
      "\n",
      "tw: [Mind that gap! You can fill it in easily with dental implants. https://t.co/w6PiMKKgVp https://t.co/ly3BbqPwEn]\n",
      "pp: [mind that gap you can fill it in easily with dental implants urlurl ]\n",
      "\n",
      "tw: [RT @DentistryToday: The material can be used in making dental implants and in restoring tooth enamel.... https://t.co/lm6z2qYXjy]\n",
      "pp: [rt mentionname the material can be used in making dental implants and in restoring tooth enamel urlurl ]\n",
      "\n",
      "tw: [#voodoo computers for sale dental implants painful]\n",
      "pp: [ voodoo computers for sale dental implants painful]\n",
      "\n",
      "tw: [#dental implants teeth in a day platform step ladders]\n",
      "pp: [ dental implants teeth in a day platform step ladders]\n",
      "\n",
      "tw: [#alexander wang style what are dental implants made of]\n",
      "pp: [ alexander wang style what are dental implants made of]\n",
      "\n",
      "tw: [Should I have all of my teeth removed and have dental implants put in place of them? #health https://t.co/lsYcIKIF0B]\n",
      "pp: [should i have all of my teeth removed and have dental implants put in place of them questionmark health urlurl ]\n",
      "\n",
      "tw: [A history of #dental implants.  #dentist #TempleCity https://t.co/KaHF3psWgR https://t.co/YWdxG2OfHb]\n",
      "pp: [a history of dental implants dentist templecity urlurl ]\n",
      "\n",
      "tw: [RT @hapkidobigdad: #RejectedUsesForCandyCorn\n",
      "\n",
      "        Dental implants https://t.co/YJq0V7ZY6F]\n",
      "pp: [rt mentionname rejectedusesforcandycorn dental implants urlurl ]\n",
      "\n",
      "tw: [Don't let missing teeth get in the way of your appearance - dental implants are an excellent alternative! https://t.co/PZJxBkLLCc]\n",
      "pp: [don t let missing teeth get in the way of your appearance dental implants are an excellent alternative urlurl ]\n",
      "\n",
      "tw: [You can have a full, permanent smile with All-on-4¬Æ dental implants! Give Dental Care of Baltimore a call, or visi‚Ä¶ https://t.co/nu7FG4YpCH https://t.co/aKFvlPrUaJ]\n",
      "pp: [you can have a full permanent smile with all on numbers dental implants give dental care of baltimore a call or visi urlurl ]\n",
      "\n",
      "tw: [@KamalaHarris @cookiebandit44 making it so dental implants cost into the hundreds, not the thousands.   Making sure it does not cost into the thousands of dollars for a broken pinky.]\n",
      "pp: [ mentionname mentionname making it so dental implants cost into the hundreds not the thousands making sure it does not cost into the thousands of dollars for a broken pinky ]\n",
      "\n",
      "tw: [Dental implants are the alternative to loose dentures. Call us today and see if dental implants are right for you!\n",
      "(301) 654 - 7070 https://t.co/nQgxavfBWL]\n",
      "pp: [dental implants are the alternative to loose dentures call us today and see if dental implants are right for you numbers numbers numbers urlurl ]\n",
      "\n",
      "tw: [Once I get all that stuff done I can fully focus on my savings and the savings for my dental implants hopefully I raise the money]\n",
      "pp: [once i get all that stuff done i can fully focus on my savings and the savings for my dental implants hopefully i raise the money]\n",
      "\n",
      "tw: [dental implant group   https://t.co/xKwfLfcxKc]\n",
      "pp: [dental implant group urlurl ]\n",
      "\n",
      "tw: [Another positive dental implant testimonial!\n",
      "\n",
      "‚ÄúMy implant experience with Dr. Rapoport and his staff was... https://t.co/lRblWElPvT]\n",
      "pp: [another positive dental implant testimonial my implant experience with dr rapoport and his staff was urlurl ]\n",
      "\n",
      "tw: [Dental implants won't just restore your smile, they will restore your confidence too! See our testimonials here https://t.co/3mj24kUIjf https://t.co/fvcVHJ1za5]\n",
      "pp: [dental implants won t just restore your smile they will restore your confidence too see our testimonials here urlurl ]\n",
      "\n",
      "tw: [Dental Implants For Older Adults ‚Äì What Are The Benefits? https://t.co/fg0rsmU9Mw]\n",
      "pp: [dental implants for older adults what are the benefits questionmark urlurl ]\n",
      "\n",
      "tw: [Dental Implant Peeps Pynadath George has found something life saving this is not just advertising- no commercial... https://t.co/EvA4zcgMxP]\n",
      "pp: [dental implant peeps pynadath george has found something life saving this is not just advertising no commercial urlurl ]\n",
      "\n",
      "tw: [‚ÄúARE YOU A CANDIDATE FOR DENTAL IMPLANTS?‚Äù https://t.co/6MqIrkCgwn #dentalimplants #FOSI #drnavidsenehi https://t.co/wo7K7EwtXz]\n",
      "pp: [ are you a candidate for dental implants questionmark urlurl ]\n",
      "\n",
      "tw: [Global Dental Implants and Prosthetics Sales Market Report 2017 https://t.co/wPRA9sOGZ4 #global #2017]\n",
      "pp: [global dental implants and prosthetics sales market report numbers urlurl ]\n",
      "\n",
      "tw: [Pricey dental implants often best but insurance rarely pays https://t.co/yb1xxjJGMP]\n",
      "pp: [pricey dental implants often best but insurance rarely pays urlurl ]\n",
      "\n",
      "tw: [Mandibular regeneration after immediate load dental implant in a periodontitis patient: A clinical and‚Ä¶ https://t.co/VsG6zDopGi]\n",
      "pp: [mandibular regeneration after immediate load dental implant in a periodontitis patient a clinical and urlurl ]\n",
      "\n",
      "tw: [Another busy dental implant day at Dollar Dental Care. Christine has been keeping Bobby organised and on track!... https://t.co/FnjhyllrpH]\n",
      "pp: [another busy dental implant day at dollar dental care christine has been keeping bobby organised and on track urlurl ]\n",
      "\n",
      "tw: [Are you interested in #dentalimplants? Watch this video filled with stories from actual dental implant patients. https://t.co/zqwvvXogxj]\n",
      "pp: [are you interested in dentalimplants questionmark watch this video filled with stories from actual dental implant patients urlurl ]\n",
      "\n",
      "tw: [Braces, Invisalign, a complete smile makeover or a full mouth restoration with dental implants, we can help! https://t.co/q1ajOcs60R]\n",
      "pp: [braces invisalign a complete smile makeover or a full mouth restoration with dental implants we can help urlurl ]\n",
      "\n",
      "tw: [Prevent facial collapse and bone loss with dental implants! https://t.co/Bc39rX5bjF https://t.co/vNREFW2XLG]\n",
      "pp: [prevent facial collapse and bone loss with dental implants urlurl ]\n",
      "\n",
      "tw: [We have a range of dental implant services whether it be just one tooth missing or a few https://t.co/VKyt65QQpy #DentalImplant #Cheshire #Chester #teeth https://t.co/sTAbDny8Fk]\n",
      "pp: [we have a range of dental implant services whether it be just one tooth missing or a few urlurl ]\n",
      "\n",
      "tw: [Dental implant surgery is a procedure that replaces tooth roots with metal, screw-like posts and replaces damaged... https://t.co/TnrCLtwYj6]\n",
      "pp: [dental implant surgery is a procedure that replaces tooth roots with metal screw like posts and replaces damaged urlurl ]\n",
      "\n",
      "tw: [Correct Missing Teeth with Dental Implants Your teeth serve‚Ä¶ #ChampionsGate https://t.co/iX7L4SJLoL https://t.co/S41QDWztje]\n",
      "pp: [correct missing teeth with dental implants your teeth serve championsgate urlurl ]\n",
      "\n",
      "tw: [RT @farbodsaraf: This is how a dental implant is installed https://t.co/WOCgsIpby7 #HowThingsWork]\n",
      "pp: [rt mentionname this is how a dental implant is installed urlurl ]\n",
      "\n",
      "tw: [Dental Implants Market To Witness Enhanced Acceptance In Procedures By Dentists &amp; Patients Till 2024 | Million‚Ä¶ https://t.co/933pd8cuP4 https://t.co/oOw0yI1taY]\n",
      "pp: [dental implants market to witness enhanced acceptance in procedures by dentists patients till numbers million urlurl ]\n",
      "\n",
      "tw: [Dental Implant System Brushless +Cordless  Reduction 16:1 Contra AngleHandpiece https://t.co/e0jZBGtHSG]\n",
      "pp: [dental implant system brushless cordless reduction numbers numbers contra anglehandpiece urlurl ]\n",
      "\n",
      "tw: [Presenting Zirconia dental implant system project and MABB Bioengineering &amp; Biomaterial Company, scaling up worldwide to generate triple impact to the‚Ä¶ https://t.co/9ScWxMakOP]\n",
      "pp: [presenting zirconia dental implant system project and mabb bioengineering biomaterial company scaling up worldwide to generate triple impact to the urlurl ]\n",
      "\n",
      "tw: [How The Dental Implants Function? \n",
      "https://t.co/RKiQdJEwBH  #teeth #health #smile #dentist]\n",
      "pp: [how the dental implants function questionmark urlurl ]\n",
      "\n",
      "tw: [RT @Zagliner: Here's what new dental implants should cost... https://t.co/xiJvnt9kT3]\n",
      "pp: [rt mentionname here s what new dental implants should cost urlurl ]\n",
      "\n",
      "tw: [RT @DrHadwenTrust: Did you know #animals are used in #dental implant testing? This work we're funding will help replace them:‚Ä¶ ]\n",
      "pp: [rt mentionname did you know animals are used in dental implant testing questionmark this work we re funding will help replace them ]\n",
      "\n",
      "tw: [Global Dental Implants Market - Increasing Number of Cosmetic Dentistry Treatments to Drive Growth | Technavio https://t.co/E8YEAKHeZu https://t.co/wMJrOVoCP5]\n",
      "pp: [global dental implants market increasing number of cosmetic dentistry treatments to drive growth technavio urlurl ]\n",
      "\n",
      "tw: [What are #dental implants? https://t.co/QAoB5kcb0G #dentist #Lichfield https://t.co/aoTic9vDPC]\n",
      "pp: [what are dental implants questionmark urlurl ]\n",
      "\n",
      "tw: [How dental implants can give you a better smile\n",
      "#AestheticGeneralDentistryPA\n",
      "#DentistOrlandoFL\n",
      "#BestDentist\n",
      "https://t.co/dshDGE188u]\n",
      "pp: [how dental implants can give you a better smile aestheticgeneraldentistrypa dentistorlandofl bestdentist urlurl ]\n",
      "\n",
      "tw: [RT @anaRocch: Get the smile of your dreams with the best Dental Implants in Las Vegas https://t.co/awlE14ANmT #DentalImplants #Allon4 #Smil‚Ä¶]\n",
      "pp: [rt mentionname get the smile of your dreams with the best dental implants in las vegas urlurl ]\n",
      "\n",
      "tw: [100 Straight Abutment Wide Platform, For Dental Implant Internal Hex https://t.co/TAXLpSAqdT https://t.co/uSpmWaHExH]\n",
      "pp: [ numbers straight abutment wide platform for dental implant internal hex urlurl ]\n",
      "\n",
      "tw: [3 Ways that Dental Implants Can Help You to Look Great https://t.co/RBXyTZZJfq https://t.co/W7tuAYBjLF]\n",
      "pp: [ numbers ways that dental implants can help you to look great urlurl ]\n",
      "\n",
      "tw: [RT @summerandrews19: Direct Dental offers many dentistry services from benefits of dental implants, cosmetic dentistry, teeth cleanings‚Ä¶ ]\n",
      "pp: [rt mentionname direct dental offers many dentistry services from benefits of dental implants cosmetic dentistry teeth cleanings ]\n",
      "\n",
      "tw: [New AZDENT Dental Implant System LED Screen Surgical Brushless Motor  https://t.co/kZqR2Tqz9q https://t.co/4P2MmYHvPv]\n",
      "pp: [new azdent dental implant system led screen surgical brushless motor urlurl ]\n",
      "\n",
      "tw: [#Dental Implants #Newbury https://t.co/FCShwfTMLv Missing teeth? Let us explain how to solve the problem. https://t.co/KhSposxg4R]\n",
      "pp: [ dental implants newbury urlurl ]\n",
      "\n",
      "tw: [Teeth Implant in Delhi: A dental implant is strong, stable and it restores a lost tooth so that it looks, feels,‚Ä¶ https://t.co/2I2TdZSKhs https://t.co/UdZKaypJFT]\n",
      "pp: [teeth implant in delhi a dental implant is strong stable and it restores a lost tooth so that it looks feels urlurl ]\n",
      "\n",
      "tw: [With proper care, dental implants can last a lifetime. https://t.co/gKe9LACMtB]\n",
      "pp: [with proper care dental implants can last a lifetime urlurl ]\n",
      "\n",
      "tw: [What Are Dental Implants?\n",
      "https://t.co/a0yx9bSFtM\n",
      "\n",
      "#dentalimplants #cosmeticdentistry #Seattle https://t.co/dBQflrkyJd]\n",
      "pp: [what are dental implants questionmark urlurl dentalimplants cosmeticdentistry seattle urlurl ]\n",
      "\n",
      "tw: [From the Blog: What Factors Contribute the the Long-Term Success of Dental Implants? https://t.co/w56S2DU3Ta]\n",
      "pp: [from the blog what factors contribute the the long term success of dental implants questionmark urlurl ]\n",
      "\n",
      "tw: [Be confident with a SMILE you have always dreamed about! Florida Dental Implants New Teeth Now is back to show us... https://t.co/5yisiA7R4i]\n",
      "pp: [be confident with a smile you have always dreamed about florida dental implants new teeth now is back to show us urlurl ]\n",
      "\n",
      "tw: [RT @TimeToBecome1: Find a Specialist to Handle your Dental Implants at Affordable Price https://t.co/zyLJZqSeui]\n",
      "pp: [rt mentionname find a specialist to handle your dental implants at affordable price urlurl ]\n",
      "\n",
      "tw: [@ESCtips_Gavster I thought it was a dental implant!]\n",
      "pp: [ mentionname i thought it was a dental implant ]\n",
      "\n",
      "tw: [Get Dental Implants in a Day from ‚ÄòNew Teeth in One Day Dental Clinics‚Äô https://t.co/0xjNhCLxz0]\n",
      "pp: [get dental implants in a day from new teeth in one day dental clinics urlurl ]\n",
      "\n",
      "tw: [Melbourne dental implant failure and how to avoid it https://t.co/620oDESzyg https://t.co/3qKZO6EVVn]\n",
      "pp: [melbourne dental implant failure and how to avoid it urlurl ]\n",
      "\n",
      "tw: [RT @iGreenMonk: No one knew she had a dental implant, until it came out in conversation.]\n",
      "pp: [rt mentionname no one knew she had a dental implant until it came out in conversation ]\n",
      "\n",
      "tw: [Allgood Family Dentistry Offers Permanent Alternative to Dentures in Midlothian, VA; All-on-4¬Æ Dental Implants https://t.co/FWF7F80bvV]\n",
      "pp: [allgood family dentistry offers permanent alternative to dentures in midlothian va all on numbers dental implants urlurl ]\n",
      "\n",
      "tw: [Lost confidence in your smile because of missing teeth? Consider dental implants #Kent]\n",
      "pp: [lost confidence in your smile because of missing teeth questionmark consider dental implants kent]\n",
      "\n",
      "tw: [Are you a candidate for dental implants? Some factors such as the health of your jaw bone can affect your... https://t.co/kCnfsSJtPc]\n",
      "pp: [are you a candidate for dental implants questionmark some factors such as the health of your jaw bone can affect your urlurl ]\n",
      "\n",
      "tw: [RT @TheQuietPsycho: Dental implants...palate reconstruction...the flashbacks haunt me...\n",
      "\n",
      "\"Put the Toblerone in the freezer\" she said...\"it‚Ä¶]\n",
      "pp: [rt mentionname dental implants palate reconstruction the flashbacks haunt me put the toblerone in the freezer she said it ]\n",
      "\n",
      "tw: [This ladyon my tv saying dental implants don‚Äôt hurt is a lying liar who lies.]\n",
      "pp: [this ladyon my tv saying dental implants don t hurt is a lying liar who lies ]\n",
      "\n",
      "tw: [Check out Dental Implant. 12 pcs Bone Expander Sinus Lift Kit.   https://t.co/huPOcCUSiT via @eBay]\n",
      "pp: [check out dental implant numbers pcs bone expander sinus lift kit urlurl ]\n",
      "\n",
      "tw: [$ALSGD SpineGuard and Adin Dental Implant Systems announce \n",
      "https://t.co/7JOuTahx6H #SpineGuard #Adin #Dental #WSJ #nytimes #reuters #bloomberg #thestreet #jimmyfallon #forbes #nasdaq #chicago #digitalmarkiting #ihub #newyork  #social #networking #business  #cnn #bet #foxnews https://t.co/q7mCJfWlka]\n",
      "pp: [ moneymark alsgd spineguard and adin dental implant systems announce urlurl ]\n",
      "\n",
      "tw: [Dental Implants in Hampshire: Easier Than You Think https://t.co/dd7nb9Oxoe]\n",
      "pp: [dental implants in hampshire easier than you think urlurl ]\n",
      "\n",
      "tw: [The Future of Dental Implants - https://t.co/UszWjpp0tr]\n",
      "pp: [the future of dental implants urlurl ]\n",
      "\n",
      "tw: [@cccarle I got a dental implant and I can't have anything chewy or crunchy until it heals in 3 monthsüò≠]\n",
      "pp: [ mentionname i got a dental implant and i can t have anything chewy or crunchy until it heals in numbers months sadFace ]\n",
      "\n",
      "tw: [RT @dose_tech: Dental Implants Fort Collins - Tim Owens DDS https://t.co/ky9N6Bdn99]\n",
      "pp: [rt mentionname dental implants fort collins tim owens dds urlurl ]\n",
      "\n",
      "tw: [#WatkinDentalAssociates 5 Reasons Why Dental Implants Are Better than Bridges and Dentures. Read Blog: https://t.co/P4DI6Zq41z]\n",
      "pp: [ watkindentalassociates numbers reasons why dental implants are better than bridges and dentures read blog urlurl ]\n",
      "\n",
      "tw: [Information about Single Tooth Dental Implants Article #OralHealth #dentalcare #implants #Chicago #Dentist https://t.co/7lpGk1KeAN]\n",
      "pp: [information about single tooth dental implants article oralhealth dentalcare implants chicago dentist urlurl ]\n",
      "\n",
      "tw: [Get Up-to-Date Info on Dental Implants in Australia | @scoopit https://t.co/ijQiEWrVpm]\n",
      "pp: [get up to date info on dental implants in australia mentionname urlurl ]\n",
      "\n",
      "tw: [Dentist San Diego, Encinitas, CA, Talks About Dental Implants in Interview https://t.co/NxuxQpeuJ0 https://t.co/NOj5zvUOcJ]\n",
      "pp: [dentist san diego encinitas ca talks about dental implants in interview urlurl ]\n",
      "\n",
      "tw: [RT @NibanezNorma: How to Find Low Cost Dental Implants #discount #dental #implants https://t.co/ndw55tm5Kc]\n",
      "pp: [rt mentionname how to find low cost dental implants discount dental implants urlurl ]\n",
      "\n",
      "tw: [RT @Millennial_Dems: .@ClearChoice we won't be smiling with your dental implants until you STOP ADVERTISING on Sean Hannity's show where‚Ä¶ ]\n",
      "pp: [rt mentionname mentionname we won t be smiling with your dental implants until you stop advertising on sean hannity s show where ]\n",
      "\n",
      "tw: [Teeth restored with dental implants can't get cavities! A replacement tooth, or crown, doesn't decay like a natural tooth, but you still must brush, floss and care for it and your surrounding natural teeth and gums in the same manner as natural teeth. #DentalImplants #Implants https://t.co/URyMMh3Kps]\n",
      "pp: [teeth restored with dental implants can t get cavities a replacement tooth or crown doesn t decay like a natural tooth but you still must brush floss and care for it and your surrounding natural teeth and gums in the same manner as natural teeth dentalimplants implants urlurl ]\n",
      "\n",
      "tw: [RT @Hesira_Med: Stop by #1703 @pdconf to learn more about #ossix and the new to Canada PTFE #biotex &amp; #Axis dental implant https://t.co/Otg‚Ä¶]\n",
      "pp: [rt mentionname stop by numbers mentionname to learn more about ossix and the new to canada ptfe biotex axis dental implant urlurl ]\n",
      "\n",
      "tw: [Training Dental Implant oleh Faith Cheong, Regional Product Manager Dentsply Implants di Cobra‚Ä¶ https://t.co/Ka8HL4KnBi]\n",
      "pp: [training dental implant oleh faith cheong regional product manager dentsply implants di cobra urlurl ]\n",
      "\n",
      "tw: [#FoothillsDentalCentre Maintenance and Care of Dental Implants. Read Blog: https://t.co/FaBJKO0Hin]\n",
      "pp: [ foothillsdentalcentre maintenance and care of dental implants read blog urlurl ]\n",
      "\n",
      "tw: [@larryellison I'm missing 9 teeth due to periodontal disease. Can you please help me have a better quality of life with dental implants ?]\n",
      "pp: [ mentionname i m missing numbers teeth due to periodontal disease can you please help me have a better quality of life with dental implants questionmark ]\n",
      "\n",
      "tw: [RT @MrG_Picks: https://t.co/Z1pAI4XCXG  Benefits of Full Mouth Dental Implants and Dentures]\n",
      "pp: [rt mentionname urlurl ]\n",
      "\n",
      "tw: [Read what to expect when it comes to the cost of dental implants, including what factors influence the final number: https://t.co/A2viRataXR]\n",
      "pp: [read what to expect when it comes to the cost of dental implants including what factors influence the final number urlurl ]\n",
      "\n",
      "tw: [RT @DentalClinicAMD: Many options exist for tooth replacement, but only Dental Implants actually look and feel like real teeth.\n",
      "\n",
      "With a #De‚Ä¶]\n",
      "pp: [rt mentionname many options exist for tooth replacement but only dental implants actually look and feel like real teeth with a de ]\n",
      "\n",
      "tw: [Missing tooth? we are here to help you. For best dental implants in Bangalore Call at +91 80 23011500 +91 94835... https://t.co/t6ZbizhnaO]\n",
      "pp: [missing tooth questionmark we are here to help you for best dental implants in bangalore call at numbers numbers numbers numbers numbers urlurl ]\n",
      "\n",
      "tw: [RT @Post_planner: https://t.co/i610fWVjmk - Dentist in Somerset West, Cape Town. Offering teeth whitening, dental implants, orthodontics an‚Ä¶]\n",
      "pp: [rt mentionname urlurl ]\n",
      "\n",
      "tw: [#ImplantDentistryandPeriodontics What's a Dental Implant Made Of?. Read Blog: https://t.co/DCoWjBzmEZ]\n",
      "pp: [ implantdentistryandperiodontics what s a dental implant made of questionmark read blog urlurl ]\n",
      "\n",
      "tw: [Dental Implants Cost Boulder CO (970) 785-6280 | Cost of dental implants in Boulder Colorado | Ch https://t.co/UN6dRLuM8l]\n",
      "pp: [dental implants cost boulder co numbers numbers numbers cost of dental implants in boulder colorado ch urlurl ]\n",
      "\n",
      "tw: [The Future of Your Smile is Here with Dental Implants https://t.co/DckzNI0zBd https://t.co/EoQSPNXdEa]\n",
      "pp: [the future of your smile is here with dental implants urlurl ]\n",
      "\n",
      "tw: [When dental implants are placed on the front teeth, restoring them to look natural and seamless‚Ä¶ https://t.co/xdAIgLLJeQ]\n",
      "pp: [when dental implants are placed on the front teeth restoring them to look natural and seamless urlurl ]\n",
      "\n",
      "tw: [How Much Does It Cost to Have Dental Implants Fitted? https://t.co/VOejmTTDJT #dentalhealth]\n",
      "pp: [how much does it cost to have dental implants fitted questionmark urlurl ]\n",
      "\n",
      "tw: [New Teeth in a Day: All-On-4 Dental Implants https://t.co/FOHBk8fuJK https://t.co/EjaYGa1fjB]\n",
      "pp: [new teeth in a day all on numbers dental implants urlurl ]\n",
      "\n",
      "tw: [#how do i add a signature to a pdf florida dental implants new teeth now]\n",
      "pp: [ how do i add a signature to a pdf florida dental implants new teeth now]\n",
      "\n",
      "tw: [I call BULLSH!T!!\n",
      "1ST: Mr. \"Billionaire\" Trump wouldn't have dentures, he'd have Dental Implants.\n",
      "2ND: Trump slurred ALL words. Loose dentures don't do that.\n",
      "3RD: Trump's continual SNIFFING is a big clue to DRUG use.\n",
      "4TH: IF it were dentures, why slur only at the END?\n",
      "#FakePOTUS! https://t.co/25oRDInlXE]\n",
      "pp: [i call bullsh t numbers st mr billionaire trump wouldn t have dentures he d have dental implants numbers nd trump slurred all words loose dentures don t do that numbers rd trump s continual sniffing is a big clue to drug use numbers th if it were dentures why slur only at the end questionmark fakepotus urlurl ]\n",
      "\n",
      "tw: [The fact is that dental implants are most permanent solution. To know more about dental implants you can contact us!\n",
      "\n",
      "0731 427 4561, 0731 427 4579\n",
      "https://t.co/giBm8w1X26 https://t.co/b2PJ1p3aaT]\n",
      "pp: [the fact is that dental implants are most permanent solution to know more about dental implants you can contact us numbers numbers numbers numbers numbers numbers urlurl ]\n",
      "\n",
      "tw: [RT @MollieS54378927: Honest Family Dental - dentist in South Austin specializing in Dental Implants, Crowns, Bridges #thiswebsite https://t‚Ä¶]\n",
      "pp: [rt mentionname honest family dental dentist in south austin specializing in dental implants crowns bridges thiswebsite https t ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tw: [Important information for anyone deciding on whether or not to get dental implants. https://t.co/s9Re3LzbV7]\n",
      "pp: [important information for anyone deciding on whether or not to get dental implants urlurl ]\n",
      "\n",
      "tw: [@cvpayne I got 10 percent discount on a dental implant for cash.]\n",
      "pp: [ mentionname i got numbers percent discount on a dental implant for cash ]\n",
      "\n",
      "tw: [Mini #Dental Implants in Glendale, AZ\n",
      "Our expert Prosthodontist can provide you the mini dental implants in Glendale. https://t.co/fHRbvTKAa3]\n",
      "pp: [mini dental implants in glendale az our expert prosthodontist can provide you the mini dental implants in glendale urlurl ]\n",
      "\n",
      "tw: [AZ Max Oral Surgery - Dental Implants https://t.co/sELZ79pnoc #azmax #oralsurgery #smiles #DentalImplants https://t.co/LG6oApEyoN]\n",
      "pp: [az max oral surgery dental implants urlurl ]\n",
      "\n",
      "tw: [RT https://t.co/H0amsWUTac THE USE OF STEM CELLS IN DENTAL IMPLANT SITE DEVELOPMENT #Dentist #dxb #Dubai #dentali‚Ä¶ https://t.co/YNERaORONV]\n",
      "pp: [rt urlurl ]\n",
      "\n",
      "tw: [How to Make the Decision to Get Dental Implants https://t.co/XEiPH7oRLv https://t.co/sLPxlZgSeJ]\n",
      "pp: [how to make the decision to get dental implants urlurl ]\n",
      "\n",
      "tw: [#area dodge dealers dental implant crown procedure]\n",
      "pp: [ area dodge dealers dental implant crown procedure]\n",
      "\n",
      "tw: [I'm worried dental implants are expensive. What are the alternatives and are they any good? #dentalimplants #Hove\n",
      "https://t.co/699ADKG1Sx]\n",
      "pp: [i m worried dental implants are expensive what are the alternatives and are they any good questionmark dentalimplants hove urlurl ]\n",
      "\n",
      "tw: [All #dental implants are performed onsite at our Sunshine Coast practice. https://t.co/WxkerbXkhV]\n",
      "pp: [all dental implants are performed onsite at our sunshine coast practice urlurl ]\n",
      "\n",
      "tw: [@realDonaldTrump. U can't afford dental implants? Oh, u can't deal w the pain. Talk about a pussy. #DentureDonald]\n",
      "pp: [ mentionname u can t afford dental implants questionmark oh u can t deal w the pain talk about a pussy denturedonald]\n",
      "\n",
      "tw: [Check out Drill Kit Dental Implant Instruments  https://t.co/qMEvGF4hs7 via @eBay]\n",
      "pp: [check out drill kit dental implant instruments urlurl ]\n",
      "\n",
      "tw: [Cost for dental implant can be confusing and intimidating. Check out our Dental implant cost break down. https://t.co/DJk5WbUcft]\n",
      "pp: [cost for dental implant can be confusing and intimidating check out our dental implant cost break down urlurl ]\n",
      "\n",
      "tw: [How Can Smoking Affect Dental Implants in Harley Street? https://t.co/Tv8pTtILLd https://t.co/WytGs7PsBg]\n",
      "pp: [how can smoking affect dental implants in harley street questionmark urlurl ]\n",
      "\n",
      "tw: [A dental implant is not only the best option to replace the look and feel of a missing tooth, it also restores overall jaw health. A missing tooth can lead to demineralization of the supporting jawbone. https://t.co/Xit5NKnRVp]\n",
      "pp: [a dental implant is not only the best option to replace the look and feel of a missing tooth it also restores overall jaw health a missing tooth can lead to demineralization of the supporting jawbone urlurl ]\n",
      "\n",
      "tw: [Dental Implant Procedure: What You Need To Know https://t.co/26KloARPXZ https://t.co/npz5pU9Eqs]\n",
      "pp: [dental implant procedure what you need to know urlurl ]\n",
      "\n",
      "tw: [@c4i \"The agent can respond to Probe Control ... by tapping out code with a dental implant, even when they don't have their scanner operating.\" https://t.co/jZV3Ve48M4 https://t.co/MGoBvM1A1r]\n",
      "pp: [ mentionname the agent can respond to probe control by tapping out code with a dental implant even when they don t have their scanner operating urlurl ]\n",
      "\n",
      "tw: [#ad Nobel Biocare Dental implant surgical kit https://t.co/qtbQp8gSDW]\n",
      "pp: [ ad nobel biocare dental implant surgical kit urlurl ]\n",
      "\n",
      "tw: [Dental Implants Market 2017 Legendary Players Forecasted Revenue, Shares, Demand and Growth Rate by 2022 https://t.co/a7xlBiR6BC https://t.co/5xDb7iiIi9]\n",
      "pp: [dental implants market numbers legendary players forecasted revenue shares demand and growth rate by numbers urlurl ]\n",
      "\n",
      "tw: [@KathleenLights1 Got 3 dental implants done today my mouth is in agony. I just think the results will be worth it!]\n",
      "pp: [ mentionname got numbers dental implants done today my mouth is in agony i just think the results will be worth it ]\n",
      "\n",
      "tw: [8 layered Emax crowns - photo courtesy of a very happy Dr Bacchus at Care Dental Implant Clinic üëåüèª‚≠êÔ∏èü¶∑\n",
      "\n",
      "#dental #dentallab #dentists #emax #dentalcrowns https://t.co/6KnBW6HkGD]\n",
      "pp: [ numbers layered emax crowns photo courtesy of a very happy dr bacchus at care dental implant clinic dental dentallab dentists emax dentalcrowns urlurl ]\n",
      "\n",
      "tw: [If you‚Äôre looking for a permanent solution for your missing teeth, dental implants are your solution. https://t.co/3vR6fFByuc]\n",
      "pp: [if you re looking for a permanent solution for your missing teeth dental implants are your solution urlurl ]\n",
      "\n",
      "tw: [Nick's special areas of interest are dental implants, endodontics, prosthetics and cosmetic dentistry. https://t.co/Af4jpeieON]\n",
      "pp: [nick s special areas of interest are dental implants endodontics prosthetics and cosmetic dentistry urlurl ]\n",
      "\n",
      "tw: [dental implants¬†hampshire -www.click4teeth.com/implant-dentist-winchester-dr-steve-larcombe/]\n",
      "pp: [dental implants hampshire www click numbers teeth com implant dentist winchester dr steve larcombe ]\n",
      "\n",
      "tw: [Are you in #Manchester tonight? Then why not attend ICE's FREE Open Day for their 'MSc Dental Implantology'. #implants #dentistry https://t.co/7lJMl3DwZc]\n",
      "pp: [are you in manchester tonight questionmark then why not attend ice s free open day for their msc dental implantology implants dentistry urlurl ]\n",
      "\n",
      "tw: [RT @MrG_Picks: https://t.co/UzXkow2y8u Dental implants are a small titanium post that replaces the root of a missing tooth and supports a c‚Ä¶]\n",
      "pp: [rt mentionname urlurl ]\n",
      "\n",
      "tw: [@william13wb Look at this. All done in one day Dental Implants. In Toronto. https://t.co/e1zOq8iNyF Excellent choice. HIH Bonus thought. Love eveyone.]\n",
      "pp: [ mentionname look at this all done in one day dental implants in toronto urlurl ]\n",
      "\n",
      "tw: [Endosteal and Subperiosteal implants are the two types of dental implants that are most commonly used. Endosteal require metal screws or cylinders to be drilled into your jawbone to hold the tooth. Subperiosteal, on the other hand, places a metal frame o‚Ä¶ https://t.co/nIMMnvR2Y1 https://t.co/iL9Im4svFo]\n",
      "pp: [endosteal and subperiosteal implants are the two types of dental implants that are most commonly used endosteal require metal screws or cylinders to be drilled into your jawbone to hold the tooth subperiosteal on the other hand places a metal frame o urlurl ]\n",
      "\n",
      "tw: [The Unseen Benefits Of Getting Dental Implants Key Largo https://t.co/zM0yV8Ocvl]\n",
      "pp: [the unseen benefits of getting dental implants key largo urlurl ]\n",
      "\n",
      "tw: [RT @LilyBerry42: Dental Implants in the bronx https://t.co/q6IdRCQ8OZ]\n",
      "pp: [rt mentionname dental implants in the bronx urlurl ]\n",
      "\n",
      "tw: [RT @col_plaza_dent: Dental implants a popular option to replace damaged or missing teeth. https://t.co/xcFp7vu0Tn https://t.co/HosJslFIdh]\n",
      "pp: [rt mentionname dental implants a popular option to replace damaged or missing teeth urlurl ]\n",
      "\n",
      "tw: [RT @KianorShah: Dental Implant Dentists In Coventry Teach Best Same Day New Implants Procedure https://t.co/i5Qe7PFhvy]\n",
      "pp: [rt mentionname dental implant dentists in coventry teach best same day new implants procedure urlurl ]\n",
      "\n",
      "tw: [Join us NEXT THURDAY 8TH JUNE on the #SMART Dental Implant Training course for training #implantdentistry  book now: https://t.co/zXB8z3QQAj https://t.co/8Fh5hTRzVo]\n",
      "pp: [join us next thurday numbers th june on the smart dental implant training course for training implantdentistry book now urlurl ]\n",
      "\n",
      "tw: [Dental Implants Dental Consumables Market 2018 Growth - Top Players Like GC Corporation, 3M Company, Danaher Corporation, Institut Straumann AG https://t.co/2cptPFb2Jn]\n",
      "pp: [dental implants dental consumables market numbers growth top players like gc corporation numbers m company danaher corporation institut straumann ag urlurl ]\n",
      "\n",
      "tw: [RT @DenPerfectCov: Living with ill-fitting dentures which cause you pain? Speak to us about how dental implants could enhance your life #Ke‚Ä¶]\n",
      "pp: [rt mentionname living with ill fitting dentures which cause you pain questionmark speak to us about how dental implants could enhance your life ke ]\n",
      "\n",
      "tw: [Since then, dental implants have improved in structure and design to treat a variety of situations and provide more natural results. https://t.co/fPNAXrtxeb]\n",
      "pp: [since then dental implants have improved in structure and design to treat a variety of situations and provide more natural results urlurl ]\n",
      "\n",
      "tw: [Check out #Dental Implants Expander Augmentation Bone Graft Technique Oral Surgery  https://t.co/K18DpTef4i via @eBay]\n",
      "pp: [check out dental implants expander augmentation bone graft technique oral surgery urlurl ]\n",
      "\n",
      "tw: [Are you considering a dental implant? \n",
      "\n",
      "Read this guide: https://t.co/DPqD63z96S\n",
      "\n",
      "#missingteeth #Dentist #dentistry #teeth #Dentalimplant #Smile #writers #article #Blog #Bloggers #LittleElm #toothreplacement https://t.co/jMbpu91QE6]\n",
      "pp: [are you considering a dental implant questionmark read this guide urlurl missingteeth dentist dentistry teeth dentalimplant smile writers article blog bloggers littleelm toothreplacement urlurl ]\n",
      "\n",
      "tw: [RT @Blossom_York: How to care for your dental implants https://t.co/yIrYVo5dGR]\n",
      "pp: [rt mentionname how to care for your dental implants urlurl ]\n",
      "\n",
      "tw: [How to Care for Your Dental Implants https://t.co/DlFdqGo7sD #localdentist]\n",
      "pp: [how to care for your dental implants urlurl ]\n",
      "\n",
      "tw: [Dental implants have never been as affordable as they are now! Here at Fillmore Dental, take advantage of our low-cost dental implant price - https://t.co/OM9PnbSQHn]\n",
      "pp: [dental implants have never been as affordable as they are now here at fillmore dental take advantage of our low cost dental implant price urlurl ]\n",
      "\n",
      "tw: [RT @4th5d: @POTUS @VP\n",
      "Are @DWStweets &amp; @JohnPodesta's FANGS Dental Implants?\n",
      "\n",
      "Nice Compliment for DevilüëπTattoos\n",
      "#BloodLust‚Ä¶ ]\n",
      "pp: [rt mentionname mentionname mentionname are mentionname mentionname s fangs dental implants questionmark nice compliment for devil tattoos bloodlust ]\n",
      "\n",
      "tw: [Too many dental implants in #Turkish society?! More filling the prisons? You would have thought #Turkey was sick to the back teeth by now! https://t.co/S2gVqgIB3R]\n",
      "pp: [too many dental implants in turkish society questionmark more filling the prisons questionmark you would have thought turkey was sick to the back teeth by now urlurl ]\n",
      "\n",
      "tw: [Does the Popularity of Dental Implants Impact Cremation? https://t.co/HvbB3uscdn]\n",
      "pp: [does the popularity of dental implants impact cremation questionmark urlurl ]\n",
      "\n",
      "tw: [New dental implant with built-in reservoir reduces risk of infections https://t.co/1GAW2yDVRq]\n",
      "pp: [new dental implant with built in reservoir reduces risk of infections urlurl ]\n",
      "\n",
      "tw: [Basal Implantation:  An Alternative to Conventional Dental Implants\n",
      "Know more @ https://t.co/f1C9pLQHgy https://t.co/FeA1BZjQAi]\n",
      "pp: [basal implantation an alternative to conventional dental implants know more urlurl ]\n",
      "\n",
      "tw: [Find out how dental implants can save your smile. If you suffer from missing teeth, implants could be the answer. https://t.co/mFgkUmm4gC]\n",
      "pp: [find out how dental implants can save your smile if you suffer from missing teeth implants could be the answer urlurl ]\n",
      "\n",
      "tw: [Keeping up Good Hygiene Still a Necessity with Dental Implants https://t.co/QjbK9ElUj7]\n",
      "pp: [keeping up good hygiene still a necessity with dental implants urlurl ]\n",
      "\n",
      "tw: [What lifestyle factors are important when you are getting dental implants? https://t.co/NjsQfg5YYB https://t.co/vNLbBPFaQ7]\n",
      "pp: [what lifestyle factors are important when you are getting dental implants questionmark urlurl ]\n",
      "\n",
      "tw: [SALE Dental Implant Demonstration Tooth Model Teeth Study Model #2019-‚Ö° HOT https://t.co/c2OweXZbaG https://t.co/Nb9bYDh9MK]\n",
      "pp: [sale dental implant demonstration tooth model teeth study model numbers ‚Ö± hot urlurl ]\n",
      "\n",
      "tw: [People often wonder if dental implants from our office are the right solution for them and their oral health needs.\n",
      "https://t.co/LFDiK1367z]\n",
      "pp: [people often wonder if dental implants from our office are the right solution for them and their oral health needs urlurl ]\n",
      "\n",
      "tw: [RT @CharlesHDean: A great resource if you're curious about dental implants. https://t.co/WRHJKNHGfR]\n",
      "pp: [rt mentionname a great resource if you re curious about dental implants urlurl ]\n",
      "\n",
      "tw: [Our patient Mark talks about receiving dental implants &amp; how Dr. Ericksen and his team make him feel comfortable when he visits our office. https://t.co/QFZFTHzklD]\n",
      "pp: [our patient mark talks about receiving dental implants how dr ericksen and his team make him feel comfortable when he visits our office urlurl ]\n",
      "\n",
      "tw: [RT @GuyKawasaki: Ceramic dental implants get crushed by a 20-ton press https://t.co/YVyROKk9bu https://t.co/m1pK8BJM4o]\n",
      "pp: [rt mentionname ceramic dental implants get crushed by a numbers ton press urlurl ]\n",
      "\n",
      "tw: [Dental Implants v/s Dental Bridges - https://t.co/yQKrAoIB8X]\n",
      "pp: [dental implants v s dental bridges urlurl ]\n",
      "\n",
      "tw: [AAOI Mini Residency is designed to jump start the general dentist to incorporate dental implants in his/her practice. Emphasis is placed on the clinical and patient management aspect of dental implantology. Join us December 8,9,10. https://t.co/2D9kWEXcfG]\n",
      "pp: [aaoi mini residency is designed to jump start the general dentist to incorporate dental implants in his her practice emphasis is placed on the clinical and patient management aspect of dental implantology join us december numbers numbers numbers urlurl ]\n",
      "\n",
      "tw: [What Happens When You Get Dental Implants?\n",
      "(https://t.co/jMClqZ04qS)\n",
      "\n",
      "#dentalimplant #aestheticdentistry #dentalprocedure]\n",
      "pp: [what happens when you get dental implants questionmark urlurl dentalimplant aestheticdentistry dentalprocedure]\n",
      "\n",
      "tw: [We have been placing #dentalimplants for over 9 years and we believe that we are at the forefront of dental implants in #Liverpool Available from just ¬£1295 - get in touch today for more information or to arrange a consultation: https://t.co/yRVP4TGa0W https://t.co/6ni9gNCw1b]\n",
      "pp: [we have been placing dentalimplants for over numbers years and we believe that we are at the forefront of dental implants in liverpool available from just numbers get in touch today for more information or to arrange a consultation urlurl ]\n",
      "\n",
      "tw: [Have you been considering dental implants? Learn all about them in this article. https://t.co/raYLCL72IS]\n",
      "pp: [have you been considering dental implants questionmark learn all about them in this article urlurl ]\n",
      "\n",
      "tw: [The Perks Of Availing The All On 4 Dental Implant https://t.co/GITk0gCEH3]\n",
      "pp: [the perks of availing the all on numbers dental implant urlurl ]\n",
      "\n",
      "tw: [From https://t.co/BsvJENAhX2 and more about Dental implants with antibacterial activity and designed to facilitate integration into the bo‚Ä¶]\n",
      "pp: [from urlurl ]\n",
      "\n",
      "tw: [RT @MollieS54378927: Visit us for all of your dental needs; General Dentistry, Dental Implants, Veneers, Bridges, Crowns #youcancheck https‚Ä¶]\n",
      "pp: [rt mentionname visit us for all of your dental needs general dentistry dental implants veneers bridges crowns youcancheck https ]\n",
      "\n",
      "tw: [Dental Implant Package 2\n",
      "All-On-Four (Nobel) + Temp. Prost. + Perm. Prost. = ¬£ 4100\n",
      "For More Info: https://t.co/hVjay5XBXT  \n",
      "#DentalClinic]\n",
      "pp: [dental implant package numbers all on four nobel temp prost perm prost numbers for more info urlurl dentalclinic]\n",
      "\n",
      "tw: [Cosmetic Dentistry with Dental Implants https://t.co/zZ2rwisVrR]\n",
      "pp: [cosmetic dentistry with dental implants urlurl ]\n",
      "\n",
      "tw: [PRF BOX With Instruments Set, prf instruments, prf box dental implant prf box pakistan, prf box... https://t.co/NpEBVhwhhF]\n",
      "pp: [prf box with instruments set prf instruments prf box dental implant prf box pakistan prf box urlurl ]\n",
      "\n",
      "tw: [Check out Straumann Dental Implant Torque Wrench Driver short &amp; long HEX 1.25mm https://t.co/CctTWATyik @eBay]\n",
      "pp: [check out straumann dental implant torque wrench driver short long hex numbers numbers mm urlurl ]\n",
      "\n",
      "tw: [Nobel Biocare Dental Implants. Various sizes, x5. https://t.co/Zrowc3WouW https://t.co/jQpuLF7BVc]\n",
      "pp: [nobel biocare dental implants various sizes x numbers urlurl ]\n",
      "\n",
      "tw: [RT @KianorShah: Dental implant instrument kits market in US scrutinized in new research report https://t.co/GRtfArfkNm]\n",
      "pp: [rt mentionname dental implant instrument kits market in us scrutinized in new research report urlurl ]\n",
      "\n",
      "tw: [RT @LeadSpeakers: Harrell Dental Implant Center - Charlotte NC https://t.co/3FEEq9iCXY]\n",
      "pp: [rt mentionname harrell dental implant center charlotte nc urlurl ]\n",
      "\n",
      "tw: [RT @Dentistry: Europe is expected to be the leading global market for dental implants up to 2020, new reports claim\n",
      "https://t.co/SJzShjyWzV]\n",
      "pp: [rt mentionname europe is expected to be the leading global market for dental implants up to numbers new reports claim urlurl ]\n",
      "\n",
      "tw: [Give it up for the Dentist that is always learning! Dr. Russell attended an Advanced Dental Implant Course in... https://t.co/pbo9b8ucL4]\n",
      "pp: [give it up for the dentist that is always learning dr russell attended an advanced dental implant course in urlurl ]\n",
      "\n",
      "tw: [#LorenzanaDentalAssociates My dental implant came off, what should I do?. Read Blog: https://t.co/sbj5F3cvj1]\n",
      "pp: [ lorenzanadentalassociates my dental implant came off what should i do questionmark read blog urlurl ]\n",
      "\n",
      "tw: [How Does Dental Insurance Work?, via Blog ‚Äì FDG Teeth in One Day Dental Implants https://t.co/yxHmHjmESI https://t.co/fRZqKHMGdp https://t.co/G38Y2mjlWN]\n",
      "pp: [how does dental insurance work questionmark via blog fdg teeth in one day dental implants urlurl ]\n",
      "\n",
      "tw: [Global Orthopedic Implants Market #Cosmetic Dentistry #Cosmetic Dentist #Dental Implants https://t.co/4aSGJIfINQ https://t.co/qfdkV4Mnt6]\n",
      "pp: [global orthopedic implants market cosmetic dentistry cosmetic dentist dental implants urlurl ]\n",
      "\n",
      "tw: [Enjoy laughing without worrying that your denture will slip and come loose. Dental implants secure restorations... https://t.co/PMFnsmcSl8]\n",
      "pp: [enjoy laughing without worrying that your denture will slip and come loose dental implants secure restorations urlurl ]\n",
      "\n",
      "tw: [#dental implant charlotte nc pet grooming salon game]\n",
      "pp: [ dental implant charlotte nc pet grooming salon game]\n",
      "\n",
      "tw: [Dental implants are a great-looking, long-lasting solution for tooth replacement: https://t.co/iW5fpS8FgI https://t.co/jYOeeZ7MmU]\n",
      "pp: [dental implants are a great looking long lasting solution for tooth replacement urlurl ]\n",
      "\n",
      "tw: [Dental Implant Global Market Insights, Forecast to 2025 https://t.co/jnWZZ1LXbY]\n",
      "pp: [dental implant global market insights forecast to numbers urlurl ]\n",
      "\n",
      "tw: [How to promote (and not promote) dental implants  - Dental Economics\n",
      " https://t.co/ppkPWnxdDw]\n",
      "pp: [how to promote and not promote dental implants dental economics urlurl ]\n",
      "\n",
      "tw: [Check out Dental implants torque wrench Nobel Biocare Fit premium tool https://t.co/jEdgWzo5WU @eBay]\n",
      "pp: [check out dental implants torque wrench nobel biocare fit premium tool urlurl ]\n",
      "\n",
      "tw: [Punjab to provide free dental implants to needy and poor https://t.co/Z2ZMaMJeWS #dentalsupplies]\n",
      "pp: [punjab to provide free dental implants to needy and poor urlurl ]\n",
      "\n",
      "tw: [#NYCCD How the Dental Implant Market Will Change by 2020. Read Blog: https://t.co/InVNcwd6KG]\n",
      "pp: [ nyccd how the dental implant market will change by numbers read blog urlurl ]\n",
      "\n",
      "tw: [Dental Implant Procedure - What to Know https://t.co/yQzkGel6Yg]\n",
      "pp: [dental implant procedure what to know urlurl ]\n",
      "\n",
      "tw: [Don‚Äôt Let Diabetes Stop You from Having Dental Implants https://t.co/LOzjVxnNBB https://t.co/OzHBq4w64M]\n",
      "pp: [don t let diabetes stop you from having dental implants urlurl ]\n",
      "\n",
      "tw: [RT @DrBillDorfman: Take a peek inside Dr. Bill Dorfman and the unique Dental Implants procedure:... https://t.co/qbzPK6AZqm]\n",
      "pp: [rt mentionname take a peek inside dr bill dorfman and the unique dental implants procedure urlurl ]\n",
      "\n",
      "tw: [Now in Riyadh ü•Å\n",
      "Dental Implant Course &amp;Hands On \n",
      "Live Demo On Patient üëå https://t.co/hxWsvxe2gU]\n",
      "pp: [now in riyadh dental implant course hands on live demo on patient urlurl ]\n",
      "\n",
      "tw: [Restore Your Smile with Dental Implants https://t.co/MwZnAjyfeA¬†‚Ä¶ https://t.co/ZjViznkQHn]\n",
      "pp: [restore your smile with dental implants urlurl ]\n",
      "\n",
      "tw: [Debbie's dental implant surgery https://t.co/iiMJRKHZDX @Ds161Deborah https://t.co/IfqoDoEdHi]\n",
      "pp: [debbie s dental implant surgery urlurl ]\n",
      "\n",
      "tw: [Find out more about dental implant treatment options #dentalimplants #treatments #implantcentre #uk\n",
      "https://t.co/tpyab3YdNc]\n",
      "pp: [find out more about dental implant treatment options dentalimplants treatments implantcentre uk urlurl ]\n",
      "\n",
      "tw: [Dr. Watkins patient explains how her life has changed after her new dental implants https://t.co/DilSnqNgPC]\n",
      "pp: [dr watkins patient explains how her life has changed after her new dental implants urlurl ]\n",
      "\n",
      "tw: [RT @DentalCareIrl: Did you know that we offer dental implant services in our practice in #Kells? Get in touch to find out more:‚Ä¶ ]\n",
      "pp: [rt mentionname did you know that we offer dental implant services in our practice in kells questionmark get in touch to find out more ]\n",
      "\n",
      "tw: [Facing dental implant placement? Learn what to expect with this guide to dental implants: https://t.co/xqd6wJNlYn https://t.co/9vOpjvKX6k]\n",
      "pp: [facing dental implant placement questionmark learn what to expect with this guide to dental implants urlurl ]\n",
      "\n",
      "tw: [Dental Implants Market is Expected to Witness Significant Growth of US$ 7879.5 Million by 2020 - Medgadget (blog):‚Ä¶ https://t.co/YfIBwNRD4H https://t.co/1k2H3hQj35]\n",
      "pp: [dental implants market is expected to witness significant growth of us moneymark numbers numbers million by numbers medgadget blog urlurl ]\n",
      "\n",
      "tw: [Brushing Techniques to Minimize Plaque : Dental Health #dental implants https://t.co/abemscLKau]\n",
      "pp: [brushing techniques to minimize plaque dental health dental implants urlurl ]\n",
      "\n",
      "tw: [RT @MindBlowing: This is how a dental implant are installed https://t.co/sBsWHEzjTn]\n",
      "pp: [rt mentionname this is how a dental implant are installed urlurl ]\n",
      "\n",
      "tw: [Dental implant treatment at Belmore Dental Implant Clinic: an infographic¬† https://t.co/DegApqmZSI]\n",
      "pp: [dental implant treatment at belmore dental implant clinic an infographic urlurl ]\n",
      "\n",
      "tw: [Dental implants have changed how dentistry is performed more than any other advancement in the last 25 years. Dental implants have many applications to enhance oral health for patients... https://t.co/9W8QIZ9Ggc]\n",
      "pp: [dental implants have changed how dentistry is performed more than any other advancement in the last numbers years dental implants have many applications to enhance oral health for patients urlurl ]\n",
      "\n",
      "tw: [Extraction of root tip and Immediate placement of CeraRoot Dental Implants https://t.co/j9nI110InH via @YouTube]\n",
      "pp: [extraction of root tip and immediate placement of ceraroot dental implants urlurl ]\n",
      "\n",
      "tw: [A PARULIS-LIKE SOFT TISSUE TUMOR IN RELATION WITH A DENTAL IMPLANT. A CASE REPORT. https://t.co/xkIWDmsyde]\n",
      "pp: [a parulis like soft tissue tumor in relation with a dental implant a case report urlurl ]\n",
      "\n",
      "tw: [RT @ArmstrongDrew: A huge portion of the fake teeth and other dental implants in Americans' mouths are made in China. They're now the targe‚Ä¶]\n",
      "pp: [rt mentionname a huge portion of the fake teeth and other dental implants in americans mouths are made in china they re now the targe ]\n",
      "\n",
      "tw: [Smile Implant Clinics are now offering Dental Implants from ¬£995 for a limited time only‚ú®\n",
      " Call us on 01273 726560 to book your consultation]\n",
      "pp: [smile implant clinics are now offering dental implants from numbers for a limited time only call us on numbers numbers to book your consultation]\n",
      "\n",
      "tw: [30 External Internal Pipe Dental Implant Surgical Straight Handpiece 1:1 Nose Co https://t.co/8MrAhggjG6]\n",
      "pp: [ numbers external internal pipe dental implant surgical straight handpiece numbers numbers nose co urlurl ]\n",
      "\n",
      "tw: [Thank you to all of our lovely patients who attended our Dental Implant Referral Evening last night. \n",
      "\n",
      "It‚Äôs was a fun and educational evening and we look forward to seeing you again very soon \n",
      "\n",
      "Also wanted to say a big thank you to @TMSwedenMartina from for his support &amp; time. https://t.co/VYH9dNHEYV]\n",
      "pp: [thank you to all of our lovely patients who attended our dental implant referral evening last night it s was a fun and educational evening and we look forward to seeing you again very soon also wanted to say a big thank you to mentionname from for his support time urlurl ]\n",
      "\n",
      "tw: [Dental implants are the best way to replace any missing tooth or multiple teeth...]\n",
      "pp: [dental implants are the best way to replace any missing tooth or multiple teeth ]\n",
      "\n",
      "tw: [Common Misconceptions About Dental Implants https://t.co/K9lBXbfJCH https://t.co/RX5lpOZ9gQ]\n",
      "pp: [common misconceptions about dental implants urlurl ]\n",
      "\n",
      "tw: [Replacement of missing teeth.. Dental implant Vs fixed bridges     \n",
      " #dental #treatment #toothless #zirconia... https://t.co/bb8DRQoM96]\n",
      "pp: [replacement of missing teeth dental implant vs fixed bridges dental treatment toothless zirconia urlurl ]\n",
      "\n",
      "tw: [RT @greatspoke: https://t.co/0OGCmgj93c San Diego Dental Implant Center : San Diego's Choice For Dental Implants And All-On-4 Teeth In One‚Ä¶]\n",
      "pp: [rt mentionname urlurl ]\n",
      "\n",
      "tw: [Are you ready for Dr Alessandro Pozzi's presentation this morning at the @aaoms Dental Implant Conference? Stop by Quintessence booth 202 afterward to pick up a copy of his book!\n",
      "https://t.co/mjEogEJqCl https://t.co/DytBPywiGb]\n",
      "pp: [are you ready for dr alessandro pozzi s presentation this morning at the mentionname dental implant conference questionmark stop by quintessence booth numbers afterward to pick up a copy of his book urlurl ]\n",
      "\n",
      "tw: [Summarize hereinafter iatric toggle and dental implants: uvA]\n",
      "pp: [summarize hereinafter iatric toggle and dental implants uva]\n",
      "\n",
      "tw: [All you need to know about dental implants by Creative Dental Clinic https://t.co/96Wlvj9EAl https://t.co/B7O6RdkTDa]\n",
      "pp: [all you need to know about dental implants by creative dental clinic urlurl ]\n",
      "\n",
      "tw: [CHRISTIAN ASKS THE LORD MATT sweetwood TO DONATE ME 300 dollars. PLEASE HELP ME, I NEED THIS MONEY BECAUSE I NEED TO DENTAL IMPLANTS .]\n",
      "pp: [christian asks the lord matt sweetwood to donate me numbers dollars please help me i need this money because i need to dental implants ]\n",
      "\n",
      "tw: [Dental Implants ‚Äì Benefits of Employing Dental Implants https://t.co/rU05Vs0pfK]\n",
      "pp: [dental implants benefits of employing dental implants urlurl ]\n",
      "\n",
      "tw: [Dental Implants market expected to witness the highest growth 2025 according to new research report https://t.co/TShYkXJelT]\n",
      "pp: [dental implants market expected to witness the highest growth numbers according to new research report urlurl ]\n",
      "\n",
      "tw: [free botox faces facelifts and dental implants on the tax payers dime  YOU AS AN AMERICAN SHOULD BE ENRAGED #CONGRESS OVERPAID AND CORRUPT https://t.co/EwHdtuNwRm]\n",
      "pp: [free botox faces facelifts and dental implants on the tax payers dime you as an american should be enraged congress overpaid and corrupt urlurl ]\n",
      "\n",
      "tw: [Dental Implants ‚Äì How Long Does It Take? - Modern Bloggers https://t.co/c7wE5zrg35]\n",
      "pp: [dental implants how long does it take questionmark modern bloggers urlurl ]\n",
      "\n",
      "tw: [#Dildo so. #Sexy farie feet. #Nyc dental implants.  CLICK HERE ‚è©‚è©‚è© https://t.co/2wt4EoPtzv ‚è™‚è™‚è™]\n",
      "pp: [ dildo so sexy farie feet nyc dental implants click here urlurl ]\n",
      "\n",
      "tw: [An average dental implant in the USA can set you back as much as five grand. This price includes the implant, the cr https://t.co/0KBv5LTcTu]\n",
      "pp: [an average dental implant in the usa can set you back as much as five grand this price includes the implant the cr urlurl ]\n",
      "\n",
      "tw: [Help restore a confident and beautiful smile with dental implants. @EnamelDental Studio is proud to be your local implant dentistry. Contact us today for a Free Consultation and renew your smile! https://t.co/CK2gtX24jH #dental #enameldentalstudio #enameldentalchicago #chicago https://t.co/oe8Ij4MLxi]\n",
      "pp: [help restore a confident and beautiful smile with dental implants mentionname studio is proud to be your local implant dentistry contact us today for a free consultation and renew your smile urlurl ]\n",
      "\n",
      "tw: [Take back your freedom to eat what you choose with dental implants https://t.co/ewXAC8ecD7 https://t.co/04aZKYTHRt]\n",
      "pp: [take back your freedom to eat what you choose with dental implants urlurl ]\n",
      "\n",
      "tw: [dental implant broken screw removal remover retrieval restoration kit set https://t.co/Q6GtxnPkp3]\n",
      "pp: [dental implant broken screw removal remover retrieval restoration kit set urlurl ]\n",
      "\n",
      "tw: [NSK Dental Implant Surgical Contra Angle 20:1 Low Speed Push Double Spray uh2 https://t.co/EczSXTEwsM https://t.co/SrOZZAQOIL]\n",
      "pp: [nsk dental implant surgical contra angle numbers numbers low speed push double spray uh numbers urlurl ]\n",
      "\n",
      "tw: [An apple a day keeps the doctor away. With dental implants, you can keep enjoying your favorite foods and good... https://t.co/cPw2VSj9Hu]\n",
      "pp: [an apple a day keeps the doctor away with dental implants you can keep enjoying your favorite foods and good urlurl ]\n",
      "\n",
      "tw: [Dental Implants Point to Numerous Wellness Benefits, Notes Smile‚Ä¶ https://t.co/GKBprQfnes]\n",
      "pp: [dental implants point to numerous wellness benefits notes smile urlurl ]\n",
      "\n",
      "tw: [Questions to Ask an Expert in Dental Implant Services in New Iberia, LA https://t.co/vZ2eDKoeCt via @]\n",
      "pp: [questions to ask an expert in dental implant services in new iberia la urlurl ]\n",
      "\n",
      "tw: [I advised some patients not to have dental implant operation because they did not have enough jaw bone.\n",
      "\n",
      "I found... https://t.co/DzmUpD6cLb]\n",
      "pp: [i advised some patients not to have dental implant operation because they did not have enough jaw bone i found urlurl ]\n",
      "\n",
      "tw: [Session #2 in the books. Learned a ton and placed a couple of dental implants. Awesome course @implantseminars #TopNotchDentalCE  #NeverStopLearning]\n",
      "pp: [session numbers in the books learned a ton and placed a couple of dental implants awesome course mentionname topnotchdentalce neverstoplearning]\n",
      "\n",
      "tw: [#dental implant cheap custom office signs]\n",
      "pp: [ dental implant cheap custom office signs]\n",
      "\n",
      "tw: [The Benefits of Dental Implants https://t.co/QFOxjOh6bS]\n",
      "pp: [the benefits of dental implants urlurl ]\n",
      "\n",
      "tw: [Did you know that having dental implants and eating your favorite foods are not mutually exclusive? Read more... https://t.co/94lGr6jBEe]\n",
      "pp: [did you know that having dental implants and eating your favorite foods are not mutually exclusive questionmark read more urlurl ]\n",
      "\n",
      "tw: [Cheap Dental Implants: 5 Tips to Find the Cheap Implants kkk\n",
      "#Cheap #dentalimplants #implants\n",
      "https://t.co/hXm07WBFRY]\n",
      "pp: [cheap dental implants numbers tips to find the cheap implants kkk cheap dentalimplants implants urlurl ]\n",
      "\n",
      "tw: [https://t.co/BibqYHIWqh \n",
      "Dental Implants Surgery in India\n",
      "https://t.co/q7RuODx23h... https://t.co/s1gbi5cNEx]\n",
      "pp: [ urlurl dental implants surgery in india urlurl ]\n",
      "\n",
      "tw: [RT @Oliviaohholt: All-on-4 Dental Implants Rockville, MD\n",
      "\n",
      "https://t.co/wQUk9zbbWy\n",
      "\n",
      "#health #dentalcare #healthyfood #healthylife #smile #de‚Ä¶]\n",
      "pp: [rt mentionname all on numbers dental implants rockville md urlurl health dentalcare healthyfood healthylife smile de ]\n",
      "\n",
      "tw: [Dental Implants are the best replacement for a missing tooth! https://t.co/BrHqEkpVgh]\n",
      "pp: [dental implants are the best replacement for a missing tooth urlurl ]\n",
      "\n",
      "tw: [On Getting Dental Implants: Is Your Mouth Implant-Ready? https://t.co/D6IDCm91zs https://t.co/3lCGuCb0RI]\n",
      "pp: [on getting dental implants is your mouth implant ready questionmark urlurl ]\n",
      "\n",
      "tw: [Implant restoration: Our dental implants can restore missing teeth. Call us to see if they're right for you.¬† https://t.co/CAaIiPJifH]\n",
      "pp: [implant restoration our dental implants can restore missing teeth call us to see if they re right for you urlurl ]\n",
      "\n",
      "tw: [Dental Implants at South Florida Smiles - https://t.co/lCGQRhf6ss]\n",
      "pp: [dental implants at south florida smiles urlurl ]\n",
      "\n",
      "tw: [Cost Effective Dental Implants Glasgow https://t.co/vMFEHyS3KG]\n",
      "pp: [cost effective dental implants glasgow urlurl ]\n",
      "\n",
      "tw: [Dental Implants have become commonplace, with over 3 million people worldwide hosting some sort of implant. Unfortunately, their rising popularity has been accompanied by a number of misconceptions about what they can and can‚Äôt do.\n",
      "https://t.co/2uyu54bsh6 | 386-837-1236 https://t.co/FWudIWuKYJ]\n",
      "pp: [dental implants have become commonplace with over numbers million people worldwide hosting some sort of implant unfortunately their rising popularity has been accompanied by a number of misconceptions about what they can and can t do urlurl ]\n",
      "\n",
      "tw: [DentalRadlett: Renew your smile with dental implants - read our infographic on our blog now https://t.co/vqAR0zfzhJ ‚Ä¶]\n",
      "pp: [dentalradlett renew your smile with dental implants read our infographic on our blog now urlurl ]\n",
      "\n",
      "tw: [Link Between Root Canals &amp; Cancer? https://t.co/6X2w7eOLjA We offer truly affordable dental implants, starting at $795. Call 239-465-4737. https://t.co/cQ2Z8IbVcH]\n",
      "pp: [link between root canals cancer questionmark urlurl ]\n",
      "\n",
      "tw: [Dental Impants - Case Study\n",
      "\n",
      "Dental Implants are the best way to restore missing teeth. Now, with 3D x-rays for... https://t.co/yqkiOA9VkS]\n",
      "pp: [dental impants case study dental implants are the best way to restore missing teeth now with numbers d x rays for urlurl ]\n",
      "\n",
      "tw: [The recovery time following dental implant surgery tends to vary, but is usually based on the amount of teeth... https://t.co/2D8WqB0mLV]\n",
      "pp: [the recovery time following dental implant surgery tends to vary but is usually based on the amount of teeth urlurl ]\n",
      "\n",
      "tw: [Dental Implants Treatments for Patients with low jaw Bone Density https://t.co/LA4gJOLili https://t.co/XPv8TLHGgx]\n",
      "pp: [dental implants treatments for patients with low jaw bone density urlurl ]\n",
      "\n",
      "tw: [RT @AFR_UK: Did you know animals are used to test dental implants? We're funding work to save them from being used:‚Ä¶ ]\n",
      "pp: [rt mentionname did you know animals are used to test dental implants questionmark we re funding work to save them from being used ]\n",
      "\n",
      "tw: [Phuket Dental Implants ‚Äì Facts To¬†Know https://t.co/yWiGmaXVHy https://t.co/6OiVIqtjGR]\n",
      "pp: [phuket dental implants facts to know urlurl ]\n",
      "\n",
      "tw: [RT @GoogleFacts: This is how a dental implant is installed https://t.co/CIxJqBNQxA]\n",
      "pp: [rt mentionname this is how a dental implant is installed urlurl ]\n",
      "\n",
      "tw: [Dental Implants\n",
      "\n",
      "Dental implants/Tooth Implants are the single most important advancement in dentistry.\n",
      "\n",
      "Tooth ext‚Ä¶ https://t.co/4inqYazrg1 https://t.co/uj73CrKKA0]\n",
      "pp: [dental implants dental implants tooth implants are the single most important advancement in dentistry tooth ext urlurl ]\n",
      "\n",
      "tw: [Dental Implants and Prosthetics Market to Reach $12.74 Bn, Globally, by 2023 at 8.8% CAGR, Says AMR https://t.co/AzdWX8q4Os]\n",
      "pp: [dental implants and prosthetics market to reach moneymark numbers numbers bn globally by numbers at numbers numbers cagr says amr urlurl ]\n",
      "\n",
      "tw: [Dental Implant Sales¬†Specialist https://t.co/9lhCH7Gkkl]\n",
      "pp: [dental implant sales specialist urlurl ]\n",
      "\n",
      "tw: [#NewYearNewYou: Dental Implant Institute of #LasVegas offers innovative WaterLase Laser Dentistry https://t.co/YZISuL8fzu #lasvegas #dentist https://t.co/YidpED0jb9]\n",
      "pp: [ newyearnewyou dental implant institute of lasvegas offers innovative waterlase laser dentistry urlurl ]\n",
      "\n",
      "tw: [Amazing price for Dental Implant 20:1 Contra Angles Fiber Optic $299 Only on https://t.co/LD2wpyVaXT https://t.co/TXbluhMwBX]\n",
      "pp: [amazing price for dental implant numbers numbers contra angles fiber optic moneymark numbers only on urlurl ]\n",
      "\n",
      "tw: [We want to keep your smile intact! Call Dr. Vitt and explore your dental implant options. https://t.co/dR1Rd0Nzgc]\n",
      "pp: [we want to keep your smile intact call dr vitt and explore your dental implant options urlurl ]\n",
      "\n",
      "tw: [Wondering How Much Dental Implants Are? We Are the Most Affordable at $1250 Each - https://t.co/5n4WTfJcAU]\n",
      "pp: [wondering how much dental implants are questionmark we are the most affordable at moneymark numbers each urlurl ]\n",
      "\n",
      "tw: [@IsUCFInTheP5Yet Clear Choice dental implants got nothin on Knugget!]\n",
      "pp: [ mentionname clear choice dental implants got nothin on knugget ]\n",
      "\n",
      "tw: [Dental Implants Can Help You Look Your Best https://t.co/DSFhEPKDVu #familydentist]\n",
      "pp: [dental implants can help you look your best urlurl ]\n",
      "\n",
      "tw: [Dental Implant Failures May Be Caused by Bisphosphonates https://t.co/wDWoweNqv4]\n",
      "pp: [dental implant failures may be caused by bisphosphonates urlurl ]\n",
      "\n",
      "tw: [Dental Implant Machine LCD System Surgical Brushless Drill Motor  Handpiece https://t.co/4KtXavFY7I]\n",
      "pp: [dental implant machine lcd system surgical brushless drill motor handpiece urlurl ]\n",
      "\n",
      "tw: [RT @ORRthodontics: Researchers create new approach¬†to reduce risk of¬†dental implant failure https://t.co/dd4DHh5eLa https://t.co/6mIY1BSXu2]\n",
      "pp: [rt mentionname researchers create new approach to reduce risk of dental implant failure urlurl ]\n",
      "\n",
      "tw: [RT @Novadontics: Set up your free dental implant consultation with one of our specialized providers today! 209-317-9097 https://t.co/Uqye5U‚Ä¶]\n",
      "pp: [rt mentionname set up your free dental implant consultation with one of our specialized providers today numbers numbers numbers urlurl ]\n",
      "\n",
      "tw: [Things People Should Know About Dental Implants In Anne Arundel | Wiki Articles https://t.co/FTeM7epqS6]\n",
      "pp: [things people should know about dental implants in anne arundel wiki articles urlurl ]\n",
      "\n",
      "tw: [RT @DrRoyT: Do you want to know more about dental implants?? \n",
      "#murfreesboro #dentistry #smiles #drroythompson https://t.co/d7bR2WjuFr]\n",
      "pp: [rt mentionname do you want to know more about dental implants questionmark questionmark murfreesboro dentistry smiles drroythompson urlurl ]\n",
      "\n",
      "tw: [15 Dental Implant Surgical Straight Handpiece 1:1 Nose Cone External Nozzle c2fp https://t.co/w5OGPIodms]\n",
      "pp: [ numbers dental implant surgical straight handpiece numbers numbers nose cone external nozzle c numbers fp urlurl ]\n",
      "\n",
      "tw: [N21_dentist: Missing teeth? Have you considered #Dental Implants https://t.co/mzo6LNuhuI #WinchmoreHill https://t.co/WC4rSbFZvH #DentalWo‚Ä¶]\n",
      "pp: [n numbers dentist missing teeth questionmark have you considered dental implants urlurl ]\n",
      "\n",
      "tw: [Yep Now i am in this mood - simply awesome at Soni Dental Implants\n",
      "#enoughsaid RHCP's sesh coming #enjoysmileive https://t.co/f3anxmA5xi]\n",
      "pp: [yep now i am in this mood simply awesome at soni dental implants enoughsaid rhcp s sesh coming enjoysmileive urlurl ]\n",
      "\n",
      "tw: [On Your Nerves: Bone &amp; nerve augmentation are an important, yet mysterious part of #dental implants! Learn more - https://t.co/a8bQqWAaaC]\n",
      "pp: [on your nerves bone nerve augmentation are an important yet mysterious part of dental implants learn more urlurl ]\n",
      "\n",
      "tw: [The history of dental implants https://t.co/PvpzBGltid https://t.co/wXyKwo2Av5]\n",
      "pp: [the history of dental implants urlurl ]\n",
      "\n",
      "tw: [#digital prints to canvas dental implants rockland county ny]\n",
      "pp: [ digital prints to canvas dental implants rockland county ny]\n",
      "\n",
      "tw: [Dental Implant Analysis Demonstration Teeth Disease Model with Restoration https://t.co/FccC7BzhUM https://t.co/6wDqjy4Ygk]\n",
      "pp: [dental implant analysis demonstration teeth disease model with restoration urlurl ]\n",
      "\n",
      "tw: [While dental implants are a safe way to restore your smile, the procedure is surgical, and may lead to... https://t.co/Glr7B9YaH6]\n",
      "pp: [while dental implants are a safe way to restore your smile the procedure is surgical and may lead to urlurl ]\n",
      "\n",
      "tw: [Explore all types of Dental Implants by Dr Tarun Giroti https://t.co/OzJtGBgvWo via @YouTube]\n",
      "pp: [explore all types of dental implants by dr tarun giroti urlurl ]\n",
      "\n",
      "tw: [Single tooth dental implants procedure - #tooth #surgery #implants zx\n",
      "https://t.co/GXVRtfFdve]\n",
      "pp: [single tooth dental implants procedure tooth surgery implants zx urlurl ]\n",
      "\n",
      "tw: [Over 3 million people have dental implants and the success rate after 5 years is over 90 percent. https://t.co/aNowp0WOIJ]\n",
      "pp: [over numbers million people have dental implants and the success rate after numbers years is over numbers percent urlurl ]\n",
      "\n",
      "tw: [RT @paul_s_murphy_z: Dental implants UK are one of the most effective ways to improve your appearance and restore your lost teeth. They‚Ä¶ ]\n",
      "pp: [rt mentionname dental implants uk are one of the most effective ways to improve your appearance and restore your lost teeth they ]\n",
      "\n",
      "tw: [Dental implants can give you a reason to smile again. Give us a call and explore your dental implant options. https://t.co/vt5SjOMqsQ]\n",
      "pp: [dental implants can give you a reason to smile again give us a call and explore your dental implant options urlurl ]\n",
      "\n",
      "tw: [Ditch the Crown: Dental Implants a Serious Long-term Solution https://t.co/tewKSFforv #dentalhealth]\n",
      "pp: [ditch the crown dental implants a serious long term solution urlurl ]\n",
      "\n",
      "tw: [Missing teeth? Struggle to bite? Or fed-up of Dentures? Find out more about Dental Implants https://t.co/WVXN78YvO0]\n",
      "pp: [missing teeth questionmark struggle to bite questionmark or fed up of dentures questionmark find out more about dental implants urlurl ]\n",
      "\n",
      "tw: [RT @Drtermechi: I will try to post from time to time articles and photos of interesting cases regarding dental implants and oral health.]\n",
      "pp: [rt mentionname i will try to post from time to time articles and photos of interesting cases regarding dental implants and oral health ]\n",
      "\n",
      "tw: [Periodontist and Dental Implant Specialist, Dr. Gordon Fraser, Extends‚Ä¶ https://t.co/4EMnINBXo7]\n",
      "pp: [periodontist and dental implant specialist dr gordon fraser extends urlurl ]\n",
      "\n",
      "tw: [Looking for affordable dental implants?  We can help. https://t.co/p8FUb5lsHQ]\n",
      "pp: [looking for affordable dental implants questionmark we can help urlurl ]\n",
      "\n",
      "tw: [@KattPackAllDay I need a smile.I have Lupus &amp; need dental implants.Just graduated college @ 48. Want to smile. Help?Asking everyone, anyone]\n",
      "pp: [ mentionname i need a smile i have lupus need dental implants just graduated college numbers want to smile help questionmark asking everyone anyone]\n",
      "\n",
      "tw: [RT @start_this_up: Now e\n",
      "A platform dedicated to dental implants #startup #dentalimplants https://t.co/So77xCG3AE]\n",
      "pp: [rt mentionname now e a platform dedicated to dental implants startup dentalimplants urlurl ]\n",
      "\n",
      "tw: [We are generally aware that the dental implants fuse with the jaw bone surrounding them and hence they provide greater stability. What precisely happens between the implants and the jaw bone? Find out. https://t.co/LdNFrH8w6y \n",
      "\n",
      "#DrKarthikeyan #ImplantsChennai https://t.co/53kgg9tHF7]\n",
      "pp: [we are generally aware that the dental implants fuse with the jaw bone surrounding them and hence they provide greater stability what precisely happens between the implants and the jaw bone questionmark find out urlurl drkarthikeyan implantschennai urlurl ]\n",
      "\n",
      "tw: [DENTAL IMPLANTS\n",
      "Is your denture uncomfortable-implants could be the solution, visit our website for more information https://t.co/aMLgdUI2wH]\n",
      "pp: [dental implants is your denture uncomfortable implants could be the solution visit our website for more information urlurl ]\n",
      "\n",
      "tw: [RT @KianorShah: Dental Implants and Prosthetics Market Trends, Regulations And Competitive Landscape Outlook to 2025 https://t.co/ZjocFFzbpl]\n",
      "pp: [rt mentionname dental implants and prosthetics market trends regulations and competitive landscape outlook to numbers urlurl ]\n",
      "\n",
      "tw: [Here‚Äôs a simple explanation of the dental implant process. https://t.co/op5IMZTCGN]\n",
      "pp: [here s a simple explanation of the dental implant process urlurl ]\n",
      "\n",
      "tw: [Twitter post with link to the website: Thrilled to announce the launch of the Dental Implant Surgical Seminar in partnership with @UFMedu]\n",
      "pp: [twitter post with link to the website thrilled to announce the launch of the dental implant surgical seminar in partnership with mentionname ]\n",
      "\n",
      "tw: [Thinking of dental implants? Take a closer look https://t.co/tNR386hVHl]\n",
      "pp: [thinking of dental implants questionmark take a closer look urlurl ]\n",
      "\n",
      "tw: [#ORALHYGIENE : Cleaning Around Dental Implants for Oral Hygiene\n",
      "WATCH THE VIDEO: https://t.co/nAObFwO9Kb\n",
      "SHARE IT https://t.co/DQRRXFMyGu]\n",
      "pp: [ oralhygiene cleaning around dental implants for oral hygiene watch the video urlurl share it urlurl ]\n",
      "\n",
      "tw: [Are you missing one or more teeth but unaware of your options? At OKC Dental Implant &amp; Periodontics our high quality #dentalimplants can help give you your smile back! Visit: https://t.co/we4uQ1HOd0 to request an appointment today #okcperiodontist #okcdentalimplants https://t.co/KmuQAKkefx]\n",
      "pp: [are you missing one or more teeth but unaware of your options questionmark at okc dental implant periodontics our high quality dentalimplants can help give you your smile back visit urlurl ]\n",
      "\n",
      "tw: [Dental Implants! 480-614-1122 #implantdentist, #scottsdaledentist, #emergencydentist]\n",
      "pp: [dental implants numbers numbers numbers implantdentist scottsdaledentist emergencydentist]\n",
      "\n",
      "tw: [5 Quasar¬Æ - Conical Spiral Dental Implant + GIFT 5 Straight Abutment Int' Hex https://t.co/eOr9IDvzcH https://t.co/3UbXsFRqWe]\n",
      "pp: [ numbers quasar conical spiral dental implant gift numbers straight abutment int hex urlurl ]\n",
      "\n",
      "tw: [PRESS RELEASE ¬ª The Dental Implants Market by Major Key Players, Revenue, Share, Demand, Analysis and Forecasts Till 2022 https://t.co/kSjsVXWW0r üó£#Opines on #Healthcare]\n",
      "pp: [press release the dental implants market by major key players revenue share demand analysis and forecasts till numbers urlurl ]\n",
      "\n",
      "tw: [RT @WebStuff5857: Dentist Charlotte NC 28277 | Cosmetic | Dental Implants | Sedation | Adult Dentistry of Ballantyne https://t.co/GwNKvMHWaI]\n",
      "pp: [rt mentionname dentist charlotte nc numbers cosmetic dental implants sedation adult dentistry of ballantyne urlurl ]\n",
      "\n",
      "tw: [Things That Affect Your Suitability for Dental Implants https://t.co/0BqSCnsxlP https://t.co/3eyTSn1EXz]\n",
      "pp: [things that affect your suitability for dental implants urlurl ]\n",
      "\n",
      "tw: [Dr. Ali Nagem (Dental Implant Specialist). #dralinagem.\n",
      "Life Time Warranty On Italian Dental Implant.\n",
      "\n",
      "Free Consultation Available until 25 september.\n",
      "\n",
      "For Appointment Call : \n",
      "Al Barsha: 043991115... https://t.co/Azd35sak5Y]\n",
      "pp: [dr ali nagem dental implant specialist dralinagem life time warranty on italian dental implant free consultation available until numbers september for appointment call al barsha numbers urlurl ]\n",
      "\n",
      "tw: [Dental Implant Dentist Sopchoppy FL Florida Teeth in A Day.\n",
      "Dental Implants have changed the face of dentistry over the last 25 years. What]\n",
      "pp: [dental implant dentist sopchoppy fl florida teeth in a day dental implants have changed the face of dentistry over the last numbers years what]\n",
      "\n",
      "tw: [Nanotechnology Dental Implant Market Segmentation by Regions, Production, Consumption and Revenue 2017-2021 https://t.co/yv06LMHFYG]\n",
      "pp: [nanotechnology dental implant market segmentation by regions production consumption and revenue numbers numbers urlurl ]\n",
      "\n",
      "tw: [Global Dental Implant Surgery Tools Market 2016 - Yes Biotech, Cowell Medi, Cortex Dental, DIO ...: T... https://t.co/7P8wuRHPOl #dental]\n",
      "pp: [global dental implant surgery tools market numbers yes biotech cowell medi cortex dental dio t urlurl ]\n",
      "\n",
      "tw: [RT @rargertank: Dental Implants | Periodontist In Pittsfield, MA Introduces Revolutionary, State-of-the-Art Laser Periodontal Di... http://‚Ä¶]\n",
      "pp: [rt mentionname dental implants periodontist in pittsfield ma introduces revolutionary state of the art laser periodontal di http ]\n",
      "\n",
      "tw: [Dental Implant Restoration Model With 4 Implants Overdenture Inferior #6002 02 https://t.co/IuDnKT3Jcf https://t.co/ky8weFSaaO]\n",
      "pp: [dental implant restoration model with numbers implants overdenture inferior numbers numbers urlurl ]\n",
      "\n",
      "tw: [hot Dental Implant Demonstration Teeth Study Teach Model #2020 with implant Sino https://t.co/6hQsEF46z3 https://t.co/aymhTj5OaY]\n",
      "pp: [hot dental implant demonstration teeth study teach model numbers with implant sino urlurl ]\n",
      "\n",
      "tw: [Are you a candidate for dental implants?\n",
      "Check out our website and see!\n",
      "https://t.co/Ky1NQCPmxI https://t.co/1ohrZQ80KB]\n",
      "pp: [are you a candidate for dental implants questionmark check out our website and see urlurl ]\n",
      "\n",
      "tw: [Why would Someone Choose a Dental Implant? https://t.co/tvN6idA7GB https://t.co/cdb5p5k54k]\n",
      "pp: [why would someone choose a dental implant questionmark urlurl ]\n",
      "\n",
      "tw: [With dental care costs rising and dental implants a particularly expensive option Aussies should be looking at... https://t.co/cypuQq9Kzm]\n",
      "pp: [with dental care costs rising and dental implants a particularly expensive option aussies should be looking at urlurl ]\n",
      "\n",
      "tw: [Dental implants in patients with osteoporosis: a systematic review with meta-analysis.  https://t.co/ggLPMKA77f]\n",
      "pp: [dental implants in patients with osteoporosis a systematic review with meta analysis urlurl ]\n",
      "\n",
      "tw: [RT @CeraRoot: This patient had a titanium dental implant placed in the central incisor. The position of the metal implant was... https://t.‚Ä¶]\n",
      "pp: [rt mentionname this patient had a titanium dental implant placed in the central incisor the position of the metal implant was urlurl ]\n",
      "\n",
      "tw: [Dental implants have a success rate of around 95%. https://t.co/jf4drvVOKc]\n",
      "pp: [dental implants have a success rate of around numbers urlurl ]\n",
      "\n",
      "tw: [Successful method to reduce dental implant failure https://t.co/65RcL7mFQz https://t.co/9d6K0Vja4U]\n",
      "pp: [successful method to reduce dental implant failure urlurl ]\n",
      "\n",
      "tw: [#dental implant nj rci timeshare reviews tripadvisor]\n",
      "pp: [ dental implant nj rci timeshare reviews tripadvisor]\n",
      "\n",
      "tw: [How Much Do Dental Implants Cost? \n",
      "If someone gives you a price without seeing your mouth first, you should be very careful.  Dental implants are customized for your mouth. No two patients or their treatment are the the exactly alike or the same,costs will vary accordingly.]\n",
      "pp: [how much do dental implants cost questionmark if someone gives you a price without seeing your mouth first you should be very careful dental implants are customized for your mouth no two patients or their treatment are the the exactly alike or the same costs will vary accordingly ]\n",
      "\n",
      "tw: [‚ÄòDr. Simon Darfoor | Why Are Dental Implants Becoming The Pref Treatment Choice‚Äô on #SoundCloud #np #youtube #drsimon #darfoor #dental #dentistry #dentalcare #dentalimplant #dentalveneers #procelainveneers\n",
      " https://t.co/pKm87tZbPe]\n",
      "pp: [ dr simon darfoor why are dental implants becoming the pref treatment choice on soundcloud np youtube drsimon darfoor dental dentistry dentalcare dentalimplant dentalveneers procelainveneers urlurl ]\n",
      "\n",
      "tw: [What You Need to Know About Dental Implants https://t.co/rgMWtbqVui via @]\n",
      "pp: [what you need to know about dental implants urlurl ]\n",
      "\n",
      "tw: [You will love your new dental implants from Frontenac Denture Clinic! https://t.co/XA0XSLDpMw https://t.co/zpTNCwvBhs]\n",
      "pp: [you will love your new dental implants from frontenac denture clinic urlurl ]\n",
      "\n",
      "tw: [What are dental implants? https://t.co/fgKPfdXVXc https://t.co/40k5LT6i7t]\n",
      "pp: [what are dental implants questionmark urlurl ]\n",
      "\n",
      "tw: [RT @ChristianLN0821: Here's a secret, just had a dental implant. My smiles back #AskChristianAnything https://t.co/bhnKTaGHdC]\n",
      "pp: [rt mentionname here s a secret just had a dental implant my smiles back askchristiananything urlurl ]\n",
      "\n",
      "tw: [Dental implants can greatly improve how dentures function for patients.  Call us today to schedule your... https://t.co/K1GOmF7Gb3]\n",
      "pp: [dental implants can greatly improve how dentures function for patients call us today to schedule your urlurl ]\n",
      "\n",
      "tw: [We will answer any and all questions you have about dental implants. https://t.co/zieJP7hJs5]\n",
      "pp: [we will answer any and all questions you have about dental implants urlurl ]\n",
      "\n",
      "tw: [Dental implants fuse to the bone, so there's no risk of your new teeth sliding out or being jolted by what you... https://t.co/oGoic9CYFq]\n",
      "pp: [dental implants fuse to the bone so there s no risk of your new teeth sliding out or being jolted by what you urlurl ]\n",
      "\n",
      "tw: [Learn the facts about dental implants with this helpful article and call our office to learn about our services. https://t.co/oFMsTgYnnn]\n",
      "pp: [learn the facts about dental implants with this helpful article and call our office to learn about our services urlurl ]\n",
      "\n",
      "tw: [I'm raising money for dental implants. Click to Donate:  https://t.co/dPtkt6LGxJ via @gofundme]\n",
      "pp: [i m raising money for dental implants click to donate urlurl ]\n",
      "\n",
      "tw: [From family dental care through to dental implants, orthodontics and facial aesthetics ‚Äì we are a full solution dental practice in #Leicester]\n",
      "pp: [from family dental care through to dental implants orthodontics and facial aesthetics we are a full solution dental practice in leicester]\n",
      "\n",
      "tw: [Eat, talk, laugh and #smile with confidence\n",
      "Dental implants\n",
      "https://t.co/rjKYIq3oTF\n",
      "#dentalcare #dentalimplants https://t.co/HsKYezuWAA]\n",
      "pp: [eat talk laugh and smile with confidence dental implants urlurl dentalcare dentalimplants urlurl ]\n",
      "\n",
      "tw: [Educate yourself, if you desire to have dental implants for your missing teeth.... https://t.co/cPYOzRNDmv]\n",
      "pp: [educate yourself if you desire to have dental implants for your missing teeth urlurl ]\n",
      "\n",
      "tw: [Dental Implants: calculating the cost and value: https://t.co/LtVvaF8tVb]\n",
      "pp: [dental implants calculating the cost and value urlurl ]\n",
      "\n",
      "tw: [Cosmetic Dentist Pearl District, Downtown Portland OR, Interviewed on Dental Implants Solutions https://t.co/iZ74vciFj0]\n",
      "pp: [cosmetic dentist pearl district downtown portland or interviewed on dental implants solutions urlurl ]\n",
      "\n",
      "tw: [#snow plowing chicago average cost of dental implants]\n",
      "pp: [ snow plowing chicago average cost of dental implants]\n",
      "\n",
      "tw: [Check out Dental Implant Broken Screw Remover &amp; Repair Kit worldwide Ship for Biomet 3i  https://t.co/butuH8Y37l via @eBay]\n",
      "pp: [check out dental implant broken screw remover repair kit worldwide ship for biomet numbers i urlurl ]\n",
      "\n",
      "tw: [Restore your beautiful smile and contact Family Dentistry for professional dental implants: https://t.co/ixcYTaEjYc]\n",
      "pp: [restore your beautiful smile and contact family dentistry for professional dental implants urlurl ]\n",
      "\n",
      "tw: [I saw Dr Sean Pierce for some new dental... - by Amy O.\n",
      "\n",
      "I saw Dr Sean Pierce for some new dental implants and... https://t.co/0ipqZqaFQE]\n",
      "pp: [i saw dr sean pierce for some new dental by amy o i saw dr sean pierce for some new dental implants and urlurl ]\n",
      "\n",
      "tw: [Bone grafting and dental implants typically go hand-in-hand, as the loss of a tooth can wear down the... https://t.co/mLOTq9vUJC]\n",
      "pp: [bone grafting and dental implants typically go hand in hand as the loss of a tooth can wear down the urlurl ]\n",
      "\n",
      "tw: [In search of a smile makeover, missing teeth, or needing to replace decayed teeth? Dental implants may be the... https://t.co/PCHTzs6usx]\n",
      "pp: [in search of a smile makeover missing teeth or needing to replace decayed teeth questionmark dental implants may be the urlurl ]\n",
      "\n",
      "tw: [@bruceprichard @HeyHeyItsConrad re listened to your podcast yesterday while having dental implants thanks for keeping mind off the pain]\n",
      "pp: [ mentionname mentionname re listened to your podcast yesterday while having dental implants thanks for keeping mind off the pain]\n",
      "\n",
      "tw: [RT @EarnKnowledge: This is how a dental implant is installed https://t.co/1Of3C0ln9a]\n",
      "pp: [rt mentionname this is how a dental implant is installed urlurl ]\n",
      "\n",
      "tw: [@IreneHandmaiden @doormatt134 Poor dear. Start a GoFundMe for dental implants?]\n",
      "pp: [ mentionname mentionname poor dear start a gofundme for dental implants questionmark ]\n",
      "\n",
      "tw: [Dental Implant Fixture and Final Abutment Market Continues to Grow as Implantology Becomes the Norm for GP https://t.co/USebdBJtZk s]\n",
      "pp: [dental implant fixture and final abutment market continues to grow as implantology becomes the norm for gp urlurl ]\n",
      "\n",
      "tw: [Houston is replacing their missing teeth with our amazing and innovative $899 Dental Implant Special! https://t.co/DPhYIiFGBS]\n",
      "pp: [houston is replacing their missing teeth with our amazing and innovative moneymark numbers dental implant special urlurl ]\n",
      "\n",
      "tw: [Experience renewed confidence when smiling and eating with dental implants. https://t.co/puYYzHBuwc https://t.co/C94UnKZ7Aw]\n",
      "pp: [experience renewed confidence when smiling and eating with dental implants urlurl ]\n",
      "\n",
      "tw: [RT @DentistryToday: The program is aimed at dentists who want to gain new skills in providing dental implants while taking away the stress‚Ä¶]\n",
      "pp: [rt mentionname the program is aimed at dentists who want to gain new skills in providing dental implants while taking away the stress ]\n",
      "\n",
      "tw: [Dr. Jig Patel Treats Tooth Loss with Modern Tooth Replacement Solutions Dental Implants in Schaumburg IL https://t.co/rPICCcWZDg]\n",
      "pp: [dr jig patel treats tooth loss with modern tooth replacement solutions dental implants in schaumburg il urlurl ]\n",
      "\n",
      "tw: [Happy National Grandparents Day from all the staff at Newcastle Denture &amp; Dental Implant Clinic. https://t.co/jKCy82Cp8D]\n",
      "pp: [happy national grandparents day from all the staff at newcastle denture dental implant clinic urlurl ]\n",
      "\n",
      "tw: [RT @isadguicu1984: Dental Implants Southampton | Solent House Dental Centre https://t.co/ZjvLEaDStr]\n",
      "pp: [rt mentionname dental implants southampton solent house dental centre urlurl ]\n",
      "\n",
      "tw: [RT @CRDentalimplant: In our Clinic the prices of our DENTAL Implants are amazing and  what's in style for COSTA RICA! https://t.co/U6sXmYzR‚Ä¶]\n",
      "pp: [rt mentionname in our clinic the prices of our dental implants are amazing and what s in style for costa rica urlurl ]\n",
      "\n",
      "tw: [Follow the link for more info on my complimentary dental implant lecture coming up on Thursday, October 19th. \n",
      "https://t.co/hO1OB8HXxI https://t.co/02DVXpLC2R]\n",
      "pp: [follow the link for more info on my complimentary dental implant lecture coming up on thursday october numbers th urlurl ]\n",
      "\n",
      "tw: [Affordable Dental Implants Sacramento | Call Toll Free 855-977-1199 https://t.co/vw3qtmS0qU via @YouTube]\n",
      "pp: [affordable dental implants sacramento call toll free numbers numbers numbers urlurl ]\n",
      "\n",
      "tw: [Dental Implant Advantages 101: Dental implants can prevent remaining natural teeth from being damaged, and can... https://t.co/IbIhAwwX1N]\n",
      "pp: [dental implant advantages numbers dental implants can prevent remaining natural teeth from being damaged and can urlurl ]\n",
      "\n",
      "tw: [What Can I Expect From the Dental Implant Process? The first phase of getting dental‚Ä¶ https://t.co/R2JgvrQghe https://t.co/uWEwqpCgJ3]\n",
      "pp: [what can i expect from the dental implant process questionmark the first phase of getting dental urlurl ]\n",
      "\n",
      "tw: [Are you living with missing teeth or uncomfortable dentures? Dental implants could be the solution to those dentures! #Coventry]\n",
      "pp: [are you living with missing teeth or uncomfortable dentures questionmark dental implants could be the solution to those dentures coventry]\n",
      "\n",
      "tw: [How long dental implants last?\n",
      "This is one of the most common and yet difficult questions that our patients ask us usually during the consultation. The reality is that dental implants don't have an... https://t.co/YwauhkRM2J]\n",
      "pp: [how long dental implants last questionmark this is one of the most common and yet difficult questions that our patients ask us usually during the consultation the reality is that dental implants don t have an urlurl ]\n",
      "\n",
      "tw: [RT @MachinePix: Installing a dental implant. https://t.co/HBPoGZzTRU]\n",
      "pp: [rt mentionname installing a dental implant urlurl ]\n",
      "\n",
      "tw: [Do you have missing teeth? Don't miss out on holiday food. Call (623) 846-5555 and get your dental implants today! https://t.co/OILVItn0vT]\n",
      "pp: [do you have missing teeth questionmark don t miss out on holiday food call numbers numbers numbers and get your dental implants today urlurl ]\n",
      "\n",
      "tw: [Dental Implants and Prosthetics #Cosmetic Dentistry #Cosmetic Dentist #Dental Implants https://t.co/yMf0k6qPWd https://t.co/go89agNPML]\n",
      "pp: [dental implants and prosthetics cosmetic dentistry cosmetic dentist dental implants urlurl ]\n",
      "\n",
      "tw: [Our specialist dental implant team are dedicated to giving patients fantastic smile they can‚Äôt help but show off! #Tunbridge Wells]\n",
      "pp: [our specialist dental implant team are dedicated to giving patients fantastic smile they can t help but show off tunbridge wells]\n",
      "\n",
      "tw: [A new study from China has explored the effects of heavy smoking on dental implants in the mandible https://t.co/qoaCSW8BYq https://t.co/traiPpap3e]\n",
      "pp: [a new study from china has explored the effects of heavy smoking on dental implants in the mandible urlurl ]\n",
      "\n",
      "tw: [Some questions answered about Southampton dental implants https://t.co/VhOWj1qyJg https://t.co/sMyGudtV78]\n",
      "pp: [some questions answered about southampton dental implants urlurl ]\n",
      "\n",
      "tw: [RT @KianorShah: A Dental Implant Evolution Challenging School Of Thought\n",
      "\n",
      "We are looking for an established‚Ä¶ https://t.co/XVydDxoycU https:‚Ä¶]\n",
      "pp: [rt mentionname a dental implant evolution challenging school of thought we are looking for an established urlurl ]\n",
      "\n",
      "tw: [What do you know about titanium and dental implants?  Find out at https://t.co/u5R0rHiu2r]\n",
      "pp: [what do you know about titanium and dental implants questionmark find out at urlurl ]\n",
      "\n",
      "tw: [\"How you can benefit from having dental implants\" https://t.co/IQP8JoVvZN #bestdentistconcord #dentalimplants #prosthodontist https://t.co/uAxkKC5kDc]\n",
      "pp: [ how you can benefit from having dental implants urlurl ]\n",
      "\n",
      "tw: [@OpineSOCIAL ¬ª Skilled Dentist, Dr. Jeffrey Lowe, Welcomes New Patients with Missing Teeth for All-on-4¬Æ Dental Implants in Hays, KS htt]\n",
      "pp: [ mentionname skilled dentist dr jeffrey lowe welcomes new patients with missing teeth for all on numbers dental implants in hays ks htt]\n",
      "\n",
      "tw: [Rockville Dentist Dr. Joseph Kravitz Discovers Combined Wavelength &amp; Pain-Modulating Dental Implants Procedure https://t.co/wRi3bz1cyo]\n",
      "pp: [rockville dentist dr joseph kravitz discovers combined wavelength pain modulating dental implants procedure urlurl ]\n",
      "\n",
      "tw: [Are you unhappy with your Smile? \n",
      "Dental Implants are a Life Changer! \n",
      "Call us for more information! \n",
      "\n",
      "#FloridaSmileStudio https://t.co/9tD03hRUKW]\n",
      "pp: [are you unhappy with your smile questionmark dental implants are a life changer call us for more information floridasmilestudio urlurl ]\n",
      "\n",
      "tw: [Dental Implants: https://t.co/1SlZXda2Rx via @YouTube]\n",
      "pp: [dental implants urlurl ]\n",
      "\n",
      "tw: [RT @MrG_Picks: https://t.co/FOFWf7vHe7  Dental Implant Care and Maintenance - Cosmetic Dentist Los Angeles]\n",
      "pp: [rt mentionname urlurl ]\n",
      "\n",
      "tw: [Why Dental Implants Are the Best Option for Replacing Your Missing Teeth! A dental implant is an artificial tooth root that Long Beach dentist Dr. Marvizi... https://t.co/wRLE4NLSKW]\n",
      "pp: [why dental implants are the best option for replacing your missing teeth a dental implant is an artificial tooth root that long beach dentist dr marvizi urlurl ]\n",
      "\n",
      "tw: [RT @BBaldessin: Honest Family Dental - dentist in South Austin specializing in Dental Implants, Crowns, Bridges, Root Canals, Invis‚Ä¶ ]\n",
      "pp: [rt mentionname honest family dental dentist in south austin specializing in dental implants crowns bridges root canals invis ]\n",
      "\n",
      "tw: [RT:@HowardFarran: Researchers create new approach to reduce risk of dental implant failure. https://t.co/gwLDAcpdcL https://t.co/gkXHyRANcG]\n",
      "pp: [rt mentionname researchers create new approach to reduce risk of dental implant failure urlurl ]\n",
      "\n",
      "tw: [RT @wwdental: This month's blog looks at the benefits of dental implants to replace your missing teeth #Dentalimplant #BeckBromFL‚Ä¶ ]\n",
      "pp: [rt mentionname this month s blog looks at the benefits of dental implants to replace your missing teeth dentalimplant beckbromfl ]\n",
      "\n",
      "tw: [RT @JogjaUpdate: #vid: This is how a dental implant is installed ~@ThingsWork~ https://t.co/hrsHtXz9s1]\n",
      "pp: [rt mentionname vid this is how a dental implant is installed mentionname urlurl ]\n",
      "\n",
      "tw: [It may be hard to know whether or not dental implants are the right choice for you. Luckily we have a checklist below to help you decide if implants are right for you! https://t.co/ziUJSnleZ2 https://t.co/E2JPFZvREx]\n",
      "pp: [it may be hard to know whether or not dental implants are the right choice for you luckily we have a checklist below to help you decide if implants are right for you urlurl ]\n",
      "\n",
      "tw: [RT @dentistriversid: RT https://t.co/Q9zdX39bSj RT https://t.co/nc3OEq8GF9 New dental implant tech speeds and improves bone growth #d‚Ä¶ http‚Ä¶]\n",
      "pp: [rt mentionname rt urlurl ]\n",
      "\n",
      "tw: [Things Your don‚Äôt know about Dental Implants\n",
      "\n",
      "For more information about dental Implants, check our latest blog: https://t.co/w6KnvWToOU‚Ä¶/\n",
      "\n",
      "Call us now : +1(323) 213-3676 and schedule your visit with Keromina Dental Group\n",
      "\n",
      "#dentist #dentalimplant #teeth #teethsurgery https://t.co/UOURU6wpvK]\n",
      "pp: [things your don t know about dental implants for more information about dental implants check our latest blog urlurl call us now numbers numbers numbers numbers and schedule your visit with keromina dental group dentist dentalimplant teeth teethsurgery urlurl ]\n",
      "\n",
      "tw: [Our award winning dentist has successful placed hundreds of dental implants and transformed the patient's life.  Find out more by asking our team today #Petersfield]\n",
      "pp: [our award winning dentist has successful placed hundreds of dental implants and transformed the patient s life find out more by asking our team today petersfield]\n",
      "\n",
      "tw: [RT @blumer_miss: Affordable Dental Implants #dental #implant #insurance https://t.co/zTkzFkCZwQ]\n",
      "pp: [rt mentionname affordable dental implants dental implant insurance urlurl ]\n",
      "\n",
      "tw: [Great to see @Somerset_Teeth mentioned in the @TelegraphNews for Dr James Main's dental implant work. Well done! https://t.co/JMBpk5jfUE]\n",
      "pp: [great to see mentionname mentioned in the mentionname for dr james main s dental implant work well done urlurl ]\n",
      "\n",
      "tw: [@ananavarro Root canal \n",
      "Colonoscopy\n",
      "Pap smear\n",
      "Mammogram\n",
      "Dental implant\n",
      "IRS audit \n",
      "Taking the GRE]\n",
      "pp: [ mentionname root canal colonoscopy pap smear mammogram dental implant irs audit taking the gre]\n",
      "\n",
      "tw: [#AdvancedDentalCareofBrandon Why a Dental Implant is Ideal at Any Age. Read Blog: https://t.co/w3iKYvTa0R]\n",
      "pp: [ advanceddentalcareofbrandon why a dental implant is ideal at any age read blog urlurl ]\n",
      "\n",
      "tw: [Question: How much do dental implants cost? Veneers? Crowns (not metallic )? https://t.co/7m5e1YAhVG]\n",
      "pp: [question how much do dental implants cost questionmark veneers questionmark crowns not metallic questionmark urlurl ]\n",
      "\n",
      "tw: [A single dental implant can replace a missing tooth.\n",
      "Open Mon-Sat 8am-5pm\n",
      "954 563-9722\n",
      "\n",
      "Fort Lauderdale https://t.co/Pc91SqOL51]\n",
      "pp: [a single dental implant can replace a missing tooth open mon sat numbers am numbers pm numbers numbers numbers fort lauderdale urlurl ]\n",
      "\n",
      "tw: [Sponsored: Dental implants can bring a whole new start ‚Äì Rochester Democrat and Chronicle https://t.co/lQPVaiB6BF]\n",
      "pp: [sponsored dental implants can bring a whole new start rochester democrat and chronicle urlurl ]\n",
      "\n",
      "tw: [Dr. Kevin Hogan Improves Accuracy of Dental Implant Treatment -‚Ä¶ https://t.co/36PxBEbGp5]\n",
      "pp: [dr kevin hogan improves accuracy of dental implant treatment urlurl ]\n",
      "\n",
      "tw: [@HardballChris Giuliani keeps on bashing Hillary about health yet he is wearing dental implants. He certainly has a health problem.]\n",
      "pp: [ mentionname giuliani keeps on bashing hillary about health yet he is wearing dental implants he certainly has a health problem ]\n",
      "\n",
      "tw: [Wall Centre Dental Offers Dental Implants in Richmond, BC, to New Patients with Missing Teeth https://t.co/aRCEB1N7SO]\n",
      "pp: [wall centre dental offers dental implants in richmond bc to new patients with missing teeth urlurl ]\n",
      "\n",
      "tw: [RT @AdvancingInnova: Your Simplified And Detailed Guide To Dental Implants! - The Amazing You Plan https://t.co/fonIsbNN1s]\n",
      "pp: [rt mentionname your simplified and detailed guide to dental implants the amazing you plan urlurl ]\n",
      "\n",
      "tw: [Ad: Dental Implants - https://t.co/5GGLDkKY3a]\n",
      "pp: [ad dental implants urlurl ]\n",
      "\n",
      "tw: [How The Dental Implants Maui Services Makes You Smile Again https://t.co/rHNbPEENqd]\n",
      "pp: [how the dental implants maui services makes you smile again urlurl ]\n",
      "\n",
      "tw: [Top Dental Implant Care Tips https://t.co/kYG6ho2k0K https://t.co/TYU2YBuOxN]\n",
      "pp: [top dental implant care tips urlurl ]\n",
      "\n",
      "tw: [Dental implants are often the best option for replacing missing teeth. Rather than resting on the gum line like... https://t.co/KNXEI9YJSM]\n",
      "pp: [dental implants are often the best option for replacing missing teeth rather than resting on the gum line like urlurl ]\n",
      "\n",
      "tw: [Dental Implants fuse to the jaw bone and support one or several teeth that are similar to one‚Äôs natural teeth in both appearance and function. If you want the many benefits of dental implants, contact Coral Gables Dentistry for a consultation! (305) 567-1992 https://t.co/3j36ur8ZAd]\n",
      "pp: [dental implants fuse to the jaw bone and support one or several teeth that are similar to one s natural teeth in both appearance and function if you want the many benefits of dental implants contact coral gables dentistry for a consultation numbers numbers numbers urlurl ]\n",
      "\n",
      "tw: [RT @FxhkjsiBertha: The Difference Between Dental Crowns and Dental Implants https://t.co/PH1RlZ8tMb]\n",
      "pp: [rt mentionname the difference between dental crowns and dental implants urlurl ]\n",
      "\n",
      "tw: [RT @Set_entertainme: The Best Dental Implants https://t.co/q8rGjj9SvC]\n",
      "pp: [rt mentionname the best dental implants urlurl ]\n",
      "\n",
      "tw: [RT @jamegodinez: Austin Dental Implant Center : Teeth In A Day Info\n",
      "https://t.co/wRMYJhfAjP]\n",
      "pp: [rt mentionname austin dental implant center teeth in a day info urlurl ]\n",
      "\n",
      "tw: [For more information on Teeth Whitening, Dental Implants, and Fillings visit our website https://t.co/e8UgjsgtiJ]\n",
      "pp: [for more information on teeth whitening dental implants and fillings visit our website urlurl ]\n",
      "\n",
      "tw: [DENTAL IMPLANT DRILL KIT 8 PCS https://t.co/VqFA6oLg08]\n",
      "pp: [dental implant drill kit numbers pcs urlurl ]\n",
      "\n",
      "tw: [Don‚Äôt miss - Dental implants: survival rates of turned and anodised-surface dental implants https://t.co/fqV5BsCHrD]\n",
      "pp: [don t miss dental implants survival rates of turned and anodised surface dental implants urlurl ]\n",
      "\n",
      "tw: [I added a video to a @YouTube playlist https://t.co/0rLcuhapjA Amazing Discovery Goodbye Dental Implants Here's How To Grow Your Own]\n",
      "pp: [i added a video to a mentionname playlist urlurl ]\n",
      "\n",
      "tw: [#ImplantDentistryandPeriodontics The Dental Implants Process. Read Blog: https://t.co/Um6gNV50gP]\n",
      "pp: [ implantdentistryandperiodontics the dental implants process read blog urlurl ]\n",
      "\n",
      "tw: [What does it mean when you might need bone-grafting before dental implant surgery? - https://t.co/wavKAWUIto\n",
      "\n",
      "  #BOMIS #branchburgoms #njoralsurgery https://t.co/k4w2IPBP8a]\n",
      "pp: [what does it mean when you might need bone grafting before dental implant surgery questionmark urlurl bomis branchburgoms njoralsurgery urlurl ]\n",
      "\n",
      "tw: [#full dental implants price bob bell ford glen burnie maryland]\n",
      "pp: [ full dental implants price bob bell ford glen burnie maryland]\n",
      "\n",
      "tw: [#first texas broadband cheap dental implants bay area]\n",
      "pp: [ first texas broadband cheap dental implants bay area]\n",
      "\n",
      "tw: [RT @TewkesburyTeeth: If you're looking for dental advice, our blog covers a wide range of dental topics, from dental implants to cosmetic d‚Ä¶]\n",
      "pp: [rt mentionname if you re looking for dental advice our blog covers a wide range of dental topics from dental implants to cosmetic d ]\n",
      "\n",
      "tw: [Kazemi Oral Surgery &amp; Dental Implants in Bethesda, MD Offers Wisdom Teeth Removal Services https://t.co/bQfJVqJXWz]\n",
      "pp: [kazemi oral surgery dental implants in bethesda md offers wisdom teeth removal services urlurl ]\n",
      "\n",
      "tw: [Dental Implant Live Patient Program\n",
      "March 05-08, 2018\n",
      "Dominican Republic\n",
      "\n",
      "#itcseminars.com, #itcgolive.com, #dental, #dentistry, #dentist, #dentalimplants, #dentalcare, #neodent, #instradent https://t.co/kX9flpPWvC]\n",
      "pp: [dental implant live patient program march numbers numbers numbers dominican republic itcseminars com itcgolive com dental dentistry dentist dentalimplants dentalcare neodent instradent urlurl ]\n",
      "\n",
      "tw: [So what exactly is an implant? Find out more about dental implants on our blog https://t.co/xi8Wj4hZOy #dentist #derby #dentistry https://t.co/Tcvb5JqCWE]\n",
      "pp: [so what exactly is an implant questionmark find out more about dental implants on our blog urlurl ]\n",
      "\n",
      "tw: [RT @sofarrsogud: Who called it a dental implant and not a 'Substitooth'?]\n",
      "pp: [rt mentionname who called it a dental implant and not a substitooth questionmark ]\n",
      "\n",
      "tw: [Maintaining good oral health care and having regular dental check-ups is important for maintaining your gum health after a dental implant.]\n",
      "pp: [maintaining good oral health care and having regular dental check ups is important for maintaining your gum health after a dental implant ]\n",
      "\n",
      "tw: [RT @MDental_Clinic: Premium quality dental implant abroad, in Hungary \n",
      "https://t.co/qJLaYX1lr2 https://t.co/X17BmneUep]\n",
      "pp: [rt mentionname premium quality dental implant abroad in hungary urlurl ]\n",
      "\n",
      "tw: [1 PC NEW Dental Implant Analysis Crown Bridge Demonstration Teeth #2017 https://t.co/Yizv3gPjOY https://t.co/cv6hA0aKdH]\n",
      "pp: [ numbers pc new dental implant analysis crown bridge demonstration teeth numbers urlurl ]\n",
      "\n",
      "tw: [Dental implants are the closest you can get to healthy, natural teeth. \n",
      "They allow you to live the way you want to without worrying about your teeth. \n",
      "Call Us At 415.481.0778 To Schedule Your Free Consultation\n",
      "https://t.co/r7LHkGEFxs\n",
      "#ImplantsProCenter #DentalImplants https://t.co/WF4SSZ51Rt]\n",
      "pp: [dental implants are the closest you can get to healthy natural teeth they allow you to live the way you want to without worrying about your teeth call us at numbers numbers numbers to schedule your free consultation urlurl implantsprocenter dentalimplants urlurl ]\n",
      "\n",
      "tw: [Ross came by to share his positive dental implant procedure experience with us. Thank you for taking the time to film a testimonial video! https://t.co/eV1qFP4Jh2]\n",
      "pp: [ross came by to share his positive dental implant procedure experience with us thank you for taking the time to film a testimonial video urlurl ]\n",
      "\n",
      "tw: [Bone Compression Kit Dental Implant\n",
      "Manufacturer &amp; Supplier\n",
      "Available In Stock\n",
      "Whatsapp: 0092-3116960642... https://t.co/ijBdUawfR2]\n",
      "pp: [bone compression kit dental implant manufacturer supplier available in stock whatsapp numbers numbers urlurl ]\n",
      "\n",
      "tw: [We have a number of ways to replace missing teeth, but dental implants are the most effective and recommended... https://t.co/13IaFLxQKO]\n",
      "pp: [we have a number of ways to replace missing teeth but dental implants are the most effective and recommended urlurl ]\n",
      "\n",
      "tw: [RT @angelamiller945: what if I had to get dental implants? Check more at https://t.co/ZeyDzElyiH]\n",
      "pp: [rt mentionname what if i had to get dental implants questionmark check more at urlurl ]\n",
      "\n",
      "tw: [RT @MyMarketingFile: Fulcurm Implant Systems All-On-8 Immediately Loaded Maxillary Dental Implants 10 Extractions, 12 Implants Total,‚Ä¶ http‚Ä¶]\n",
      "pp: [rt mentionname fulcurm implant systems all on numbers immediately loaded maxillary dental implants numbers extractions numbers implants total http ]\n",
      "\n",
      "tw: [Cleaning Your Dental Implants https://t.co/uXot7Nh4jG]\n",
      "pp: [cleaning your dental implants urlurl ]\n",
      "\n",
      "tw: [@TomthunkitsMind @Alankaye9 Does anyone else think it‚Äôs weird that a ‚Äúbillionaire‚Äù has dentures? He can afford dental implants.....]\n",
      "pp: [ mentionname mentionname does anyone else think it s weird that a billionaire has dentures questionmark he can afford dental implants ]\n",
      "\n",
      "tw: [#ad Nobel Biocare Dental Implants Replace NP 3.5 X10 mm. A BRAND-NEW sealed EXP 2022 https://t.co/XLhZVp77K5]\n",
      "pp: [ ad nobel biocare dental implants replace np numbers numbers x numbers mm a brand new sealed exp numbers urlurl ]\n",
      "\n",
      "tw: [Missing teeth? Restore your teeth with our quality dental implants. Schedule an appointment with us today. https://t.co/etPE1axYHZ]\n",
      "pp: [missing teeth questionmark restore your teeth with our quality dental implants schedule an appointment with us today urlurl ]\n",
      "\n",
      "tw: [More than 3 million Americans have successful dental implants. https://t.co/zM0oD6nypI]\n",
      "pp: [more than numbers million americans have successful dental implants urlurl ]\n",
      "\n",
      "tw: [RT @jamesaiken09: Medical Curios Dental Implants - photograph by James Aiken \n",
      "https://t.co/EG6Xzq6ap7 \n",
      "A Dental Training kit entertains our‚Ä¶]\n",
      "pp: [rt mentionname medical curios dental implants photograph by james aiken urlurl a dental training kit entertains our ]\n",
      "\n",
      "tw: [Ideal Dental Solutions Offers Dental Implants in Arlington, VA https://t.co/4lIlxEVMwZ]\n",
      "pp: [ideal dental solutions offers dental implants in arlington va urlurl ]\n",
      "\n",
      "tw: [Amazing dentistry ‚Äì dental implants https://t.co/7A9Pd5HRrl https://t.co/0NVFD5U8kZ]\n",
      "pp: [amazing dentistry dental implants urlurl ]\n",
      "\n",
      "tw: [RT @RcasdizKitty: Who Is a Candidate for Dental Implants? | Psychreg https://t.co/dGC2t0Hqzc]\n",
      "pp: [rt mentionname who is a candidate for dental implants questionmark psychreg urlurl ]\n",
      "\n",
      "tw: [Sinus Surgery for Dental Implants - All On 4 Dental Implants \n",
      "\n",
      "#dentalimplants #dentist #dentistry #dentalcare #australia\n",
      "\n",
      "https://t.co/QGnvVseRmX https://t.co/BEnuaXZkqm]\n",
      "pp: [sinus surgery for dental implants all on numbers dental implants dentalimplants dentist dentistry dentalcare australia urlurl ]\n",
      "\n",
      "tw: [Dental implant screw removal kit https://t.co/ZYzNAL6RC9 @eBay]\n",
      "pp: [dental implant screw removal kit urlurl ]\n",
      "\n",
      "tw: [MISSING A TOOTH? Ask us about dental implants! https://t.co/rkiugA4QRq]\n",
      "pp: [missing a tooth questionmark ask us about dental implants urlurl ]\n",
      "\n",
      "tw: [Dr. Steven Eckert, Director of Clinical Research &amp; Development @ClearChoice Dental Implant Centers, receives third prestigious award in 2017 for his leadership in prosthodontics and implant dentistry. https://t.co/8arAEXXh6n https://t.co/JXf3mtkM2h]\n",
      "pp: [dr steven eckert director of clinical research development mentionname dental implant centers receives third prestigious award in numbers for his leadership in prosthodontics and implant dentistry urlurl ]\n",
      "\n",
      "tw: [Dental Implant Melbourne Now Announces To Offer the Latest Prosthodontics Treatments https://t.co/adQPdufIZJ https://t.co/O7CXyQvKli]\n",
      "pp: [dental implant melbourne now announces to offer the latest prosthodontics treatments urlurl ]\n",
      "\n",
      "tw: [Don‚Äôt avoid those summer BBQ‚Äôs because of ill fitting, uncomfortable dentures! Speak to us about dental implants today #Leicester]\n",
      "pp: [don t avoid those summer bbq s because of ill fitting uncomfortable dentures speak to us about dental implants today leicester]\n",
      "\n",
      "tw: [RT @click4teeth: Did you know it's possible to replace a whole arch of failing or missing teeth with dental implants in a single visit? Lea‚Ä¶]\n",
      "pp: [rt mentionname did you know it s possible to replace a whole arch of failing or missing teeth with dental implants in a single visit questionmark lea ]\n",
      "\n",
      "tw: [Improve Your Smile With Best Dental Implants Surgery #dental #care https://t.co/1iUr5tfMjE]\n",
      "pp: [improve your smile with best dental implants surgery dental care urlurl ]\n",
      "\n",
      "tw: [#dental implant cleaning cyber schooling]\n",
      "pp: [ dental implant cleaning cyber schooling]\n",
      "\n",
      "tw: [Are @dentalimplants Treatment Right For Me?   What Vita #Dentist Professions says about Dental Implants selection? https://t.co/iQ1eukVnA7]\n",
      "pp: [are mentionname treatment right for me questionmark what vita dentist professions says about dental implants selection questionmark urlurl ]\n",
      "\n",
      "tw: [Best Waukesha Dentist for Dental Implants, Veneers, Whitening and more https://t.co/9EU518UUtQ]\n",
      "pp: [best waukesha dentist for dental implants veneers whitening and more urlurl ]\n",
      "\n",
      "tw: [World Dental Implants and Orthodontics Congress 2019 in August Tokyo https://t.co/1qlsX72LSS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pp: [world dental implants and orthodontics congress numbers in august tokyo urlurl ]\n",
      "\n",
      "tw: [Dentures With Dental Implants Leicester &amp;¬†Loughborough https://t.co/iWv99XtdG6]\n",
      "pp: [dentures with dental implants leicester loughborough urlurl ]\n",
      "\n",
      "tw: [Have a missing tooth? Contact our office to learn about dental implants today! https://t.co/GxkcX6xHmd]\n",
      "pp: [have a missing tooth questionmark contact our office to learn about dental implants today urlurl ]\n",
      "\n",
      "tw: [Personal Care for Dental Implants. Find out more https://t.co/ZNcAsJzIpT #DentalImplants]\n",
      "pp: [personal care for dental implants find out more urlurl ]\n",
      "\n",
      "tw: [\"Dental Implants\" https://t.co/9yGviDjKUl]\n",
      "pp: [ dental implants urlurl ]\n",
      "\n",
      "tw: [Is there something about your smile you are less than proud of? Dental implants may be the solution! We will help you regain your confidence from your smile! https://t.co/1AYxej1aGq]\n",
      "pp: [is there something about your smile you are less than proud of questionmark dental implants may be the solution we will help you regain your confidence from your smile urlurl ]\n",
      "\n",
      "tw: [Have You Lost a Tooth or Teeth? The Cost of Affordable Dental Implants  https://t.co/kLNSG5q5lM]\n",
      "pp: [have you lost a tooth or teeth questionmark the cost of affordable dental implants urlurl ]\n",
      "\n",
      "tw: [Decoding Dental Implants With Manhattan‚Äôs Top Dentist https://t.co/ig0tFtUEDA]\n",
      "pp: [decoding dental implants with manhattan s top dentist urlurl ]\n",
      "\n",
      "tw: [Are dental implants right for you? Get in touch with a local implant dentist today: \n",
      "\n",
      "https://t.co/LYGoVIyrVX https://t.co/j4oVS3BEui]\n",
      "pp: [are dental implants right for you questionmark get in touch with a local implant dentist today urlurl ]\n",
      "\n",
      "tw: [Cost of dental implants in Longmont CO: https://t.co/MwpxsClfmm #dentist #longmontdentalimplants]\n",
      "pp: [cost of dental implants in longmont co urlurl ]\n",
      "\n",
      "tw: [At #portmore, we have been placing &amp; restoring dental implants for many years. https://t.co/u8tgXJUwQ9]\n",
      "pp: [at portmore we have been placing restoring dental implants for many years urlurl ]\n",
      "\n",
      "tw: [Nervous Dental Patients and Sedation: Dental Implants in North London https://t.co/L586c9i078 https://t.co/Is6PhnVJTt]\n",
      "pp: [nervous dental patients and sedation dental implants in north london urlurl ]\n",
      "\n",
      "tw: [RT @DrGorczyca: Now or Never- Orthodontics Prior to Dental Implants https://t.co/KakEA4HovP #orthodontics #dentalimplants https://t.co/WjyX‚Ä¶]\n",
      "pp: [rt mentionname now or never orthodontics prior to dental implants urlurl ]\n",
      "\n",
      "tw: [When it comes to tooth replacement options, dental implants are your best bet! The longevity &amp; practicality of these little guys make them a breeze for patients compared to other options like removable dentures. https://t.co/9eZJt7L9d2]\n",
      "pp: [when it comes to tooth replacement options dental implants are your best bet the longevity practicality of these little guys make them a breeze for patients compared to other options like removable dentures urlurl ]\n",
      "\n",
      "tw: [@DollyRayDigital @meet_captureme  \"I really need to get some dental implants\"]\n",
      "pp: [ mentionname mentionname i really need to get some dental implants ]\n",
      "\n",
      "tw: [Have you been considering dental implants? Learn all about them in this article. https://t.co/raYLCL72IS]\n",
      "pp: [have you been considering dental implants questionmark learn all about them in this article urlurl ]\n",
      "\n",
      "tw: [RT @thenbodentist: What Are Dental Implants? \n",
      "scheduling your #dental appointments!call us 254725526047/+254732690149\n",
      "#NBOdentist‚Ä¶ ]\n",
      "pp: [rt mentionname what are dental implants questionmark scheduling your dental appointments call us numbers numbers nbodentist ]\n",
      "\n",
      "tw: [Dental Implants in Buffalo, NY https://t.co/CyJJDGOjqV #Buffalo #Dentist https://t.co/RLSciR755w]\n",
      "pp: [dental implants in buffalo ny urlurl ]\n",
      "\n",
      "tw: [Dental Implant https://t.co/cP2BiWPt5A]\n",
      "pp: [dental implant urlurl ]\n",
      "\n",
      "tw: [Dental implants are One Third the price in Mexico, with the same high quality as in the US https://t.co/FP5Dtlnu9j]\n",
      "pp: [dental implants are one third the price in mexico with the same high quality as in the us urlurl ]\n",
      "\n",
      "tw: [Get Dental Implants Treatment in New Delhi, India at Dr.Kathuria's Multispeciality Dental Clinic #Dentalimplants https://t.co/ylYs1fnXKi]\n",
      "pp: [get dental implants treatment in new delhi india at dr kathuria s multispeciality dental clinic dentalimplants urlurl ]\n",
      "\n",
      "tw: [@HoarseWisperer I finally got my dental implants procedure completed today! I'm smiling (and chewing) all day.  #OneGoodThing https://t.co/UUbhXShfrM]\n",
      "pp: [ mentionname i finally got my dental implants procedure completed today i m smiling and chewing all day onegoodthing urlurl ]\n",
      "\n",
      "tw: [RT @ErmaWaters20: Hate Dentures? Dental Implants May be the Answer - The Frisky https://t.co/lOik6z1yM2]\n",
      "pp: [rt mentionname hate dentures questionmark dental implants may be the answer the frisky urlurl ]\n",
      "\n",
      "tw: [Dr. David McIntyre, Dentist in Kyle, TX, Completes Latest Dental Implant Training at the Engel Institute https://t.co/CzBV2xHf9Q]\n",
      "pp: [dr david mcintyre dentist in kyle tx completes latest dental implant training at the engel institute urlurl ]\n",
      "\n",
      "tw: [#delaware corporate agents dental implants for multiple teeth]\n",
      "pp: [ delaware corporate agents dental implants for multiple teeth]\n",
      "\n",
      "tw: [@eric_d_williams Received his dental implant this week, no?]\n",
      "pp: [ mentionname received his dental implant this week no questionmark ]\n",
      "\n",
      "tw: [Dental Implants: Thier types and Costs #DentalImplants c\n",
      "https://t.co/M3bRRwahkU]\n",
      "pp: [dental implants thier types and costs dentalimplants c urlurl ]\n",
      "\n",
      "tw: [It's time to choose your food and not let it choose you! Dental implants can be the answer! Call today!  https://t.co/zxUVC9c6r8 https://t.co/oVlA8hkexG]\n",
      "pp: [it s time to choose your food and not let it choose you dental implants can be the answer call today urlurl ]\n",
      "\n",
      "tw: [Your permanent solution for missing teeth that fit your active style. For more information on dental implants,... https://t.co/kd0em9NEi7]\n",
      "pp: [your permanent solution for missing teeth that fit your active style for more information on dental implants urlurl ]\n",
      "\n",
      "tw: [Just filmed a dental implant surgery and üò∑]\n",
      "pp: [just filmed a dental implant surgery and ]\n",
      "\n",
      "tw: [\"All On 4 Dental Implants vs. Traditional Dental Implants\" https://t.co/4RC4UNkXYT #dentalimplants #woodlandhillsdentist #allon4 #fosi https://t.co/k4tXI9um86]\n",
      "pp: [ all on numbers dental implants vs traditional dental implants urlurl ]\n",
      "\n",
      "tw: [#new dental implant technology tech support for macbook pro]\n",
      "pp: [ new dental implant technology tech support for macbook pro]\n",
      "\n",
      "tw: [Dentistry is turning to robotics for help, as the world‚Äôs first dental implant by robot was performed https://t.co/xmiip7EkdB¬† via ‚Ä¶]\n",
      "pp: [dentistry is turning to robotics for help as the world s first dental implant by robot was performed urlurl ]\n",
      "\n",
      "tw: [RT @BendBdxlsir: How Much Do Dental Implants Cost in Thailand? - Dental Guide Australia https://t.co/DQE3hd8G1m]\n",
      "pp: [rt mentionname how much do dental implants cost in thailand questionmark dental guide australia urlurl ]\n",
      "\n",
      "tw: [We have huge experience with all manner of dental implant systems ‚Äì you only have to ask!]\n",
      "pp: [we have huge experience with all manner of dental implant systems you only have to ask ]\n",
      "\n",
      "tw: [dental implants abutment analog lab \n",
      "https://t.co/Q6GtxnPkp3]\n",
      "pp: [dental implants abutment analog lab urlurl ]\n",
      "\n",
      "tw: [Global Dental Implants Market Is Expected To Reach US$ 8.4 Bn By 2025 ‚Äì Credence Research https://t.co/GKy7kkIIAb https://t.co/KhTmMTmu2s]\n",
      "pp: [global dental implants market is expected to reach us moneymark numbers numbers bn by numbers credence research urlurl ]\n",
      "\n",
      "tw: [Dental Implants and Prosthetics Market Top Key Manufacturer Detail, Product Types, Size and Demand by 2023 https://t.co/O1SkKiXO38]\n",
      "pp: [dental implants and prosthetics market top key manufacturer detail product types size and demand by numbers urlurl ]\n",
      "\n",
      "tw: [Benefits Of Dental Implants - Happy Teeth And Gums https://t.co/Zt5huazK9S]\n",
      "pp: [benefits of dental implants happy teeth and gums urlurl ]\n",
      "\n",
      "tw: [Here are 3 Benefits of Dental Implants. Visit our office to find out if dental implants are right for you. https://t.co/0GfUkzUVnZ]\n",
      "pp: [here are numbers benefits of dental implants visit our office to find out if dental implants are right for you urlurl ]\n",
      "\n",
      "tw: [Dental implants can be a smile saver! Call our office to schedule your consultation! (940) 264-6000 https://t.co/AM1kByByqd]\n",
      "pp: [dental implants can be a smile saver call our office to schedule your consultation numbers numbers numbers urlurl ]\n",
      "\n",
      "tw: [Dental Implant Professionals Now Offers Australian Approved High Quality Dental Implants https://t.co/GYjyxluCng https://t.co/hm5pWYEptb]\n",
      "pp: [dental implant professionals now offers australian approved high quality dental implants urlurl ]\n",
      "\n",
      "tw: [AD Surgical Announces the Release of a New Universal Dental Implant Drill Stopper System https://t.co/bAnqxh5sHH]\n",
      "pp: [ad surgical announces the release of a new universal dental implant drill stopper system urlurl ]\n",
      "\n",
      "tw: [Emergency Dentist Richardson\n",
      "Mirsepasi Dentistry in Richardson, TX, offers Dental Implants, Dentures, Crowns....\n",
      "https://t.co/Hcv0TSWjH0 https://t.co/moDgLDE0j4]\n",
      "pp: [emergency dentist richardson mirsepasi dentistry in richardson tx offers dental implants dentures crowns urlurl ]\n",
      "\n",
      "tw: [PCCD designs each dental implant to blend and align perfectly with the surrounding teeth. They feel and function just like your natural teeth.\n",
      "\n",
      "#smile #teeth #cosmeticdentistry #losaltosdentist #sanjose #sanjosedentist #bayareadentist #bayarea #losaltos #CA #dentalimplants #PCC https://t.co/JwJzeW6h3Q]\n",
      "pp: [pccd designs each dental implant to blend and align perfectly with the surrounding teeth they feel and function just like your natural teeth smile teeth cosmeticdentistry losaltosdentist sanjose sanjosedentist bayareadentist bayarea losaltos ca dentalimplants pcc urlurl ]\n",
      "\n",
      "tw: [RT @smilemaker19116: Dental Implants Dentist In Philadelphia.\n",
      "#dentist #dentalimplant #dentistry #philadelphia #teeth #smile #healthcare #a‚Ä¶]\n",
      "pp: [rt mentionname dental implants dentist in philadelphia dentist dentalimplant dentistry philadelphia teeth smile healthcare a ]\n",
      "\n",
      "tw: [RT @DenPerfectCov: Dental implants are life enhancing.  They are the long term solution to tooth loss #Coventry]\n",
      "pp: [rt mentionname dental implants are life enhancing they are the long term solution to tooth loss coventry]\n",
      "\n",
      "tw: [#dental implants minnesota ford dealers in huntsville al]\n",
      "pp: [ dental implants minnesota ford dealers in huntsville al]\n",
      "\n",
      "tw: [Get dental Implants In Short Hills NJ With An Oral Surgeon - https://t.co/ABQZXlMOrK]\n",
      "pp: [get dental implants in short hills nj with an oral surgeon urlurl ]\n",
      "\n",
      "tw: [Dental Implants: Cost, Options, Recovery &amp; Risk https://t.co/yDNkWqvF8Y https://t.co/m1Uz60hfAs]\n",
      "pp: [dental implants cost options recovery risk urlurl ]\n",
      "\n",
      "tw: [How can dental implants benefit you? #implants #truro #cornwall https://t.co/KaLq4rWk4K]\n",
      "pp: [how can dental implants benefit you questionmark implants truro cornwall urlurl ]\n",
      "\n",
      "tw: [45 years of dental implant education. https://t.co/OLX8JLXOG6   @ICOIImplants Contact me for information.   membership@icoi.org https://t.co/iAwDln5Y5C]\n",
      "pp: [ numbers years of dental implant education urlurl ]\n",
      "\n",
      "tw: [TIL that a Chinese robot dentist is the first to install dental implants into a patient's mouth without human assi‚Ä¶ https://t.co/lOw0srp5fI]\n",
      "pp: [til that a chinese robot dentist is the first to install dental implants into a patient s mouth without human assi urlurl ]\n",
      "\n",
      "tw: [New post (Why dental implants are the treatment of choice for missing teeth ‚Äì dental implants Staten Island and... https://t.co/LSGLvwuFvb]\n",
      "pp: [new post why dental implants are the treatment of choice for missing teeth dental implants staten island and urlurl ]\n",
      "\n",
      "tw: [Advantages Of The Dental Implants Maui Gives https://t.co/mQ2cbpKFKB]\n",
      "pp: [advantages of the dental implants maui gives urlurl ]\n",
      "\n",
      "tw: [Patient Intake Coordinator - ClearChoice Dental Implant Centers - Atlanta, GA https://t.co/1uoG5t98UO Jobs Atlanta]\n",
      "pp: [patient intake coordinator clearchoice dental implant centers atlanta ga urlurl ]\n",
      "\n",
      "tw: [Missing teeth? Have you considered #Dental Implants https://t.co/x6rMHoXrW1 #WinchmoreHill https://t.co/du5hLHUo1Q]\n",
      "pp: [missing teeth questionmark have you considered dental implants urlurl ]\n",
      "\n",
      "tw: [RT @ModernDentalDrB: Are you thinking about dental implants? üëÑ üòÉ \n",
      "\r",
      " It is so important you stay focused on the aftercare! üëç \n",
      "\n",
      "\r",
      " #Implants #‚Ä¶]\n",
      "pp: [rt mentionname are you thinking about dental implants questionmark it is so important you stay focused on the aftercare implants ]\n",
      "\n",
      "tw: [Ask us about the difference between natural tooth and dental implant‚Ä¶ https://t.co/JfbieAbdj8]\n",
      "pp: [ask us about the difference between natural tooth and dental implant urlurl ]\n",
      "\n",
      "tw: [If you are missing a few‚Äîbut only a few‚Äîteeth you may be wondering whether a bridge or a dental implant would... https://t.co/ccRtC16Oy1]\n",
      "pp: [if you are missing a few but only a few teeth you may be wondering whether a bridge or a dental implant would urlurl ]\n",
      "\n",
      "tw: [Tooth implant Sydney offers $1500 high quality dental implants https://t.co/oarRys2cwj https://t.co/ifcQ6PpLJ7]\n",
      "pp: [tooth implant sydney offers moneymark numbers high quality dental implants urlurl ]\n",
      "\n",
      "tw: [With a 98% success rate, dental implants are your permanent solution to missing teeth. Give us a call to learn... https://t.co/p9EOpkbhbY]\n",
      "pp: [with a numbers success rate dental implants are your permanent solution to missing teeth give us a call to learn urlurl ]\n",
      "\n",
      "tw: [#office chairs on wheels dental implants los gatos]\n",
      "pp: [ office chairs on wheels dental implants los gatos]\n",
      "\n",
      "tw: [Pointers On Affordable Dental Implants Michigan https://t.co/0CHj2VyF4y]\n",
      "pp: [pointers on affordable dental implants michigan urlurl ]\n",
      "\n",
      "tw: [Get your implants from the leader in dental implantology. Let us help you bring back your smile. #HughesSmile https://t.co/vuidKhqYYD]\n",
      "pp: [get your implants from the leader in dental implantology let us help you bring back your smile hughessmile urlurl ]\n",
      "\n",
      "tw: [Natural appearance &amp; biocompatibility are just 2 of the reasons we recommend ceramic #dental implants over titanium. https://t.co/pDmQ7vT7v2]\n",
      "pp: [natural appearance biocompatibility are just numbers of the reasons we recommend ceramic dental implants over titanium urlurl ]\n",
      "\n",
      "tw: [RT @TheDentalDaily: Dental Implants - By Dr MOUNICA MAGANTI https://t.co/UfBDGCH8O9 https://t.co/f3Lp3hjtzF]\n",
      "pp: [rt mentionname dental implants by dr mounica maganti urlurl ]\n",
      "\n",
      "tw: [Ever wondered how long dental implants last? Learn all about the components, benefits, and lifespan of implants!... https://t.co/akKOSrsrNF]\n",
      "pp: [ever wondered how long dental implants last questionmark learn all about the components benefits and lifespan of implants urlurl ]\n",
      "\n",
      "tw: [Dental implants have some big advantages over dentures. They look natural and can last a lifetime, but that's not... https://t.co/KlPrFn8cPa]\n",
      "pp: [dental implants have some big advantages over dentures they look natural and can last a lifetime but that s not urlurl ]\n",
      "\n",
      "tw: [From whitening, through to veneers all the way to dental implants ‚Äì Let us show you how we can improve your smile #Coventry]\n",
      "pp: [from whitening through to veneers all the way to dental implants let us show you how we can improve your smile coventry]\n",
      "\n",
      "tw: [RT @mostjij21: The best country for dental implants - dental implants https://t.co/qnx5Ty4hq3]\n",
      "pp: [rt mentionname the best country for dental implants dental implants urlurl ]\n",
      "\n",
      "tw: [RT @allenasamrxtg54: Tips to Locate the Best Dentist near you - Pisa Dental Implants https://t.co/XjYKjyHKVs]\n",
      "pp: [rt mentionname tips to locate the best dentist near you pisa dental implants urlurl ]\n",
      "\n",
      "tw: [RT @orlandokgw: Some of media scrum w/Jusuf Nurkic on wearing face mask. Healing from dental implants (teeth knocked out vs Toronto‚Ä¶ ]\n",
      "pp: [rt mentionname some of media scrum w jusuf nurkic on wearing face mask healing from dental implants teeth knocked out vs toronto ]\n",
      "\n",
      "tw: [Why, with Dental Implants, You Get Back So Much More Than Just Your Smile https://t.co/YxkiOEDngJ https://t.co/KbSYXxVKcr]\n",
      "pp: [why with dental implants you get back so much more than just your smile urlurl ]\n",
      "\n",
      "tw: [RT @MariaKe45: DENTAL IMPLANTS: SURGERY, COST, ADVANTAGES AND RISKS https://t.co/SCtvRbPM3o]\n",
      "pp: [rt mentionname dental implants surgery cost advantages and risks urlurl ]\n",
      "\n",
      "tw: [Dental Implants Scunthorpe | Dental Implant And Dentistry Procedure https://t.co/JhNwq2T0zG]\n",
      "pp: [dental implants scunthorpe dental implant and dentistry procedure urlurl ]\n",
      "\n",
      "tw: [At Orchard Lane Dental we have extensive experience of Placing Dental Implants to fill your awkward gaps 0146073250 https://t.co/y6j5U6iNuA]\n",
      "pp: [at orchard lane dental we have extensive experience of placing dental implants to fill your awkward gaps numbers urlurl ]\n",
      "\n",
      "tw: [Are you considering dental implants? See why dentures might be better! #Oralcare #Denture #DentalImplant https://t.co/rYhX8gy7lI]\n",
      "pp: [are you considering dental implants questionmark see why dentures might be better oralcare denture dentalimplant urlurl ]\n",
      "\n",
      "tw: [The Importance of Cosmetic Dentistry for Braces and Dental Implant Treatments https://t.co/I8lInTsoQV]\n",
      "pp: [the importance of cosmetic dentistry for braces and dental implant treatments urlurl ]\n",
      "\n",
      "tw: [Are Dental Implants The Right Option For You? https://t.co/OD2Xq8sDYO]\n",
      "pp: [are dental implants the right option for you questionmark urlurl ]\n",
      "\n",
      "tw: [RT @Cisco_research: A Brief History of Dental Implants.My contribution @for_org #dentistry  https://t.co/Jf3DlLkMwS https://t.co/khWRGX2X4m]\n",
      "pp: [rt mentionname a brief history of dental implants my contribution mentionname dentistry urlurl ]\n",
      "\n",
      "tw: [RT @RogueLegFiscal: Fun fact: All of Steve Bannon's dental implants are made out of @MattRosendale's spine. #mtpol #mtleg]\n",
      "pp: [rt mentionname fun fact all of steve bannon s dental implants are made out of mentionname s spine mtpol mtleg]\n",
      "\n",
      "tw: [@moorsmiles Can't  recommend @moorsmiles highly enough. Must be the only place to go for dental implants.]\n",
      "pp: [ mentionname can t recommend mentionname highly enough must be the only place to go for dental implants ]\n",
      "\n",
      "tw: [Affordable Dental Implants Fort Collins CO 970-785-8009 Fort Collins Mini Implants by Dr. Dorian https://t.co/rc2K6k3EOe]\n",
      "pp: [affordable dental implants fort collins co numbers numbers numbers fort collins mini implants by dr dorian urlurl ]\n",
      "\n",
      "tw: [https://t.co/vKi3tWan47\n",
      "Cheap Dental Implants For a Healthy Smile]\n",
      "pp: [ urlurl cheap dental implants for a healthy smile]\n",
      "\n",
      "tw: [August is Dental Implant Month Implant Practice US https://t.co/rSXrfY8EHT https://t.co/APVl8o5EXs]\n",
      "pp: [august is dental implant month implant practice us urlurl ]\n",
      "\n",
      "tw: [Dental implants are so comfortable and natural looking that you may not eve know you lost a tooth!... https://t.co/KvB0DErbkK]\n",
      "pp: [dental implants are so comfortable and natural looking that you may not eve know you lost a tooth urlurl ]\n",
      "\n",
      "tw: [With dental implants, you'll be back in business in no time. üëî https://t.co/Lb0p3Ii1qC https://t.co/VaJHneUduJ]\n",
      "pp: [with dental implants you ll be back in business in no time urlurl ]\n",
      "\n",
      "tw: [San Francisco Dental Implant Center Announces Internet Success with... - https://t.co/4T5tmokmXS https://t.co/ZfdwI34LWh]\n",
      "pp: [san francisco dental implant center announces internet success with urlurl ]\n",
      "\n",
      "tw: [Read our book on dental implants (usual price ¬£15) and find our about what they are and how they can be used to... https://t.co/hpVYNzKCBp]\n",
      "pp: [read our book on dental implants usual price numbers and find our about what they are and how they can be used to urlurl ]\n",
      "\n",
      "tw: [dental surgery and dental implant marketplace - open to bidding by johnwillardsen https://t.co/Pkz1uXur0z https://t.co/jdn8amclWD]\n",
      "pp: [dental surgery and dental implant marketplace open to bidding by johnwillardsen urlurl ]\n",
      "\n",
      "tw: [Dental Implants Can Improve Your Smile https://t.co/VmCgUbLnK2]\n",
      "pp: [dental implants can improve your smile urlurl ]\n",
      "\n",
      "tw: [Dental Implants In Newcastle: Advantages - Next Gen Bloggers https://t.co/9qPXd2uXoa]\n",
      "pp: [dental implants in newcastle advantages next gen bloggers urlurl ]\n",
      "\n",
      "tw: [RT @protectjaebum: Driving past a dental implant center and thought of jaebum. I really love my baby boy so much https://t.co/H1rqf4P6wG]\n",
      "pp: [rt mentionname driving past a dental implant center and thought of jaebum i really love my baby boy so much urlurl ]\n",
      "\n",
      "tw: [Here are some great reasons why you should consider All-on-4 dental implants!¬†https://t.co/ggxWuYjSVI https://t.co/517vnr22aJ]\n",
      "pp: [here are some great reasons why you should consider all on numbers dental implants urlurl ]\n",
      "\n",
      "tw: [#dental implants phoenix arizona engineering sheet metal]\n",
      "pp: [ dental implants phoenix arizona engineering sheet metal]\n",
      "\n",
      "tw: [RT @dibelousova2236: How to Find Low Cost Dental Implants #discount #dental #implants https://t.co/bfAHtAMhQs]\n",
      "pp: [rt mentionname how to find low cost dental implants discount dental implants urlurl ]\n",
      "\n",
      "tw: [#investigation company 4 on 4 dental implants cost]\n",
      "pp: [ investigation company numbers on numbers dental implants cost]\n",
      "\n",
      "tw: [Missing Teeth? Discuss Dental Implants with a Dentist In Mount Prospect https://t.co/LAiuZeFl2Y via @]\n",
      "pp: [missing teeth questionmark discuss dental implants with a dentist in mount prospect urlurl ]\n",
      "\n",
      "tw: [Cosmetic Dentist San Diego, Encinitas, CA: New Interview on Dental Implants https://t.co/sFGKI7wf2Z https://t.co/MPLUjlZ7t7]\n",
      "pp: [cosmetic dentist san diego encinitas ca new interview on dental implants urlurl ]\n",
      "\n",
      "tw: [\"The origin of dental implants\" via @TahoeDailyTrib https://t.co/AUwieIfdiX https://t.co/LJp2ZpqMe7]\n",
      "pp: [ the origin of dental implants via mentionname urlurl ]\n",
      "\n",
      "tw: [#SheenDental Can I Have Dental Implants If I've Suffered Bone Loss?. Read Blog: https://t.co/CasUT2HPDc]\n",
      "pp: [ sheendental can i have dental implants if i ve suffered bone loss questionmark read blog urlurl ]\n",
      "\n",
      "tw: [X-Cube Dental Implant Surgery Motor Complete-  High Tech. Low Price-FDA Approv. https://t.co/TJpvt3ZMmB https://t.co/0I41laH3La]\n",
      "pp: [x cube dental implant surgery motor complete high tech low price fda approv urlurl ]\n",
      "\n",
      "tw: [RT @sergeydenyvi: Dental Implants -Missing Teeth ‚Äì Dentiq Dentistry #implant #dentistry https://t.co/RMhyMVuXdU]\n",
      "pp: [rt mentionname dental implants missing teeth dentiq dentistry implant dentistry urlurl ]\n",
      "\n",
      "tw: [Oasis Family Dental Offers the Latest Technologies in Dental Implant Services https://t.co/BS4MjkNdn0]\n",
      "pp: [oasis family dental offers the latest technologies in dental implant services urlurl ]\n",
      "\n",
      "tw: [Eat that apple, talk with confidence, and never worry about your teeth with dental implants. Learn the advantages... https://t.co/TigNQWAH1B]\n",
      "pp: [eat that apple talk with confidence and never worry about your teeth with dental implants learn the advantages urlurl ]\n",
      "\n",
      "tw: [Learn More About Dental Implants in Suffolk County NY | https://t.co/1hzQbyHOLs]\n",
      "pp: [learn more about dental implants in suffolk county ny urlurl ]\n",
      "\n",
      "tw: [What All You Need To Know About Dental Implants? Find here at our website:\n",
      "\n",
      "https://t.co/IYGfjvv1gJ‚Ä¶/What-All-You-Need-To‚Ä¶\n",
      "\n",
      "#Ahmedabad #DentalImplant #DentalClinic https://t.co/mkiTQCU9zA]\n",
      "pp: [what all you need to know about dental implants questionmark find here at our website urlurl ahmedabad dentalimplant dentalclinic urlurl ]\n",
      "\n",
      "tw: [Dental Implant Melbourne Helps Edentulous to Find High Quality Dental Implant https://t.co/TgvJGIngCq https://t.co/UouSQvCrB7]\n",
      "pp: [dental implant melbourne helps edentulous to find high quality dental implant urlurl ]\n",
      "\n",
      "tw: [$ALSGD announce technology for dental implant placement https://t.co/2ANbaTJwNK  #WSJ #nytimes #reuters #bloomberg #thestreet #jimmyfallon #forbes #nasdaq #chicago #digitalmarkiting #ihub #newyork  #social #networking #business  #cnn #bet #foxnews https://t.co/9E2e0DRnNO]\n",
      "pp: [ moneymark alsgd announce technology for dental implant placement urlurl ]\n",
      "\n",
      "tw: [Gary L. Cash, DDS ‚Äì dental implants austin https://t.co/LxcDoeTDe0]\n",
      "pp: [gary l cash dds dental implants austin urlurl ]\n",
      "\n",
      "tw: [#London ¬£895; Titanium Dental Implant and Crown, Harley St https://t.co/7UKjxrrVk6\n",
      "\n",
      "Transform your smile with a dental implant and crown. You'll get a sterile titanium implant surgically fitted on your first visit. Followed by abutment and crown two mont‚Ä¶ https://t.co/1kQqpbIe3C]\n",
      "pp: [ london numbers titanium dental implant and crown harley st urlurl transform your smile with a dental implant and crown you ll get a sterile titanium implant surgically fitted on your first visit followed by abutment and crown two mont urlurl ]\n",
      "\n",
      "tw: [@_embutler Yeah I got a dental implant too but that's fine, it's the bottom wisdom teeth that are causing me hell and I'm literally going on 3 weeks]\n",
      "pp: [ mentionname yeah i got a dental implant too but that s fine it s the bottom wisdom teeth that are causing me hell and i m literally going on numbers weeks]\n",
      "\n",
      "tw: [This coming new year, resolve to improve your smile. Call for a consultation to find out if Dental Implants ar https://t.co/fMssX9rMVe]\n",
      "pp: [this coming new year resolve to improve your smile call for a consultation to find out if dental implants ar urlurl ]\n",
      "\n",
      "tw: [RT @_captainpike: Idea: all teeth removed when fully grown. Replaced with one solid dental implant. No more flossing]\n",
      "pp: [rt mentionname idea all teeth removed when fully grown replaced with one solid dental implant no more flossing]\n",
      "\n",
      "tw: [@AndreaHorwath I need a full mouth of dental implants!]\n",
      "pp: [ mentionname i need a full mouth of dental implants ]\n",
      "\n",
      "tw: [I've just posted on my Blog about: Important Information On Dental Implants Maui https://t.co/Njq11FofBK]\n",
      "pp: [i ve just posted on my blog about important information on dental implants maui urlurl ]\n",
      "\n",
      "tw: [Having Dental Implants is much more than merely being about looks; they also provide functional improvements. #LetsSmileAgain https://t.co/gu5IZFuo4i]\n",
      "pp: [having dental implants is much more than merely being about looks they also provide functional improvements letssmileagain urlurl ]\n",
      "\n",
      "tw: [Applications of Dental Implants https://t.co/MBT06eEZsW]\n",
      "pp: [applications of dental implants urlurl ]\n",
      "\n",
      "tw: [Get in touch for a FREE consultation to discuss #dental implants - 061 480 070. \n",
      "#limerickpost #dentist #limerick https://t.co/3txQsxLCi4]\n",
      "pp: [get in touch for a free consultation to discuss dental implants numbers numbers numbers limerickpost dentist limerick urlurl ]\n",
      "\n",
      "tw: [Clinico-Pathological Evaluation of Malignancy Adjacent to Dental Implants https://t.co/u5d7PHnJ9c]\n",
      "pp: [clinico pathological evaluation of malignancy adjacent to dental implants urlurl ]\n",
      "\n",
      "tw: [How will I feel after the dental implant treatment? - PermaDontics https://t.co/Y0WgrDAcrx]\n",
      "pp: [how will i feel after the dental implant treatment questionmark permadontics urlurl ]\n",
      "\n",
      "tw: [Involved in Dental Implants? You may be losing out on an important Tax Refund - Discover more https://t.co/i2EPVxYVkM]\n",
      "pp: [involved in dental implants questionmark you may be losing out on an important tax refund discover more urlurl ]\n",
      "\n",
      "tw: [\"Dental Implants, Crowns, Bridges, and Veneers: 8 Tips from Prosthodontists to Avoid Failures\" via @dentalaegis https://t.co/SmvA43MsV9 https://t.co/cVUh1DP2dj]\n",
      "pp: [ dental implants crowns bridges and veneers numbers tips from prosthodontists to avoid failures via mentionname urlurl ]\n",
      "\n",
      "tw: [Harthisha Dental Implants done the  720p https://t.co/M4uhNYnvES via @YouTube]\n",
      "pp: [harthisha dental implants done the numbers p urlurl ]\n",
      "\n",
      "tw: [#IslandDentalAz A Badly Decayed Molar Can Often Be Restored with a Dental Implant. Read Blog: https://t.co/B0xi2mCCR6]\n",
      "pp: [ islanddentalaz a badly decayed molar can often be restored with a dental implant read blog urlurl ]\n",
      "\n",
      "tw: [Dental Implants Market To Be Driven By Increasing Demand For Tooth Or Teeth Replacement Till 2024: Grand View‚Ä¶ https://t.co/g4WjcFqwW6 https://t.co/aaCHNttTbR]\n",
      "pp: [dental implants market to be driven by increasing demand for tooth or teeth replacement till numbers grand view urlurl ]\n",
      "\n",
      "tw: [Oral Care for dental implants. \n",
      "https://t.co/T2y1lm5BUQ https://t.co/iJWe1hpsf1]\n",
      "pp: [oral care for dental implants urlurl ]\n",
      "\n",
      "tw: [Patient Seminars on Dental Implants are a powerful way to attract more of the right patients to your practice! Tips: https://t.co/tGkLHSG032 https://t.co/EIlXNrlIlv]\n",
      "pp: [patient seminars on dental implants are a powerful way to attract more of the right patients to your practice tips urlurl ]\n",
      "\n",
      "tw: [@0vidius Is this it? Dr. Botbol. DDS. North York near Bayview and Sheppard. The Dentist of dental implants. https://t.co/F6igIJrAJj Good results. Best of luck! My poetry. I'm alive!]\n",
      "pp: [ mentionname is this it questionmark dr botbol dds north york near bayview and sheppard the dentist of dental implants urlurl ]\n",
      "\n",
      "tw: [#ZahnSply¬Æ #Dental #ImplantStudyModel #Chinese \n",
      "\n",
      "Dental Implant Disease Teeth Model with‚Ä¶ https://t.co/BKvx7WCr72]\n",
      "pp: [ zahnsply dental implantstudymodel chinese dental implant disease teeth model with urlurl ]\n",
      "\n",
      "tw: [#EastTennesseePeriodonticsRobertCain,Dds Are Dental Implants Always the Answer? Hardly!. Read Blog: https://t.co/QxbiUb54iP]\n",
      "pp: [ easttennesseeperiodonticsrobertcain dds are dental implants always the answer questionmark hardly read blog urlurl ]\n",
      "\n",
      "tw: [High Quality Las Vegas Dental Implants https://t.co/bA2JR5dJuR #cosmeticdentistry]\n",
      "pp: [high quality las vegas dental implants urlurl ]\n",
      "\n",
      "tw: [We have some cancellation appointments available tomorrow for a dental implant assessment with Dr Allsopp for... https://t.co/eO8iUFgFRs]\n",
      "pp: [we have some cancellation appointments available tomorrow for a dental implant assessment with dr allsopp for urlurl ]\n",
      "\n",
      "tw: [Dental Education Meets the Forefront of Automotive Innovation International Dental Implant Association ... https://t.co/gRivmsnDlb]\n",
      "pp: [dental education meets the forefront of automotive innovation international dental implant association urlurl ]\n",
      "\n",
      "tw: [Dr. Noh is a skilled implants dentist who is qualified to surgically place your dental implants, as well as... https://t.co/gVKadWXleF]\n",
      "pp: [dr noh is a skilled implants dentist who is qualified to surgically place your dental implants as well as urlurl ]\n",
      "\n",
      "tw: [How to Tell if You Need Dental Implants in Westchester County, NY | Healthy Smile Happy Smile https://t.co/6wInjWQCcW]\n",
      "pp: [how to tell if you need dental implants in westchester county ny healthy smile happy smile urlurl ]\n",
      "\n",
      "tw: [@jimpjorps @AllenVentano @edzitron but dental implants are metal? on both ends]\n",
      "pp: [ mentionname mentionname mentionname but dental implants are metal questionmark on both ends]\n",
      "\n",
      "tw: [Smokers who have had dental implant surgery - did you continue smoking immediately afterwards?]\n",
      "pp: [smokers who have had dental implant surgery did you continue smoking immediately afterwards questionmark ]\n",
      "\n",
      "tw: [#1 day dental implants wine bar houston]\n",
      "pp: [ numbers day dental implants wine bar houston]\n",
      "\n",
      "tw: [#edh deck building dental implant supplies]\n",
      "pp: [ edh deck building dental implant supplies]\n",
      "\n",
      "tw: [Overcome Tooth Loss With Dental Implants https://t.co/8WyTXHzSxm https://t.co/c5iiBOBGPv]\n",
      "pp: [overcome tooth loss with dental implants urlurl ]\n",
      "\n",
      "tw: [Straumann Compatible Dental Implants WN Impression Cap, height 8mm https://t.co/VsaYaDTSFE https://t.co/UsXLOcRJCW]\n",
      "pp: [straumann compatible dental implants wn impression cap height numbers mm urlurl ]\n",
      "\n",
      "tw: [First-Class Dental Implants Las Vegas https://t.co/mf7fKX9vYL #teethinaday]\n",
      "pp: [first class dental implants las vegas urlurl ]\n",
      "\n",
      "tw: [1 x Low Speed Hex Driver 1.25mm For Dental Implant,Implants, Abutment, Screws  https://t.co/jb9IrvqYQ1 https://t.co/bQwexdLF34]\n",
      "pp: [ numbers x low speed hex driver numbers numbers mm for dental implant implants abutment screws urlurl ]\n",
      "\n",
      "tw: [RT @CleoburyDental: We offer the latest techniques for #dental implants, jaw pain relief #orthodontics regular hygienist appointments as we‚Ä¶]\n",
      "pp: [rt mentionname we offer the latest techniques for dental implants jaw pain relief orthodontics regular hygienist appointments as we ]\n",
      "\n",
      "tw: [Dental implant basics: Oral hygiene instructions for multi-unit bridgework: https://t.co/Myj5WusuSU https://t.co/FAHqUnFWuQ]\n",
      "pp: [dental implant basics oral hygiene instructions for multi unit bridgework urlurl ]\n",
      "\n",
      "tw: [What to Know Before Your Dental Implant Procedure. by @DentistPalmBay #dentalimplants #restorativedentistry\n",
      "https://t.co/6hroFD9EO3]\n",
      "pp: [what to know before your dental implant procedure by mentionname dentalimplants restorativedentistry urlurl ]\n",
      "\n",
      "tw: [RT @dprinter365: #3dprinter Next generation of 3D printed medical &amp; dental implants could be made from PEEK https://t.co/y1R5ozG9xx #3d]\n",
      "pp: [rt mentionname numbers dprinter next generation of numbers d printed medical dental implants could be made from peek urlurl ]\n",
      "\n",
      "tw: [We at Veritaas Offer: Dental Implants, Smile Makeover, Full Mouth Restoration, Root Canal Treatment ( Single Sitting ), Dental Crowns &amp; Bridges, Laser Teeth Whitening, Invisible Braces &amp; Aligners, Kids Dental Treatment. https://t.co/IU8Wa6AeHM https://t.co/ilfrw88kzI]\n",
      "pp: [we at veritaas offer dental implants smile makeover full mouth restoration root canal treatment single sitting dental crowns bridges laser teeth whitening invisible braces aligners kids dental treatment urlurl ]\n",
      "\n",
      "tw: [2 pieces SANDENT 20:1 Dental Implant Reduction Low Speed Contra Angle Handpiece  https://t.co/tUMs9cM0Tr https://t.co/dDK2k6mHe2]\n",
      "pp: [ numbers pieces sandent numbers numbers dental implant reduction low speed contra angle handpiece urlurl ]\n",
      "\n",
      "tw: [How Dental Implants Protect Your Jawbone After Tooth Loss https://t.co/TWCVU9ODeM https://t.co/Zh2eBqDMFM]\n",
      "pp: [how dental implants protect your jawbone after tooth loss urlurl ]\n",
      "\n",
      "tw: [Dental implants is best way to replace missing teeth . One stop Dent..For more info visit... https://t.co/F0KUEzzfs8 https://t.co/3H1m0r4tqk]\n",
      "pp: [dental implants is best way to replace missing teeth one stop dent for more info visit urlurl ]\n",
      "\n",
      "tw: [Dr. Curry Leavitt Offers Guided Dental Implant Placement to Replace Missing Teeth in Las Vegas, NV https://t.co/YZy79jKuAb]\n",
      "pp: [dr curry leavitt offers guided dental implant placement to replace missing teeth in las vegas nv urlurl ]\n",
      "\n",
      "tw: [Dental Implants Grand Prairie TX - Dr. Behrooz Khademazad https://t.co/ovmWnGjOGj]\n",
      "pp: [dental implants grand prairie tx dr behrooz khademazad urlurl ]\n",
      "\n",
      "tw: [DO Dental Implants BREAK? - https://t.co/3AKO6XBfqW https://t.co/0aML4CLCBq]\n",
      "pp: [do dental implants break questionmark urlurl ]\n",
      "\n",
      "tw: [Stem Cell Dental Implants Grow New Teeth in Your Mouth https://t.co/f9cvTjvsTz via @YouTube]\n",
      "pp: [stem cell dental implants grow new teeth in your mouth urlurl ]\n",
      "\n",
      "tw: [#how much does a full set of dental implants cost gems tv online shop]\n",
      "pp: [ how much does a full set of dental implants cost gems tv online shop]\n",
      "\n",
      "tw: [Did you know that dental implants have an overall success rate of 95 percent? You can‚Äôt beat those odds. To read... https://t.co/D8Jk0scFy1]\n",
      "pp: [did you know that dental implants have an overall success rate of numbers percent questionmark you can t beat those odds to read urlurl ]\n",
      "\n",
      "tw: [Are You the One? Questions to Ask Before Opting For Dental Implant Dentist Abroad https://t.co/QO7b7vVP4M]\n",
      "pp: [are you the one questionmark questions to ask before opting for dental implant dentist abroad urlurl ]\n",
      "\n",
      "tw: [3 Reasons People Get Dental Implants https://t.co/R6xTfmakdb https://t.co/AaAG92lcCD]\n",
      "pp: [ numbers reasons people get dental implants urlurl ]\n",
      "\n",
      "tw: [Salem Cosmetic Dentistry &amp; Restorative Dentist Dental Implants Service Announced - Yahoo‚Ä¶ https://t.co/Fx8sJ81mXu]\n",
      "pp: [salem cosmetic dentistry restorative dentist dental implants service announced yahoo urlurl ]\n",
      "\n",
      "tw: [What questions should you be asking when investigating potential dental implant providers?\n",
      "\n",
      "Dr Misagh Habibi from our Perth clinic has a rundown of the best questions to ask, to help you make an informed decision about your dental treatment. https://t.co/OVDK1AsNcn]\n",
      "pp: [what questions should you be asking when investigating potential dental implant providers questionmark dr misagh habibi from our perth clinic has a rundown of the best questions to ask to help you make an informed decision about your dental treatment urlurl ]\n",
      "\n",
      "tw: [Global Dental Implants Market to surpass US$ 6269.8 Million by 2024 https://t.co/yDmuD7FwuV https://t.co/i47VWNch8Q]\n",
      "pp: [global dental implants market to surpass us moneymark numbers numbers million by numbers urlurl ]\n",
      "\n",
      "tw: [RT @feel_vibes: Dental Implants Fort Collins - Tim Owens DDS https://t.co/yuH2YYr6tt]\n",
      "pp: [rt mentionname dental implants fort collins tim owens dds urlurl ]\n",
      "\n",
      "tw: [RT @FxhkjsiBertha: Dental Implants Center of Beverly Hills https://t.co/he2ZURoJUN]\n",
      "pp: [rt mentionname dental implants center of beverly hills urlurl ]\n",
      "\n",
      "tw: [Global Dental Implant Sales Market Segmentation by Product Types and Application with Forecast to 2025 https://t.co/4Ix5f2GMWY]\n",
      "pp: [global dental implant sales market segmentation by product types and application with forecast to numbers urlurl ]\n",
      "\n",
      "tw: [RT @Alex9Dn: Dental Implants https://t.co/UfUTpNluh4]\n",
      "pp: [rt mentionname dental implants urlurl ]\n",
      "\n",
      "tw: [Dental Implant Treatment in Khar\n",
      "Get fixed teeth t..For more info visit...https://t.co/6igGjwyCsO https://t.co/YrfMGT7ryf]\n",
      "pp: [dental implant treatment in khar get fixed teeth t for more info visit urlurl ]\n",
      "\n",
      "tw: [Dentist in Clearwater FL, FeatherSound Smiles provide Restorative, Cosmetic and Dental Implant Dentistry in and aro https://t.co/1RNqAZIsU8 https://t.co/17TADtEwIK]\n",
      "pp: [dentist in clearwater fl feathersound smiles provide restorative cosmetic and dental implant dentistry in and aro urlurl ]\n",
      "\n",
      "tw: [@peterhartlaub Cropped the dental implants out for you. Distracting! I overpaid. https://t.co/OvFddxkjDR]\n",
      "pp: [ mentionname cropped the dental implants out for you distracting i overpaid urlurl ]\n",
      "\n",
      "tw: [Did you know that dental implants are an excellent way to replace missing teeth? Here we explain more &gt;&gt; https://t.co/rORS59uLAa]\n",
      "pp: [did you know that dental implants are an excellent way to replace missing teeth questionmark here we explain more urlurl ]\n",
      "\n",
      "tw: [RT @GainesvilleDG: If you have an existing dental implant that needs restoration or repair, we can help. While implants are meant to... htt‚Ä¶]\n",
      "pp: [rt mentionname if you have an existing dental implant that needs restoration or repair we can help while implants are meant to htt ]\n",
      "\n",
      "tw: [Discover a new lease of life with All-on-4 dental implants https://t.co/q04NOJDlH3 https://t.co/zVEjSa7k2L]\n",
      "pp: [discover a new lease of life with all on numbers dental implants urlurl ]\n",
      "\n",
      "tw: [Progressive Dental to Speak on Dental Implant Marketing at California... - https://t.co/QAOTVSsIdx]\n",
      "pp: [progressive dental to speak on dental implant marketing at california urlurl ]\n",
      "\n",
      "tw: [RT @progdinche90: How much are dental implants #dental #coverage https://t.co/v7prxtsPta #Boston]\n",
      "pp: [rt mentionname how much are dental implants dental coverage urlurl ]\n",
      "\n",
      "tw: [RT @minonuz: The best dental implant cent in Los Angeles. https://t.co/PTCMUaidbz\n",
      "\n",
      "#implantcenterlosangeles #losangelesimplantcenter]\n",
      "pp: [rt mentionname the best dental implant cent in los angeles urlurl implantcenterlosangeles losangelesimplantcenter]\n",
      "\n",
      "tw: [Business name:\n",
      "Affordable Dental Implants\n",
      "\n",
      "Address:\n",
      "Serving\n",
      "Delmar, NY 12054\n",
      "\n",
      "Phone:\n",
      "(518) 401-0900\n",
      "\n",
      "Website:\n",
      "https://t.co/z9CBy7WbmM]\n",
      "pp: [business name affordable dental implants address serving delmar ny numbers phone numbers numbers numbers website urlurl ]\n",
      "\n",
      "tw: [Panoramic radiograph of dental implants, circa 1978 via /r/WTF https://t.co/x6D1qWhSwr https://t.co/ah8ryv9bwz]\n",
      "pp: [panoramic radiograph of dental implants circa numbers via r wtf urlurl ]\n",
      "\n",
      "tw: [RT @xGamecrawlx: Operate Now Dental Surgery Game - Dental Implant - OPERATE ON YOUNG BILLY Operate Now Den https://t.co/krwGm2z3pe https://‚Ä¶]\n",
      "pp: [rt mentionname operate now dental surgery game dental implant operate on young billy operate now den urlurl ]\n",
      "\n",
      "tw: [Dental Implants Procedure Advantages Disadvantages And Risks\n",
      "\n",
      "https://t.co/ExA6X9DB9E\n",
      "\n",
      "#health #dentalcare #healthyfood #healthylife #smile #dentist #doctors #HealthyLiving #healthylifestyle #dentistry #healthcare #dentalhealth]\n",
      "pp: [dental implants procedure advantages disadvantages and risks urlurl health dentalcare healthyfood healthylife smile dentist doctors healthyliving healthylifestyle dentistry healthcare dentalhealth]\n",
      "\n",
      "tw: [RT https://t.co/10pau9RSOP Did you know we take referrals for Dental Implant surgery as well as Periodontics? https://t.co/DWhzeMv2QM #Dentist #CosmeticDentist #DentalReferral #Peridontics #D‚Ä¶ https://t.co/tGq1U4m7SU]\n",
      "pp: [rt urlurl ]\n",
      "\n",
      "tw: [One Dental Implant, 2 Crowns, &amp; a Great Smile https://t.co/C5NMeVlKyB #charlottecosmeticdentist #dentalimplants #dentalcrowns]\n",
      "pp: [one dental implant numbers crowns a great smile urlurl ]\n",
      "\n",
      "tw: [RT @swapanseth: So dentists and hospitals have a 500% mark up on dental implants. The MRP is Rs 8,000. You and I are charged Rs 40,000]\n",
      "pp: [rt mentionname so dentists and hospitals have a numbers mark up on dental implants the mrp is rs numbers numbers you and i are charged rs numbers numbers ]\n",
      "\n",
      "tw: [According to the American Academy of Implant Dentistry, dental implants possess a very high success rate of 98... https://t.co/yfnoppvrs3]\n",
      "pp: [according to the american academy of implant dentistry dental implants possess a very high success rate of numbers urlurl ]\n",
      "\n",
      "tw: [RT @Melody1_Spencer: 7 Advantages of a No-Metal Dental Implant https://t.co/tFfycs1KVi]\n",
      "pp: [rt mentionname numbers advantages of a no metal dental implant urlurl ]\n",
      "\n",
      "tw: [RT @KianorShah: Attention To Dentists Who Place Implants:\n",
      "\n",
      "If you want to generate more Dental Implant patients, then I would highly‚Ä¶https:‚Ä¶]\n",
      "pp: [rt mentionname attention to dentists who place implants if you want to generate more dental implant patients then i would highly https ]\n",
      "\n",
      "tw: [I got a letter from my Medicaid provider saying you can now get dental implants covered and, seriously, that's huge.]\n",
      "pp: [i got a letter from my medicaid provider saying you can now get dental implants covered and seriously that s huge ]\n",
      "\n",
      "tw: [Dental Implants - Do You Need Them? https://t.co/hmy0SQjz56 #business, #entrepreneur https://t.co/KglzcXrovX]\n",
      "pp: [dental implants do you need them questionmark urlurl ]\n",
      "\n",
      "tw: [For those who may never have had a dental implant procedure, the idea of the process can be a little scary. There are some things you should know prior to the procedure that will be able to make the process a little less intimidating.\n",
      "\n",
      " #dentalcare\n",
      "https://t.co/WYdpxv80n0 https://t.co/vnpzggIysq]\n",
      "pp: [for those who may never have had a dental implant procedure the idea of the process can be a little scary there are some things you should know prior to the procedure that will be able to make the process a little less intimidating dentalcare urlurl ]\n",
      "\n",
      "tw: [\"Considering Dental Implants? Here Are 5 Things You Should Know\" via @aaid_lifesmiles https://t.co/MFj3Okeaoe https://t.co/llxcFT3zSX]\n",
      "pp: [ considering dental implants questionmark here are numbers things you should know via mentionname urlurl ]\n",
      "\n",
      "tw: [Clear Choice Dental Implants I have a extremely situation my front tooth broke off this am. Do u need a pic on twitter.  Can u respond]\n",
      "pp: [clear choice dental implants i have a extremely situation my front tooth broke off this am do u need a pic on twitter can u respond]\n",
      "\n",
      "tw: [How to look after your dental implants to ensure a life-long happy smile https://t.co/80Irp2uJ8h https://t.co/H0RwW4ORFG]\n",
      "pp: [how to look after your dental implants to ensure a life long happy smile urlurl ]\n",
      "\n",
      "tw: [Don't hide your smile: we are experts on dental implants and we offer low-costs up to 50% less https://t.co/gUyGL2lTXm]\n",
      "pp: [don t hide your smile we are experts on dental implants and we offer low costs up to numbers less urlurl ]\n",
      "\n",
      "tw: [What is mechanism of   dental implants ? https://t.co/nFmO1nJSiN]\n",
      "pp: [what is mechanism of dental implants questionmark urlurl ]\n",
      "\n",
      "tw: [RT @KianorShah: Dental implant care basics: Single-tooth replacement https://t.co/Po0PRTGCcZ]\n",
      "pp: [rt mentionname dental implant care basics single tooth replacement urlurl ]\n",
      "\n",
      "tw: [Dental Implants Cost: 5 Factors that Affecting Implant Cost\n",
      "#dental #implants #cost\n",
      "https://t.co/HMUx873aa2]\n",
      "pp: [dental implants cost numbers factors that affecting implant cost dental implants cost urlurl ]\n",
      "\n",
      "tw: [Dental implants are an excellent way to get a perfect smile without the hassle of dentures.\n",
      "https://t.co/y3qXdZ049A https://t.co/fFxWfHNdL7]\n",
      "pp: [dental implants are an excellent way to get a perfect smile without the hassle of dentures urlurl ]\n",
      "\n",
      "tw: [@YahItsKhyz Don't, it'll crack after 2 years and you'll have to get a dental implant after. (Currently suffering from it)]\n",
      "pp: [ mentionname don t it ll crack after numbers years and you ll have to get a dental implant after currently suffering from it ]\n",
      "\n",
      "tw: [#how to do business in cost full mouth dental implants]\n",
      "pp: [ how to do business in cost full mouth dental implants]\n",
      "\n",
      "tw: [RT @Spera_O: How dental implants are done ! üòÅ\n",
      "\n",
      "#ÿ™ÿ¨ŸÖÿπ_ÿ£ÿ∑ÿ®ÿßÿ°_ÿßŸÑÿπÿßŸÑŸÖ \n",
      "#ÿ∑ÿ®_ÿßŸÑÿßÿ≥ŸÜÿßŸÜ https://t.co/IPjhLaaPPs]\n",
      "pp: [rt mentionname how dental implants are done happyFace ÿ™ÿ¨ŸÖÿπ ÿ£ÿ∑ÿ®ÿßÿ° ÿßŸÑÿπÿßŸÑŸÖ ÿ∑ÿ® ÿßŸÑÿßÿ≥ŸÜÿßŸÜ urlurl ]\n",
      "\n",
      "tw: [Considering dental implants? Learn more here to see if they might be a good option for you. https://t.co/vPyYqNignl]\n",
      "pp: [considering dental implants questionmark learn more here to see if they might be a good option for you urlurl ]\n",
      "\n",
      "tw: [Thumbay Dental Hospital has established an Advanced Dental Implant Centre to cater to patients of the GCC region https://t.co/i34W621oqf]\n",
      "pp: [thumbay dental hospital has established an advanced dental implant centre to cater to patients of the gcc region urlurl ]\n",
      "\n",
      "tw: [RT @bac5665: @JedKolko @mattyglesias Spaghetti carbonara makes an excellent substitute for all things.  Except dental implants.‚Ä¶ ]\n",
      "pp: [rt mentionname mentionname mentionname spaghetti carbonara makes an excellent substitute for all things except dental implants ]\n",
      "\n",
      "tw: [#latptop i need dental implants]\n",
      "pp: [ latptop i need dental implants]\n",
      "\n",
      "tw: [Bacterial infection of a #dental implant is a dreaded complication, as it carries with it a high risk of #jawbone degeneration.  https://t.co/UQYYrcQr4w https://t.co/SK41tO50er]\n",
      "pp: [bacterial infection of a dental implant is a dreaded complication as it carries with it a high risk of jawbone degeneration urlurl ]\n",
      "\n",
      "tw: [Looking for Dental implants  in Costa Rica? Let Dr. Luis G. Obando help you with your needs. https://t.co/DQQsSAAkAo https://t.co/ZMNqVDlEm3]\n",
      "pp: [looking for dental implants in costa rica questionmark let dr luis g obando help you with your needs urlurl ]\n",
      "\n",
      "tw: [RT @jfgierat: @ClearChoice Advertising on Sinclair? If I'm interested in dental implants, I don't want them with a side of state-run propag‚Ä¶]\n",
      "pp: [rt mentionname mentionname advertising on sinclair questionmark if i m interested in dental implants i don t want them with a side of state run propag ]\n",
      "\n",
      "tw: [US SALE! dentist Dental Implant Disease Teeth Model Restoration  https://t.co/q0NIjXBXnz https://t.co/qkCeTPbAZW]\n",
      "pp: [us sale dentist dental implant disease teeth model restoration urlurl ]\n",
      "\n",
      "tw: [If your smile is letting you down then you might want to seriously consider dental implants! Let us help you restore your smile! https://t.co/6skKLHgSFw]\n",
      "pp: [if your smile is letting you down then you might want to seriously consider dental implants let us help you restore your smile urlurl ]\n",
      "\n",
      "tw: [Comfort food Just awesome #enjoysmilelive @ Soni Dental Implants https://t.co/KxbLvleGEy]\n",
      "pp: [comfort food just awesome enjoysmilelive soni dental implants urlurl ]\n",
      "\n",
      "tw: [Dental Design of Arcadia can help your smile with dental implants https://t.co/9ZW9byjul3]\n",
      "pp: [dental design of arcadia can help your smile with dental implants urlurl ]\n",
      "\n",
      "tw: [Common Misconceptions About Dental Implants https://t.co/WqugsuU0Mq https://t.co/A1fokUcnSb]\n",
      "pp: [common misconceptions about dental implants urlurl ]\n",
      "\n",
      "tw: [RT @MedicaITerms: This is how a dental implant is installed https://t.co/5KQhWvBB2v]\n",
      "pp: [rt mentionname this is how a dental implant is installed urlurl ]\n",
      "\n",
      "tw: [50 x Dental Implant Implants FORTIS¬Æ Semi Conical Body Internal Hex System CE https://t.co/bPfEY38Q1u https://t.co/xaskNkPw6o]\n",
      "pp: [ numbers x dental implant implants fortis semi conical body internal hex system ce urlurl ]\n",
      "\n",
      "tw: [Dental Implants - What are dental implants? Dental implants are artificial titanium roots, which are... https://t.co/wgmshMKv6E]\n",
      "pp: [dental implants what are dental implants questionmark dental implants are artificial titanium roots which are urlurl ]\n",
      "\n",
      "tw: [The Advantages Of dental implants in Stockton¬†California https://t.co/N9vQuM8xaG]\n",
      "pp: [the advantages of dental implants in stockton california urlurl ]\n",
      "\n",
      "tw: [CLICK HERE ‚ñ∂Ô∏è https://t.co/ITI9Y4WI21 ‚óÄÔ∏è #Redhead #babe #movies Discounted free dental implants for seniors.]\n",
      "pp: [click here urlurl ]\n",
      "\n",
      "tw: [Dental_Concepts: Read our blog to find out how dental implants changed Graham's life: https://t.co/wplTOs7Y86 https://t.co/Zdz7ByMhAn #De‚Ä¶]\n",
      "pp: [dental concepts read our blog to find out how dental implants changed graham s life urlurl ]\n",
      "\n",
      "tw: [Do you have a missing tooth? Is it keeping you from smiling? Consider dental implants! They can bring you back to your old self with a confident smile! Interested? Give our office a call to schedule a consultation! https://t.co/DtD9bntfWl]\n",
      "pp: [do you have a missing tooth questionmark is it keeping you from smiling questionmark consider dental implants they can bring you back to your old self with a confident smile interested questionmark give our office a call to schedule a consultation urlurl ]\n",
      "\n",
      "tw: [With new advanced modern techniques we are able to produce custom, well fitting Dental Implants. Contact us to book your Dental Implant consultation today: 01494 854 053]\n",
      "pp: [with new advanced modern techniques we are able to produce custom well fitting dental implants contact us to book your dental implant consultation today numbers numbers numbers ]\n",
      "\n",
      "tw: [Dental Implants at Lindley #Dental https://t.co/bBVdlAe756 #Huddersfield #Smiles https://t.co/zkvnicnomj]\n",
      "pp: [dental implants at lindley dental urlurl ]\n",
      "\n",
      "tw: [#dental implants clinic bloom energy fuel cell]\n",
      "pp: [ dental implants clinic bloom energy fuel cell]\n",
      "\n",
      "tw: [The major difference between traditional dental implants and mini implants is the fact that mini implants are about half the diameter of traditional ones. https://t.co/pd3u1jHLSV\n",
      "\n",
      "#AdvancedDental #HarveyReiter #LasCrucesNM #LasCruces #NewMexico #DentalImplants #LasCrucesDentist https://t.co/4xwD53gI8K]\n",
      "pp: [the major difference between traditional dental implants and mini implants is the fact that mini implants are about half the diameter of traditional ones urlurl advanceddental harveyreiter lascrucesnm lascruces newmexico dentalimplants lascrucesdentist urlurl ]\n",
      "\n",
      "tw: [Our dental implants are a great way to restore the missing gaps from lost teeth: https://t.co/BdT0LydPg7]\n",
      "pp: [our dental implants are a great way to restore the missing gaps from lost teeth urlurl ]\n",
      "\n",
      "tw: [RT @1800VENEERS: Check this out @ Dental Implantology - Standard Surgery Animation: SICmax implant insertion: http://t.co/tF55rB3nuV via @Y‚Ä¶]\n",
      "pp: [rt mentionname check this out dental implantology standard surgery animation sicmax implant insertion urlurl ]\n",
      "\n",
      "tw: [Dental Implant Brushless Motor 20:1 Reduction Handpiece Surgical Drill SYSTEM https://t.co/ba84bqOiUb]\n",
      "pp: [dental implant brushless motor numbers numbers reduction handpiece surgical drill system urlurl ]\n",
      "\n",
      "tw: [Dental implants require the same care as your real teeth but generally, they are much easier to clean. To learn more about dental implants and how they can benefit your oral health contact Implant Solutions Today.\n",
      "https://t.co/sSu5cQDpBZ | 386-837-1236 https://t.co/90IRFSySSw]\n",
      "pp: [dental implants require the same care as your real teeth but generally they are much easier to clean to learn more about dental implants and how they can benefit your oral health contact implant solutions today urlurl ]\n",
      "\n",
      "tw: [Dental implants are also known as implantology and @DeNeckerGroup offers this surgical speciality https://t.co/bUGh8GisF8 https://t.co/fITqt544no]\n",
      "pp: [dental implants are also known as implantology and mentionname offers this surgical speciality urlurl ]\n",
      "\n",
      "tw: [Dental implants are ever-increasing in popularity as a way to replace your missing teeth.  \n",
      "\n",
      "They are the only permanent way to replace lost teeth, and are the only tooth replacement option that helps to prevent... https://t.co/aN8Wmr1oVv]\n",
      "pp: [dental implants are ever increasing in popularity as a way to replace your missing teeth they are the only permanent way to replace lost teeth and are the only tooth replacement option that helps to prevent urlurl ]\n",
      "\n",
      "tw: [God needs dental implants and called Carl Misch. The Master. The Man who set the rules. The Master who championed predictable implantology..]\n",
      "pp: [god needs dental implants and called carl misch the master the man who set the rules the master who championed predictable implantology ]\n",
      "\n",
      "tw: [Learn About The Advantages Of Dental Implants In Hervey Bay | Smile Of An Angel https://t.co/eBzGYQRdNe]\n",
      "pp: [learn about the advantages of dental implants in hervey bay smile of an angel urlurl ]\n",
      "\n",
      "tw: [Sinus lifts: making dental implants possible for more people https://t.co/5eByawHYB9 https://t.co/Cg6YzySxxm]\n",
      "pp: [sinus lifts making dental implants possible for more people urlurl ]\n",
      "\n",
      "tw: [Dental Implants In Annapolis Will Improve Your Appearance And Self-Esteem - Dental Health Answer https://t.co/pqQY7zKE3G]\n",
      "pp: [dental implants in annapolis will improve your appearance and self esteem dental health answer urlurl ]\n",
      "\n",
      "tw: [BioHorizons' dental implant products are designed to maintain facial aesthetics while offering high implant... https://t.co/LzW7THDRUs]\n",
      "pp: [biohorizons dental implant products are designed to maintain facial aesthetics while offering high implant urlurl ]\n",
      "\n",
      "tw: [What are the most popular destinations for dental implants? #europe #dental #holidays #tourism #teeth #new #smile #travel #healthy https://t.co/mxyMxK9b6l]\n",
      "pp: [what are the most popular destinations for dental implants questionmark europe dental holidays tourism teeth new smile travel healthy urlurl ]\n",
      "\n",
      "tw: [RT @williswood92: Need for Hiring the Best Dentist near You ‚Äì Pisa Dental Implants https://t.co/dgL7RphXWK]\n",
      "pp: [rt mentionname need for hiring the best dentist near you pisa dental implants urlurl ]\n",
      "\n",
      "tw: [#dental implant cheap u s department of veteran affairs]\n",
      "pp: [ dental implant cheap u s department of veteran affairs]\n",
      "\n",
      "tw: [Here is our latest blog post \"All you wanted to know about Dental Implants\" https://t.co/KkmMfRosFe https://t.co/y3EzXxajGE]\n",
      "pp: [here is our latest blog post all you wanted to know about dental implants urlurl ]\n",
      "\n",
      "tw: [Dental Implant Abutment Systems to Garner US$ 1,900 Mn by the end of 2027- Persistence Market Research https://t.co/od7MTYh3zz]\n",
      "pp: [dental implant abutment systems to garner us moneymark numbers numbers mn by the end of numbers persistence market research urlurl ]\n",
      "\n",
      "tw: [Thought about dental implants but do not know where to start? Contact us today on 01752 343140 or visit our website for more information. https://t.co/FzpbNF6TeC]\n",
      "pp: [thought about dental implants but do not know where to start questionmark contact us today on numbers numbers or visit our website for more information urlurl ]\n",
      "\n",
      "tw: [Global Dental Implants and Prosthetics Market - Anticipated to Witness a Growth Rate of‚Ä¶ https://t.co/SmzwTAuOIJ https://t.co/vJXB5Y3lPj]\n",
      "pp: [global dental implants and prosthetics market anticipated to witness a growth rate of urlurl ]\n",
      "\n",
      "tw: [RT @Post_planner: https://t.co/cPAjmPFXD6  What Are Dental Implants? - Cosmetic Dentist Los Angeles]\n",
      "pp: [rt mentionname urlurl ]\n",
      "\n",
      "tw: [Dental Implants Abroad in Polladras #Dental #Implant #Abroad #Polladras https://t.co/1p8IlcZ1Ka]\n",
      "pp: [dental implants abroad in polladras dental implant abroad polladras urlurl ]\n",
      "\n",
      "tw: [4 Best Tips to Prepare for Dental Implant Surgery https://t.co/VFowpG9xv6 https://t.co/yXrKL5fgCH]\n",
      "pp: [ numbers best tips to prepare for dental implant surgery urlurl ]\n",
      "\n",
      "tw: [Dental implants provide the best, most-consistent results in replacing compromised teeth. https://t.co/nbnEQtfszz]\n",
      "pp: [dental implants provide the best most consistent results in replacing compromised teeth urlurl ]\n",
      "\n",
      "tw: [RT @goodsearch1: Do you have any idea what dental implants cost? It may surprise you. See dental implant sponsored listings.]\n",
      "pp: [rt mentionname do you have any idea what dental implants cost questionmark it may surprise you see dental implant sponsored listings ]\n",
      "\n",
      "tw: [How to Ensure Your Dental Implants Last for Life https://t.co/YK8QO9z0Ef https://t.co/4ua78j7YXQ]\n",
      "pp: [how to ensure your dental implants last for life urlurl ]\n",
      "\n",
      "tw: [RT @angelamiller945: Cosmetic Dentist Houston TX | Dental Implants in Houston\n",
      "https://t.co/VQsSZzUtBM]\n",
      "pp: [rt mentionname cosmetic dentist houston tx dental implants in houston urlurl ]\n",
      "\n",
      "tw: [RT @LeazySunny: https://t.co/eZEK0kxnxm\n",
      "\n",
      "Bishara Dental Implants in Houston Texas are an alternative solution]\n",
      "pp: [rt mentionname urlurl bishara dental implants in houston texas are an alternative solution]\n",
      "\n",
      "tw: [Billings Oral Surgery and Dental Implant Center Sheridan https://t.co/nlVsXKA1tW https://t.co/FptSCOb3tW]\n",
      "pp: [billings oral surgery and dental implant center sheridan urlurl ]\n",
      "\n",
      "tw: [Look Implant Surgery - Is dental implant surgery painful? https://t.co/B501Qf4Sik]\n",
      "pp: [look implant surgery is dental implant surgery painful questionmark urlurl ]\n",
      "\n",
      "tw: [Are dental implants painful? Learn more here: https://t.co/umhpPQnDAF https://t.co/c2L0D35KJS]\n",
      "pp: [are dental implants painful questionmark learn more here urlurl ]\n",
      "\n",
      "tw: [What is a Dental Implant? https://t.co/UwB3DaXRCE https://t.co/d9brZAH8m1]\n",
      "pp: [what is a dental implant questionmark urlurl ]\n",
      "\n",
      "tw: [Dr. Fiss provides cosmetic dentistry procedures including porcelain veneers, dental implants, and tooth whitening. https://t.co/5YPC9h6PB3]\n",
      "pp: [dr fiss provides cosmetic dentistry procedures including porcelain veneers dental implants and tooth whitening urlurl ]\n",
      "\n",
      "tw: [RT @hrmironova50706: Affordable cost Dental Implants #discount #dental #implants https://t.co/CYYfOgxpnt]\n",
      "pp: [rt mentionname affordable cost dental implants discount dental implants urlurl ]\n",
      "\n",
      "tw: [What Might Lead To Dental Implants Failure According To Family Cosmetic Dentistry https://t.co/h8mfs1IIEN]\n",
      "pp: [what might lead to dental implants failure according to family cosmetic dentistry urlurl ]\n",
      "\n",
      "tw: [RT @chstonCowg: Best Dental Implants in New Jersey  https://t.co/sgMv3HE07u #AllendaleDentalImplants  #FortLeeDentalImplants #GarfieldDenta‚Ä¶]\n",
      "pp: [rt mentionname best dental implants in new jersey urlurl ]\n",
      "\n",
      "tw: [RT @DestroyingClip: Hydrualic Press vs Ceramic Dental Implants üò¨ https://t.co/9Bvp4wHSPP]\n",
      "pp: [rt mentionname hydrualic press vs ceramic dental implants urlurl ]\n",
      "\n",
      "tw: [#Osseointegration : Osseointegration of Dental Implants to Jawbone in‚Ä¶ https://t.co/1hRAjuJJ76 #Health #OralCare]\n",
      "pp: [ osseointegration osseointegration of dental implants to jawbone in urlurl ]\n",
      "\n",
      "tw: [Curious about your dental implant treatment? Find out what happens during the procedure right here.   #Implants #Dentures #DentalCare\n",
      "https://t.co/Y2G8xg1z10]\n",
      "pp: [curious about your dental implant treatment questionmark find out what happens during the procedure right here implants dentures dentalcare urlurl ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetsPP = pre_proc(tweets)\n",
    "for t, p in zip(tweets[:1000], tweetsPP[:1000]):\n",
    "    print(\"tw: [%s]\" % t)\n",
    "    print(\"pp: [%s]\" % p)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr√© Proc 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "stemmer.stem(\"dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "stop = set(stopwords.words('english')).union({'dental', 'implant', 'implants', 'rt'})\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "#doc_complete = [ t[0] for t in tweets ]\n",
    "#print(stop)\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop and len(i) > 3])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(stemmer.stem(word) for word in punc_free.split())\n",
    "    #normalized = [ w for w in punc_free if len(w) > 3 ]\n",
    "    #normalized = punc_free\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.8\n",
      "('urlurl', 847)\n",
      "('number', 366)\n",
      "('mentionnam', 259)\n",
      "('questionmark', 165)\n",
      "('teeth', 136)\n",
      "('smile', 92)\n",
      "('dentist', 75)\n",
      "('miss', 73)\n",
      "('tooth', 60)\n",
      "('cost', 54)\n",
      "('dentistri', 51)\n",
      "('replac', 48)\n",
      "('restor', 46)\n",
      "('market', 40)\n",
      "('patient', 37)\n",
      "('know', 35)\n",
      "('best', 35)\n",
      "('need', 35)\n",
      "('help', 34)\n",
      "('dentur', 34)\n",
      "('surgeri', 33)\n",
      "('look', 31)\n",
      "('call', 31)\n",
      "('learn', 30)\n",
      "('treatment', 30)\n",
      "('dentalimpl', 30)\n",
      "('blog', 29)\n",
      "('cosmet', 28)\n",
      "('offer', 28)\n",
      "('find', 28)\n"
     ]
    }
   ],
   "source": [
    "doc_clean = [clean(doc).split() for doc in tweetsPP] \n",
    "#print(doc_clean[:20])\n",
    "\n",
    "\n",
    "print(len(doc_clean)*0.4)\n",
    "ws = []\n",
    "for f in doc_clean:\n",
    "    for w in f:\n",
    "        ws.append(w)\n",
    "cws = Counter(ws)\n",
    "rWords = [ x[0] for x in cws.most_common() ]\n",
    "for x in cws.most_common(30):\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsClean = []\n",
    "for t in doc_clean:\n",
    "    tweetsClean.append(\" \".join([i for i in t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tw: [Dental Implant machine w motor w reduction 16:1 push contra angle handpiece SET https://t.co/afHj5VEuga https://t.co/ZJOWzvs3dl]\n",
      "pp: [dental implant machine w motor w reduction numbers numbers push contra angle handpiece set urlurl ]\n",
      "cl: [['machin', 'motor', 'reduct', 'number', 'number', 'push', 'contra', 'angl', 'handpiec', 'urlurl']]\n",
      "tc: [machin motor reduct number number push contra angl handpiec urlurl]\n",
      "\n",
      "tw: [RT https://t.co/rvhOgYkKw2 Dental Implants are More Efficient than Traditional Dentures and Bridges. Explore details on https://t.co/PXAAhAgdDc #teeth #hornsbydentist #dentisthornsby #Oral‚Ä¶ https://t.co/4GAfBK8nTi]\n",
      "pp: [rt urlurl ]\n",
      "cl: [['urlurl']]\n",
      "tc: [urlurl]\n",
      "\n",
      "tw: [Losing #teeth creates the potential for your oral #health to decline quickly‚Ä¶\n",
      "\n",
      "Consider a #dental implants: https://t.co/FCtOAjdmJP\n",
      "\n",
      "#tcmi https://t.co/vUapu9seeq]\n",
      "pp: [losing teeth creates the potential for your oral health to decline quickly consider a dental implants urlurl tcmi urlurl ]\n",
      "cl: [['lose', 'teeth', 'creat', 'potenti', 'oral', 'health', 'declin', 'quick', 'consid', 'urlurl', 'tcmi', 'urlurl']]\n",
      "tc: [lose teeth creat potenti oral health declin quick consid urlurl tcmi urlurl]\n",
      "\n",
      "tw: [We're on Pinterest!! \n",
      "\n",
      "Did you ever wonder what a dental implant looks like? ...or how Inv... https://t.co/LBqQ6fKLYA https://t.co/9Y8CQpQ538]\n",
      "pp: [we re on pinterest did you ever wonder what a dental implant looks like questionmark or how inv urlurl ]\n",
      "cl: [['pinterest', 'ever', 'wonder', 'look', 'like', 'questionmark', 'urlurl']]\n",
      "tc: [pinterest ever wonder look like questionmark urlurl]\n",
      "\n",
      "tw: [Preferred Dental Technologies is the First Dental Implant Company to 3D Print Solid Custom ... https://t.co/Um9otgSCSS]\n",
      "pp: [preferred dental technologies is the first dental implant company to numbers d print solid custom urlurl ]\n",
      "cl: [['prefer', 'technolog', 'first', 'compani', 'number', 'print', 'solid', 'custom', 'urlurl']]\n",
      "tc: [prefer technolog first compani number print solid custom urlurl]\n",
      "\n",
      "tw: [RT @lintenaju7: Know More about Dental Implants and its Positive Features ‚Äì Medaka Doctor https://t.co/hQhYkBsIQs]\n",
      "pp: [rt mentionname know more about dental implants and its positive features medaka doctor urlurl ]\n",
      "cl: [['mentionnam', 'know', 'posit', 'featur', 'medaka', 'doctor', 'urlurl']]\n",
      "tc: [mentionnam know posit featur medaka doctor urlurl]\n",
      "\n",
      "tw: [RT @olivesanders15: Orange County Dentist  provides patients with quality general and cosmetic dentistry such as dental implants, Invisalig‚Ä¶]\n",
      "pp: [rt mentionname orange county dentist provides patients with quality general and cosmetic dentistry such as dental implants invisalig ]\n",
      "cl: [['mentionnam', 'orang', 'counti', 'dentist', 'provid', 'patient', 'qualiti', 'general', 'cosmet', 'dentistri', 'invisalig']]\n",
      "tc: [mentionnam orang counti dentist provid patient qualiti general cosmet dentistri invisalig]\n",
      "\n",
      "tw: [How to Enhance Your Best Smile With Dental Implants,see this post on Cnipo https://t.co/LmcQMV1xLr]\n",
      "pp: [how to enhance your best smile with dental implants see this post on cnipo urlurl ]\n",
      "cl: [['enhanc', 'best', 'smile', 'post', 'cnipo', 'urlurl']]\n",
      "tc: [enhanc best smile post cnipo urlurl]\n",
      "\n",
      "tw: [Tooth loss can happen at any age! Don't let your smile hold you back, ask us about dental implants! https://t.co/1VuY6HTUTt]\n",
      "pp: [tooth loss can happen at any age don t let your smile hold you back ask us about dental implants urlurl ]\n",
      "cl: [['tooth', 'loss', 'happen', 'smile', 'hold', 'back', 'urlurl']]\n",
      "tc: [tooth loss happen smile hold back urlurl]\n",
      "\n",
      "tw: [Dental Implant Market Is Projected To Witness Significant Growth By 2021 | Radiant Insights,Inc https://t.co/LdrY0VCVqB https://t.co/eyiMLaDGtR]\n",
      "pp: [dental implant market is projected to witness significant growth by numbers radiant insights inc urlurl ]\n",
      "cl: [['market', 'project', 'wit', 'signific', 'growth', 'number', 'radiant', 'insight', 'urlurl']]\n",
      "tc: [market project wit signific growth number radiant insight urlurl]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetsPP = pre_proc(tweets)\n",
    "for t, p, dc, tc in zip(tweets[:10], tweetsPP[:10], doc_clean[:10], tweetsClean[:10]):\n",
    "    print(\"tw: [%s]\" % t)\n",
    "    print(\"pp: [%s]\" % p)\n",
    "    print(\"cl: [%s]\" % dc)\n",
    "    print(\"tc: [%s]\" % tc)\n",
    "    print()\n",
    "#print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"Accuracy: %s\" % classifier.score(X_test, y_test))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MNB [alpha: 0.0 max_df:0.5 min_df: 1]\n",
      "1 MNB [alpha: 0.0 max_df:0.5 min_df: 2]\n",
      "2 MNB [alpha: 0.0 max_df:0.5 min_df: 3]\n",
      "3 MNB [alpha: 0.0 max_df:0.5 min_df: 4]\n",
      "4 MNB [alpha: 0.0 max_df:0.5 min_df: 5]\n",
      "5 MNB [alpha: 0.0 max_df:0.5 min_df: 6]\n",
      "6 MNB [alpha: 0.0 max_df:0.5 min_df: 7]\n",
      "7 MNB [alpha: 0.0 max_df:0.5 min_df: 8]\n",
      "8 MNB [alpha: 0.0 max_df:0.5 min_df: 9]\n",
      "9 MNB [alpha: 0.0 max_df:0.5 min_df: 10]\n",
      "660\n",
      "--\n",
      "MNB [alpha: 0.0 max_df:0.5 min_df: 3]\n",
      "--\n",
      "660\n",
      "(0, 'MNB [alpha: 0.0 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.5, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('classifier',\n",
      "                 MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
      "         verbose=False))\n",
      "(659, 'MNB [alpha: 1.0 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=None,\n",
      "                                 min_df=10, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('classifier',\n",
      "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
      "         verbose=False))\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "MNBtrials = []\n",
    "cont = 0\n",
    "contAll = 0\n",
    "allTrials = []\n",
    "\n",
    "for i in range(0, 11, 1):\n",
    "    alpha = i/10\n",
    "    for j in range(5, 11, 1):\n",
    "        max_df = j/10\n",
    "        for k in range(1, 11, 1):\n",
    "            min_df = k\n",
    "            trial = Pipeline([\n",
    "                ('vectorizer', TfidfVectorizer(min_df=min_df, max_df=max_df)),\n",
    "                ('classifier', MultinomialNB(alpha=alpha))\n",
    "                ])\n",
    "            desc = 'MNB [alpha: ' + str(alpha) + ' max_df:' + str(max_df) + ' min_df: ' + str(min_df) + ']'\n",
    "            MNBtrials.append((cont,desc,trial))\n",
    "            cont+=1\n",
    "            allTrials.append((contAll,desc,trial))\n",
    "            contAll+=1\n",
    "\n",
    "for t in MNBtrials[:10]:\n",
    "    print(t[0],t[1])\n",
    "print(len(MNBtrials))\n",
    "print('--')\n",
    "print(MNBtrials[2][1])\n",
    "print('--')\n",
    "print(len(allTrials))\n",
    "print(allTrials[0])\n",
    "print(allTrials[659])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RF [val: 1.0 max_df:0.5 min_df: 1]\n",
      "1 RF [val: 1.0 max_df:0.5 min_df: 2]\n",
      "2 RF [val: 1.0 max_df:0.5 min_df: 3]\n",
      "3 RF [val: 1.0 max_df:0.5 min_df: 4]\n",
      "4 RF [val: 1.0 max_df:0.5 min_df: 5]\n",
      "5 RF [val: 1.0 max_df:0.5 min_df: 6]\n",
      "6 RF [val: 1.0 max_df:0.5 min_df: 7]\n",
      "7 RF [val: 1.0 max_df:0.5 min_df: 8]\n",
      "8 RF [val: 1.0 max_df:0.5 min_df: 9]\n",
      "9 RF [val: 1.0 max_df:0.5 min_df: 10]\n",
      "10 RF [val: 1.0 max_df:0.6 min_df: 1]\n",
      "11 RF [val: 1.0 max_df:0.6 min_df: 2]\n",
      "2400\n",
      "--\n",
      "RF [val: 1.0 max_df:0.5 min_df: 3]\n",
      "--\n",
      "3060\n",
      "(0, 'MNB [alpha: 0.0 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.5, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('classifier',\n",
      "                 MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
      "         verbose=False))\n",
      "(659, 'MNB [alpha: 1.0 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=None,\n",
      "                                 min_df=10, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('classifier',\n",
      "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
      "         verbose=False))\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "RFtrials = []\n",
    "cont = 0\n",
    "\n",
    "for val in range(1, 41):\n",
    "    for j in range(5, 11, 1):\n",
    "        max_df = j/10\n",
    "        for k in range(1, 11, 1):\n",
    "            min_df = k\n",
    "            trial = Pipeline([\n",
    "                ('vectorizer', TfidfVectorizer(min_df=min_df, max_df=max_df)),\n",
    "                ('classifier', RandomForestClassifier(n_estimators=val,random_state=10)),\n",
    "                ])\n",
    "            desc = 'RF [val: ' + str(alpha) + ' max_df:' + str(max_df) + ' min_df: ' + str(min_df) + ']'\n",
    "            RFtrials.append((cont,desc,trial))\n",
    "            cont+=1\n",
    "            allTrials.append((contAll,desc,trial))\n",
    "            contAll+=1            \n",
    "\n",
    "for t in RFtrials[:12]:\n",
    "    print(t[0],t[1])\n",
    "print(len(RFtrials))\n",
    "print('--')\n",
    "print(RFtrials[2][1])\n",
    "\n",
    "\n",
    "#for val in range(1,41):\n",
    "#    trial = Pipeline([\n",
    "#        ('vectorizer', TfidfVectorizer(min_df=5, max_df=0.9)),\n",
    "#        ('classifier', RandomForestClassifier(n_estimators=val)),\n",
    "#    ])\n",
    "#    trials.append(trial)\n",
    "print('--')\n",
    "print(len(allTrials))\n",
    "print(allTrials[0])\n",
    "print(allTrials[659])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 SGD [max_df:0.5 min_df: 1 learning_rate : 0.1]\n",
      "1 SGD [max_df:0.5 min_df: 2 learning_rate : 0.1]\n",
      "2 SGD [max_df:0.5 min_df: 3 learning_rate : 0.1]\n",
      "3 SGD [max_df:0.5 min_df: 4 learning_rate : 0.1]\n",
      "4 SGD [max_df:0.5 min_df: 5 learning_rate : 0.1]\n",
      "5 SGD [max_df:0.5 min_df: 6 learning_rate : 0.1]\n",
      "6 SGD [max_df:0.5 min_df: 7 learning_rate : 0.1]\n",
      "7 SGD [max_df:0.5 min_df: 8 learning_rate : 0.1]\n",
      "8 SGD [max_df:0.5 min_df: 9 learning_rate : 0.1]\n",
      "9 SGD [max_df:0.5 min_df: 10 learning_rate : 0.1]\n",
      "10 SGD [max_df:0.6 min_df: 1 learning_rate : 0.1]\n",
      "11 SGD [max_df:0.6 min_df: 2 learning_rate : 0.1]\n",
      "12 SGD [max_df:0.6 min_df: 3 learning_rate : 0.1]\n",
      "13 SGD [max_df:0.6 min_df: 4 learning_rate : 0.1]\n",
      "14 SGD [max_df:0.6 min_df: 5 learning_rate : 0.1]\n",
      "15 SGD [max_df:0.6 min_df: 6 learning_rate : 0.1]\n",
      "16 SGD [max_df:0.6 min_df: 7 learning_rate : 0.1]\n",
      "17 SGD [max_df:0.6 min_df: 8 learning_rate : 0.1]\n",
      "18 SGD [max_df:0.6 min_df: 9 learning_rate : 0.1]\n",
      "19 SGD [max_df:0.6 min_df: 10 learning_rate : 0.1]\n",
      "20 SGD [max_df:0.7 min_df: 1 learning_rate : 0.1]\n",
      "21 SGD [max_df:0.7 min_df: 2 learning_rate : 0.1]\n",
      "22 SGD [max_df:0.7 min_df: 3 learning_rate : 0.1]\n",
      "23 SGD [max_df:0.7 min_df: 4 learning_rate : 0.1]\n",
      "24 SGD [max_df:0.7 min_df: 5 learning_rate : 0.1]\n",
      "25 SGD [max_df:0.7 min_df: 6 learning_rate : 0.1]\n",
      "26 SGD [max_df:0.7 min_df: 7 learning_rate : 0.1]\n",
      "27 SGD [max_df:0.7 min_df: 8 learning_rate : 0.1]\n",
      "28 SGD [max_df:0.7 min_df: 9 learning_rate : 0.1]\n",
      "29 SGD [max_df:0.7 min_df: 10 learning_rate : 0.1]\n",
      "30 SGD [max_df:0.8 min_df: 1 learning_rate : 0.1]\n",
      "31 SGD [max_df:0.8 min_df: 2 learning_rate : 0.1]\n",
      "32 SGD [max_df:0.8 min_df: 3 learning_rate : 0.1]\n",
      "33 SGD [max_df:0.8 min_df: 4 learning_rate : 0.1]\n",
      "34 SGD [max_df:0.8 min_df: 5 learning_rate : 0.1]\n",
      "35 SGD [max_df:0.8 min_df: 6 learning_rate : 0.1]\n",
      "36 SGD [max_df:0.8 min_df: 7 learning_rate : 0.1]\n",
      "37 SGD [max_df:0.8 min_df: 8 learning_rate : 0.1]\n",
      "38 SGD [max_df:0.8 min_df: 9 learning_rate : 0.1]\n",
      "39 SGD [max_df:0.8 min_df: 10 learning_rate : 0.1]\n",
      "40 SGD [max_df:0.9 min_df: 1 learning_rate : 0.1]\n",
      "41 SGD [max_df:0.9 min_df: 2 learning_rate : 0.1]\n",
      "42 SGD [max_df:0.9 min_df: 3 learning_rate : 0.1]\n",
      "43 SGD [max_df:0.9 min_df: 4 learning_rate : 0.1]\n",
      "44 SGD [max_df:0.9 min_df: 5 learning_rate : 0.1]\n",
      "45 SGD [max_df:0.9 min_df: 6 learning_rate : 0.1]\n",
      "46 SGD [max_df:0.9 min_df: 7 learning_rate : 0.1]\n",
      "47 SGD [max_df:0.9 min_df: 8 learning_rate : 0.1]\n",
      "48 SGD [max_df:0.9 min_df: 9 learning_rate : 0.1]\n",
      "49 SGD [max_df:0.9 min_df: 10 learning_rate : 0.1]\n",
      "50 SGD [max_df:1.0 min_df: 1 learning_rate : 0.1]\n",
      "51 SGD [max_df:1.0 min_df: 2 learning_rate : 0.1]\n",
      "52 SGD [max_df:1.0 min_df: 3 learning_rate : 0.1]\n",
      "53 SGD [max_df:1.0 min_df: 4 learning_rate : 0.1]\n",
      "54 SGD [max_df:1.0 min_df: 5 learning_rate : 0.1]\n",
      "55 SGD [max_df:1.0 min_df: 6 learning_rate : 0.1]\n",
      "56 SGD [max_df:1.0 min_df: 7 learning_rate : 0.1]\n",
      "57 SGD [max_df:1.0 min_df: 8 learning_rate : 0.1]\n",
      "58 SGD [max_df:1.0 min_df: 9 learning_rate : 0.1]\n",
      "59 SGD [max_df:1.0 min_df: 10 learning_rate : 0.1]\n",
      "60 SGD [max_df:0.5 min_df: 1 learning_rate : 0.01]\n",
      "61 SGD [max_df:0.5 min_df: 2 learning_rate : 0.01]\n",
      "62 SGD [max_df:0.5 min_df: 3 learning_rate : 0.01]\n",
      "63 SGD [max_df:0.5 min_df: 4 learning_rate : 0.01]\n",
      "64 SGD [max_df:0.5 min_df: 5 learning_rate : 0.01]\n",
      "65 SGD [max_df:0.5 min_df: 6 learning_rate : 0.01]\n",
      "66 SGD [max_df:0.5 min_df: 7 learning_rate : 0.01]\n",
      "67 SGD [max_df:0.5 min_df: 8 learning_rate : 0.01]\n",
      "68 SGD [max_df:0.5 min_df: 9 learning_rate : 0.01]\n",
      "69 SGD [max_df:0.5 min_df: 10 learning_rate : 0.01]\n",
      "70 SGD [max_df:0.6 min_df: 1 learning_rate : 0.01]\n",
      "71 SGD [max_df:0.6 min_df: 2 learning_rate : 0.01]\n",
      "72 SGD [max_df:0.6 min_df: 3 learning_rate : 0.01]\n",
      "73 SGD [max_df:0.6 min_df: 4 learning_rate : 0.01]\n",
      "74 SGD [max_df:0.6 min_df: 5 learning_rate : 0.01]\n",
      "75 SGD [max_df:0.6 min_df: 6 learning_rate : 0.01]\n",
      "76 SGD [max_df:0.6 min_df: 7 learning_rate : 0.01]\n",
      "77 SGD [max_df:0.6 min_df: 8 learning_rate : 0.01]\n",
      "78 SGD [max_df:0.6 min_df: 9 learning_rate : 0.01]\n",
      "79 SGD [max_df:0.6 min_df: 10 learning_rate : 0.01]\n",
      "80 SGD [max_df:0.7 min_df: 1 learning_rate : 0.01]\n",
      "81 SGD [max_df:0.7 min_df: 2 learning_rate : 0.01]\n",
      "82 SGD [max_df:0.7 min_df: 3 learning_rate : 0.01]\n",
      "83 SGD [max_df:0.7 min_df: 4 learning_rate : 0.01]\n",
      "84 SGD [max_df:0.7 min_df: 5 learning_rate : 0.01]\n",
      "85 SGD [max_df:0.7 min_df: 6 learning_rate : 0.01]\n",
      "86 SGD [max_df:0.7 min_df: 7 learning_rate : 0.01]\n",
      "87 SGD [max_df:0.7 min_df: 8 learning_rate : 0.01]\n",
      "88 SGD [max_df:0.7 min_df: 9 learning_rate : 0.01]\n",
      "89 SGD [max_df:0.7 min_df: 10 learning_rate : 0.01]\n",
      "90 SGD [max_df:0.8 min_df: 1 learning_rate : 0.01]\n",
      "91 SGD [max_df:0.8 min_df: 2 learning_rate : 0.01]\n",
      "92 SGD [max_df:0.8 min_df: 3 learning_rate : 0.01]\n",
      "93 SGD [max_df:0.8 min_df: 4 learning_rate : 0.01]\n",
      "94 SGD [max_df:0.8 min_df: 5 learning_rate : 0.01]\n",
      "95 SGD [max_df:0.8 min_df: 6 learning_rate : 0.01]\n",
      "96 SGD [max_df:0.8 min_df: 7 learning_rate : 0.01]\n",
      "97 SGD [max_df:0.8 min_df: 8 learning_rate : 0.01]\n",
      "98 SGD [max_df:0.8 min_df: 9 learning_rate : 0.01]\n",
      "99 SGD [max_df:0.8 min_df: 10 learning_rate : 0.01]\n",
      "100 SGD [max_df:0.9 min_df: 1 learning_rate : 0.01]\n",
      "101 SGD [max_df:0.9 min_df: 2 learning_rate : 0.01]\n",
      "102 SGD [max_df:0.9 min_df: 3 learning_rate : 0.01]\n",
      "103 SGD [max_df:0.9 min_df: 4 learning_rate : 0.01]\n",
      "104 SGD [max_df:0.9 min_df: 5 learning_rate : 0.01]\n",
      "105 SGD [max_df:0.9 min_df: 6 learning_rate : 0.01]\n",
      "106 SGD [max_df:0.9 min_df: 7 learning_rate : 0.01]\n",
      "107 SGD [max_df:0.9 min_df: 8 learning_rate : 0.01]\n",
      "108 SGD [max_df:0.9 min_df: 9 learning_rate : 0.01]\n",
      "109 SGD [max_df:0.9 min_df: 10 learning_rate : 0.01]\n",
      "110 SGD [max_df:1.0 min_df: 1 learning_rate : 0.01]\n",
      "111 SGD [max_df:1.0 min_df: 2 learning_rate : 0.01]\n",
      "112 SGD [max_df:1.0 min_df: 3 learning_rate : 0.01]\n",
      "113 SGD [max_df:1.0 min_df: 4 learning_rate : 0.01]\n",
      "114 SGD [max_df:1.0 min_df: 5 learning_rate : 0.01]\n",
      "115 SGD [max_df:1.0 min_df: 6 learning_rate : 0.01]\n",
      "116 SGD [max_df:1.0 min_df: 7 learning_rate : 0.01]\n",
      "117 SGD [max_df:1.0 min_df: 8 learning_rate : 0.01]\n",
      "118 SGD [max_df:1.0 min_df: 9 learning_rate : 0.01]\n",
      "119 SGD [max_df:1.0 min_df: 10 learning_rate : 0.01]\n",
      "120 SGD [max_df:0.5 min_df: 1 learning_rate : 0.001]\n",
      "121 SGD [max_df:0.5 min_df: 2 learning_rate : 0.001]\n",
      "122 SGD [max_df:0.5 min_df: 3 learning_rate : 0.001]\n",
      "123 SGD [max_df:0.5 min_df: 4 learning_rate : 0.001]\n",
      "124 SGD [max_df:0.5 min_df: 5 learning_rate : 0.001]\n",
      "125 SGD [max_df:0.5 min_df: 6 learning_rate : 0.001]\n",
      "126 SGD [max_df:0.5 min_df: 7 learning_rate : 0.001]\n",
      "127 SGD [max_df:0.5 min_df: 8 learning_rate : 0.001]\n",
      "128 SGD [max_df:0.5 min_df: 9 learning_rate : 0.001]\n",
      "129 SGD [max_df:0.5 min_df: 10 learning_rate : 0.001]\n",
      "130 SGD [max_df:0.6 min_df: 1 learning_rate : 0.001]\n",
      "131 SGD [max_df:0.6 min_df: 2 learning_rate : 0.001]\n",
      "132 SGD [max_df:0.6 min_df: 3 learning_rate : 0.001]\n",
      "133 SGD [max_df:0.6 min_df: 4 learning_rate : 0.001]\n",
      "134 SGD [max_df:0.6 min_df: 5 learning_rate : 0.001]\n",
      "135 SGD [max_df:0.6 min_df: 6 learning_rate : 0.001]\n",
      "136 SGD [max_df:0.6 min_df: 7 learning_rate : 0.001]\n",
      "137 SGD [max_df:0.6 min_df: 8 learning_rate : 0.001]\n",
      "138 SGD [max_df:0.6 min_df: 9 learning_rate : 0.001]\n",
      "139 SGD [max_df:0.6 min_df: 10 learning_rate : 0.001]\n",
      "140 SGD [max_df:0.7 min_df: 1 learning_rate : 0.001]\n",
      "141 SGD [max_df:0.7 min_df: 2 learning_rate : 0.001]\n",
      "142 SGD [max_df:0.7 min_df: 3 learning_rate : 0.001]\n",
      "143 SGD [max_df:0.7 min_df: 4 learning_rate : 0.001]\n",
      "144 SGD [max_df:0.7 min_df: 5 learning_rate : 0.001]\n",
      "145 SGD [max_df:0.7 min_df: 6 learning_rate : 0.001]\n",
      "146 SGD [max_df:0.7 min_df: 7 learning_rate : 0.001]\n",
      "147 SGD [max_df:0.7 min_df: 8 learning_rate : 0.001]\n",
      "148 SGD [max_df:0.7 min_df: 9 learning_rate : 0.001]\n",
      "149 SGD [max_df:0.7 min_df: 10 learning_rate : 0.001]\n",
      "150 SGD [max_df:0.8 min_df: 1 learning_rate : 0.001]\n",
      "151 SGD [max_df:0.8 min_df: 2 learning_rate : 0.001]\n",
      "152 SGD [max_df:0.8 min_df: 3 learning_rate : 0.001]\n",
      "153 SGD [max_df:0.8 min_df: 4 learning_rate : 0.001]\n",
      "154 SGD [max_df:0.8 min_df: 5 learning_rate : 0.001]\n",
      "155 SGD [max_df:0.8 min_df: 6 learning_rate : 0.001]\n",
      "156 SGD [max_df:0.8 min_df: 7 learning_rate : 0.001]\n",
      "157 SGD [max_df:0.8 min_df: 8 learning_rate : 0.001]\n",
      "158 SGD [max_df:0.8 min_df: 9 learning_rate : 0.001]\n",
      "159 SGD [max_df:0.8 min_df: 10 learning_rate : 0.001]\n",
      "160 SGD [max_df:0.9 min_df: 1 learning_rate : 0.001]\n",
      "161 SGD [max_df:0.9 min_df: 2 learning_rate : 0.001]\n",
      "162 SGD [max_df:0.9 min_df: 3 learning_rate : 0.001]\n",
      "163 SGD [max_df:0.9 min_df: 4 learning_rate : 0.001]\n",
      "164 SGD [max_df:0.9 min_df: 5 learning_rate : 0.001]\n",
      "165 SGD [max_df:0.9 min_df: 6 learning_rate : 0.001]\n",
      "166 SGD [max_df:0.9 min_df: 7 learning_rate : 0.001]\n",
      "167 SGD [max_df:0.9 min_df: 8 learning_rate : 0.001]\n",
      "168 SGD [max_df:0.9 min_df: 9 learning_rate : 0.001]\n",
      "169 SGD [max_df:0.9 min_df: 10 learning_rate : 0.001]\n",
      "170 SGD [max_df:1.0 min_df: 1 learning_rate : 0.001]\n",
      "171 SGD [max_df:1.0 min_df: 2 learning_rate : 0.001]\n",
      "172 SGD [max_df:1.0 min_df: 3 learning_rate : 0.001]\n",
      "173 SGD [max_df:1.0 min_df: 4 learning_rate : 0.001]\n",
      "174 SGD [max_df:1.0 min_df: 5 learning_rate : 0.001]\n",
      "175 SGD [max_df:1.0 min_df: 6 learning_rate : 0.001]\n",
      "176 SGD [max_df:1.0 min_df: 7 learning_rate : 0.001]\n",
      "177 SGD [max_df:1.0 min_df: 8 learning_rate : 0.001]\n",
      "178 SGD [max_df:1.0 min_df: 9 learning_rate : 0.001]\n",
      "179 SGD [max_df:1.0 min_df: 10 learning_rate : 0.001]\n",
      "180 SGD [max_df:0.5 min_df: 1 learning_rate : 0.0001]\n",
      "181 SGD [max_df:0.5 min_df: 2 learning_rate : 0.0001]\n",
      "182 SGD [max_df:0.5 min_df: 3 learning_rate : 0.0001]\n",
      "183 SGD [max_df:0.5 min_df: 4 learning_rate : 0.0001]\n",
      "184 SGD [max_df:0.5 min_df: 5 learning_rate : 0.0001]\n",
      "185 SGD [max_df:0.5 min_df: 6 learning_rate : 0.0001]\n",
      "186 SGD [max_df:0.5 min_df: 7 learning_rate : 0.0001]\n",
      "187 SGD [max_df:0.5 min_df: 8 learning_rate : 0.0001]\n",
      "188 SGD [max_df:0.5 min_df: 9 learning_rate : 0.0001]\n",
      "189 SGD [max_df:0.5 min_df: 10 learning_rate : 0.0001]\n",
      "190 SGD [max_df:0.6 min_df: 1 learning_rate : 0.0001]\n",
      "191 SGD [max_df:0.6 min_df: 2 learning_rate : 0.0001]\n",
      "192 SGD [max_df:0.6 min_df: 3 learning_rate : 0.0001]\n",
      "193 SGD [max_df:0.6 min_df: 4 learning_rate : 0.0001]\n",
      "194 SGD [max_df:0.6 min_df: 5 learning_rate : 0.0001]\n",
      "195 SGD [max_df:0.6 min_df: 6 learning_rate : 0.0001]\n",
      "196 SGD [max_df:0.6 min_df: 7 learning_rate : 0.0001]\n",
      "197 SGD [max_df:0.6 min_df: 8 learning_rate : 0.0001]\n",
      "198 SGD [max_df:0.6 min_df: 9 learning_rate : 0.0001]\n",
      "199 SGD [max_df:0.6 min_df: 10 learning_rate : 0.0001]\n",
      "200 SGD [max_df:0.7 min_df: 1 learning_rate : 0.0001]\n",
      "201 SGD [max_df:0.7 min_df: 2 learning_rate : 0.0001]\n",
      "202 SGD [max_df:0.7 min_df: 3 learning_rate : 0.0001]\n",
      "203 SGD [max_df:0.7 min_df: 4 learning_rate : 0.0001]\n",
      "204 SGD [max_df:0.7 min_df: 5 learning_rate : 0.0001]\n",
      "205 SGD [max_df:0.7 min_df: 6 learning_rate : 0.0001]\n",
      "206 SGD [max_df:0.7 min_df: 7 learning_rate : 0.0001]\n",
      "207 SGD [max_df:0.7 min_df: 8 learning_rate : 0.0001]\n",
      "208 SGD [max_df:0.7 min_df: 9 learning_rate : 0.0001]\n",
      "209 SGD [max_df:0.7 min_df: 10 learning_rate : 0.0001]\n",
      "210 SGD [max_df:0.8 min_df: 1 learning_rate : 0.0001]\n",
      "211 SGD [max_df:0.8 min_df: 2 learning_rate : 0.0001]\n",
      "212 SGD [max_df:0.8 min_df: 3 learning_rate : 0.0001]\n",
      "213 SGD [max_df:0.8 min_df: 4 learning_rate : 0.0001]\n",
      "214 SGD [max_df:0.8 min_df: 5 learning_rate : 0.0001]\n",
      "215 SGD [max_df:0.8 min_df: 6 learning_rate : 0.0001]\n",
      "216 SGD [max_df:0.8 min_df: 7 learning_rate : 0.0001]\n",
      "217 SGD [max_df:0.8 min_df: 8 learning_rate : 0.0001]\n",
      "218 SGD [max_df:0.8 min_df: 9 learning_rate : 0.0001]\n",
      "219 SGD [max_df:0.8 min_df: 10 learning_rate : 0.0001]\n",
      "220 SGD [max_df:0.9 min_df: 1 learning_rate : 0.0001]\n",
      "221 SGD [max_df:0.9 min_df: 2 learning_rate : 0.0001]\n",
      "222 SGD [max_df:0.9 min_df: 3 learning_rate : 0.0001]\n",
      "223 SGD [max_df:0.9 min_df: 4 learning_rate : 0.0001]\n",
      "224 SGD [max_df:0.9 min_df: 5 learning_rate : 0.0001]\n",
      "225 SGD [max_df:0.9 min_df: 6 learning_rate : 0.0001]\n",
      "226 SGD [max_df:0.9 min_df: 7 learning_rate : 0.0001]\n",
      "227 SGD [max_df:0.9 min_df: 8 learning_rate : 0.0001]\n",
      "228 SGD [max_df:0.9 min_df: 9 learning_rate : 0.0001]\n",
      "229 SGD [max_df:0.9 min_df: 10 learning_rate : 0.0001]\n",
      "230 SGD [max_df:1.0 min_df: 1 learning_rate : 0.0001]\n",
      "231 SGD [max_df:1.0 min_df: 2 learning_rate : 0.0001]\n",
      "232 SGD [max_df:1.0 min_df: 3 learning_rate : 0.0001]\n",
      "233 SGD [max_df:1.0 min_df: 4 learning_rate : 0.0001]\n",
      "234 SGD [max_df:1.0 min_df: 5 learning_rate : 0.0001]\n",
      "235 SGD [max_df:1.0 min_df: 6 learning_rate : 0.0001]\n",
      "236 SGD [max_df:1.0 min_df: 7 learning_rate : 0.0001]\n",
      "237 SGD [max_df:1.0 min_df: 8 learning_rate : 0.0001]\n",
      "238 SGD [max_df:1.0 min_df: 9 learning_rate : 0.0001]\n",
      "239 SGD [max_df:1.0 min_df: 10 learning_rate : 0.0001]\n",
      "240 SGD [max_df:0.5 min_df: 1 learning_rate : 1e-05]\n",
      "241 SGD [max_df:0.5 min_df: 2 learning_rate : 1e-05]\n",
      "242 SGD [max_df:0.5 min_df: 3 learning_rate : 1e-05]\n",
      "243 SGD [max_df:0.5 min_df: 4 learning_rate : 1e-05]\n",
      "244 SGD [max_df:0.5 min_df: 5 learning_rate : 1e-05]\n",
      "245 SGD [max_df:0.5 min_df: 6 learning_rate : 1e-05]\n",
      "246 SGD [max_df:0.5 min_df: 7 learning_rate : 1e-05]\n",
      "247 SGD [max_df:0.5 min_df: 8 learning_rate : 1e-05]\n",
      "248 SGD [max_df:0.5 min_df: 9 learning_rate : 1e-05]\n",
      "249 SGD [max_df:0.5 min_df: 10 learning_rate : 1e-05]\n",
      "250 SGD [max_df:0.6 min_df: 1 learning_rate : 1e-05]\n",
      "251 SGD [max_df:0.6 min_df: 2 learning_rate : 1e-05]\n",
      "252 SGD [max_df:0.6 min_df: 3 learning_rate : 1e-05]\n",
      "253 SGD [max_df:0.6 min_df: 4 learning_rate : 1e-05]\n",
      "254 SGD [max_df:0.6 min_df: 5 learning_rate : 1e-05]\n",
      "255 SGD [max_df:0.6 min_df: 6 learning_rate : 1e-05]\n",
      "256 SGD [max_df:0.6 min_df: 7 learning_rate : 1e-05]\n",
      "257 SGD [max_df:0.6 min_df: 8 learning_rate : 1e-05]\n",
      "258 SGD [max_df:0.6 min_df: 9 learning_rate : 1e-05]\n",
      "259 SGD [max_df:0.6 min_df: 10 learning_rate : 1e-05]\n",
      "260 SGD [max_df:0.7 min_df: 1 learning_rate : 1e-05]\n",
      "261 SGD [max_df:0.7 min_df: 2 learning_rate : 1e-05]\n",
      "262 SGD [max_df:0.7 min_df: 3 learning_rate : 1e-05]\n",
      "263 SGD [max_df:0.7 min_df: 4 learning_rate : 1e-05]\n",
      "264 SGD [max_df:0.7 min_df: 5 learning_rate : 1e-05]\n",
      "265 SGD [max_df:0.7 min_df: 6 learning_rate : 1e-05]\n",
      "266 SGD [max_df:0.7 min_df: 7 learning_rate : 1e-05]\n",
      "267 SGD [max_df:0.7 min_df: 8 learning_rate : 1e-05]\n",
      "268 SGD [max_df:0.7 min_df: 9 learning_rate : 1e-05]\n",
      "269 SGD [max_df:0.7 min_df: 10 learning_rate : 1e-05]\n",
      "270 SGD [max_df:0.8 min_df: 1 learning_rate : 1e-05]\n",
      "271 SGD [max_df:0.8 min_df: 2 learning_rate : 1e-05]\n",
      "272 SGD [max_df:0.8 min_df: 3 learning_rate : 1e-05]\n",
      "273 SGD [max_df:0.8 min_df: 4 learning_rate : 1e-05]\n",
      "274 SGD [max_df:0.8 min_df: 5 learning_rate : 1e-05]\n",
      "275 SGD [max_df:0.8 min_df: 6 learning_rate : 1e-05]\n",
      "276 SGD [max_df:0.8 min_df: 7 learning_rate : 1e-05]\n",
      "277 SGD [max_df:0.8 min_df: 8 learning_rate : 1e-05]\n",
      "278 SGD [max_df:0.8 min_df: 9 learning_rate : 1e-05]\n",
      "279 SGD [max_df:0.8 min_df: 10 learning_rate : 1e-05]\n",
      "280 SGD [max_df:0.9 min_df: 1 learning_rate : 1e-05]\n",
      "281 SGD [max_df:0.9 min_df: 2 learning_rate : 1e-05]\n",
      "282 SGD [max_df:0.9 min_df: 3 learning_rate : 1e-05]\n",
      "283 SGD [max_df:0.9 min_df: 4 learning_rate : 1e-05]\n",
      "284 SGD [max_df:0.9 min_df: 5 learning_rate : 1e-05]\n",
      "285 SGD [max_df:0.9 min_df: 6 learning_rate : 1e-05]\n",
      "286 SGD [max_df:0.9 min_df: 7 learning_rate : 1e-05]\n",
      "287 SGD [max_df:0.9 min_df: 8 learning_rate : 1e-05]\n",
      "288 SGD [max_df:0.9 min_df: 9 learning_rate : 1e-05]\n",
      "289 SGD [max_df:0.9 min_df: 10 learning_rate : 1e-05]\n",
      "290 SGD [max_df:1.0 min_df: 1 learning_rate : 1e-05]\n",
      "291 SGD [max_df:1.0 min_df: 2 learning_rate : 1e-05]\n",
      "292 SGD [max_df:1.0 min_df: 3 learning_rate : 1e-05]\n",
      "293 SGD [max_df:1.0 min_df: 4 learning_rate : 1e-05]\n",
      "294 SGD [max_df:1.0 min_df: 5 learning_rate : 1e-05]\n",
      "295 SGD [max_df:1.0 min_df: 6 learning_rate : 1e-05]\n",
      "296 SGD [max_df:1.0 min_df: 7 learning_rate : 1e-05]\n",
      "297 SGD [max_df:1.0 min_df: 8 learning_rate : 1e-05]\n",
      "298 SGD [max_df:1.0 min_df: 9 learning_rate : 1e-05]\n",
      "299 SGD [max_df:1.0 min_df: 10 learning_rate : 1e-05]\n",
      "300 SGD [max_df:0.5 min_df: 1 learning_rate : 1.0000000000000002e-06]\n",
      "301 SGD [max_df:0.5 min_df: 2 learning_rate : 1.0000000000000002e-06]\n",
      "302 SGD [max_df:0.5 min_df: 3 learning_rate : 1.0000000000000002e-06]\n",
      "303 SGD [max_df:0.5 min_df: 4 learning_rate : 1.0000000000000002e-06]\n",
      "304 SGD [max_df:0.5 min_df: 5 learning_rate : 1.0000000000000002e-06]\n",
      "305 SGD [max_df:0.5 min_df: 6 learning_rate : 1.0000000000000002e-06]\n",
      "306 SGD [max_df:0.5 min_df: 7 learning_rate : 1.0000000000000002e-06]\n",
      "307 SGD [max_df:0.5 min_df: 8 learning_rate : 1.0000000000000002e-06]\n",
      "308 SGD [max_df:0.5 min_df: 9 learning_rate : 1.0000000000000002e-06]\n",
      "309 SGD [max_df:0.5 min_df: 10 learning_rate : 1.0000000000000002e-06]\n",
      "310 SGD [max_df:0.6 min_df: 1 learning_rate : 1.0000000000000002e-06]\n",
      "311 SGD [max_df:0.6 min_df: 2 learning_rate : 1.0000000000000002e-06]\n",
      "312 SGD [max_df:0.6 min_df: 3 learning_rate : 1.0000000000000002e-06]\n",
      "313 SGD [max_df:0.6 min_df: 4 learning_rate : 1.0000000000000002e-06]\n",
      "314 SGD [max_df:0.6 min_df: 5 learning_rate : 1.0000000000000002e-06]\n",
      "315 SGD [max_df:0.6 min_df: 6 learning_rate : 1.0000000000000002e-06]\n",
      "316 SGD [max_df:0.6 min_df: 7 learning_rate : 1.0000000000000002e-06]\n",
      "317 SGD [max_df:0.6 min_df: 8 learning_rate : 1.0000000000000002e-06]\n",
      "318 SGD [max_df:0.6 min_df: 9 learning_rate : 1.0000000000000002e-06]\n",
      "319 SGD [max_df:0.6 min_df: 10 learning_rate : 1.0000000000000002e-06]\n",
      "320 SGD [max_df:0.7 min_df: 1 learning_rate : 1.0000000000000002e-06]\n",
      "321 SGD [max_df:0.7 min_df: 2 learning_rate : 1.0000000000000002e-06]\n",
      "322 SGD [max_df:0.7 min_df: 3 learning_rate : 1.0000000000000002e-06]\n",
      "323 SGD [max_df:0.7 min_df: 4 learning_rate : 1.0000000000000002e-06]\n",
      "324 SGD [max_df:0.7 min_df: 5 learning_rate : 1.0000000000000002e-06]\n",
      "325 SGD [max_df:0.7 min_df: 6 learning_rate : 1.0000000000000002e-06]\n",
      "326 SGD [max_df:0.7 min_df: 7 learning_rate : 1.0000000000000002e-06]\n",
      "327 SGD [max_df:0.7 min_df: 8 learning_rate : 1.0000000000000002e-06]\n",
      "328 SGD [max_df:0.7 min_df: 9 learning_rate : 1.0000000000000002e-06]\n",
      "329 SGD [max_df:0.7 min_df: 10 learning_rate : 1.0000000000000002e-06]\n",
      "330 SGD [max_df:0.8 min_df: 1 learning_rate : 1.0000000000000002e-06]\n",
      "331 SGD [max_df:0.8 min_df: 2 learning_rate : 1.0000000000000002e-06]\n",
      "332 SGD [max_df:0.8 min_df: 3 learning_rate : 1.0000000000000002e-06]\n",
      "333 SGD [max_df:0.8 min_df: 4 learning_rate : 1.0000000000000002e-06]\n",
      "334 SGD [max_df:0.8 min_df: 5 learning_rate : 1.0000000000000002e-06]\n",
      "335 SGD [max_df:0.8 min_df: 6 learning_rate : 1.0000000000000002e-06]\n",
      "336 SGD [max_df:0.8 min_df: 7 learning_rate : 1.0000000000000002e-06]\n",
      "337 SGD [max_df:0.8 min_df: 8 learning_rate : 1.0000000000000002e-06]\n",
      "338 SGD [max_df:0.8 min_df: 9 learning_rate : 1.0000000000000002e-06]\n",
      "339 SGD [max_df:0.8 min_df: 10 learning_rate : 1.0000000000000002e-06]\n",
      "340 SGD [max_df:0.9 min_df: 1 learning_rate : 1.0000000000000002e-06]\n",
      "341 SGD [max_df:0.9 min_df: 2 learning_rate : 1.0000000000000002e-06]\n",
      "342 SGD [max_df:0.9 min_df: 3 learning_rate : 1.0000000000000002e-06]\n",
      "343 SGD [max_df:0.9 min_df: 4 learning_rate : 1.0000000000000002e-06]\n",
      "344 SGD [max_df:0.9 min_df: 5 learning_rate : 1.0000000000000002e-06]\n",
      "345 SGD [max_df:0.9 min_df: 6 learning_rate : 1.0000000000000002e-06]\n",
      "346 SGD [max_df:0.9 min_df: 7 learning_rate : 1.0000000000000002e-06]\n",
      "347 SGD [max_df:0.9 min_df: 8 learning_rate : 1.0000000000000002e-06]\n",
      "348 SGD [max_df:0.9 min_df: 9 learning_rate : 1.0000000000000002e-06]\n",
      "349 SGD [max_df:0.9 min_df: 10 learning_rate : 1.0000000000000002e-06]\n",
      "350 SGD [max_df:1.0 min_df: 1 learning_rate : 1.0000000000000002e-06]\n",
      "351 SGD [max_df:1.0 min_df: 2 learning_rate : 1.0000000000000002e-06]\n",
      "352 SGD [max_df:1.0 min_df: 3 learning_rate : 1.0000000000000002e-06]\n",
      "353 SGD [max_df:1.0 min_df: 4 learning_rate : 1.0000000000000002e-06]\n",
      "354 SGD [max_df:1.0 min_df: 5 learning_rate : 1.0000000000000002e-06]\n",
      "355 SGD [max_df:1.0 min_df: 6 learning_rate : 1.0000000000000002e-06]\n",
      "356 SGD [max_df:1.0 min_df: 7 learning_rate : 1.0000000000000002e-06]\n",
      "357 SGD [max_df:1.0 min_df: 8 learning_rate : 1.0000000000000002e-06]\n",
      "358 SGD [max_df:1.0 min_df: 9 learning_rate : 1.0000000000000002e-06]\n",
      "359 SGD [max_df:1.0 min_df: 10 learning_rate : 1.0000000000000002e-06]\n",
      "360 SGD [max_df:0.5 min_df: 1 learning_rate : 1.0000000000000002e-07]\n",
      "361 SGD [max_df:0.5 min_df: 2 learning_rate : 1.0000000000000002e-07]\n",
      "362 SGD [max_df:0.5 min_df: 3 learning_rate : 1.0000000000000002e-07]\n",
      "363 SGD [max_df:0.5 min_df: 4 learning_rate : 1.0000000000000002e-07]\n",
      "364 SGD [max_df:0.5 min_df: 5 learning_rate : 1.0000000000000002e-07]\n",
      "365 SGD [max_df:0.5 min_df: 6 learning_rate : 1.0000000000000002e-07]\n",
      "366 SGD [max_df:0.5 min_df: 7 learning_rate : 1.0000000000000002e-07]\n",
      "367 SGD [max_df:0.5 min_df: 8 learning_rate : 1.0000000000000002e-07]\n",
      "368 SGD [max_df:0.5 min_df: 9 learning_rate : 1.0000000000000002e-07]\n",
      "369 SGD [max_df:0.5 min_df: 10 learning_rate : 1.0000000000000002e-07]\n",
      "370 SGD [max_df:0.6 min_df: 1 learning_rate : 1.0000000000000002e-07]\n",
      "371 SGD [max_df:0.6 min_df: 2 learning_rate : 1.0000000000000002e-07]\n",
      "372 SGD [max_df:0.6 min_df: 3 learning_rate : 1.0000000000000002e-07]\n",
      "373 SGD [max_df:0.6 min_df: 4 learning_rate : 1.0000000000000002e-07]\n",
      "374 SGD [max_df:0.6 min_df: 5 learning_rate : 1.0000000000000002e-07]\n",
      "375 SGD [max_df:0.6 min_df: 6 learning_rate : 1.0000000000000002e-07]\n",
      "376 SGD [max_df:0.6 min_df: 7 learning_rate : 1.0000000000000002e-07]\n",
      "377 SGD [max_df:0.6 min_df: 8 learning_rate : 1.0000000000000002e-07]\n",
      "378 SGD [max_df:0.6 min_df: 9 learning_rate : 1.0000000000000002e-07]\n",
      "379 SGD [max_df:0.6 min_df: 10 learning_rate : 1.0000000000000002e-07]\n",
      "380 SGD [max_df:0.7 min_df: 1 learning_rate : 1.0000000000000002e-07]\n",
      "381 SGD [max_df:0.7 min_df: 2 learning_rate : 1.0000000000000002e-07]\n",
      "382 SGD [max_df:0.7 min_df: 3 learning_rate : 1.0000000000000002e-07]\n",
      "383 SGD [max_df:0.7 min_df: 4 learning_rate : 1.0000000000000002e-07]\n",
      "384 SGD [max_df:0.7 min_df: 5 learning_rate : 1.0000000000000002e-07]\n",
      "385 SGD [max_df:0.7 min_df: 6 learning_rate : 1.0000000000000002e-07]\n",
      "386 SGD [max_df:0.7 min_df: 7 learning_rate : 1.0000000000000002e-07]\n",
      "387 SGD [max_df:0.7 min_df: 8 learning_rate : 1.0000000000000002e-07]\n",
      "388 SGD [max_df:0.7 min_df: 9 learning_rate : 1.0000000000000002e-07]\n",
      "389 SGD [max_df:0.7 min_df: 10 learning_rate : 1.0000000000000002e-07]\n",
      "390 SGD [max_df:0.8 min_df: 1 learning_rate : 1.0000000000000002e-07]\n",
      "391 SGD [max_df:0.8 min_df: 2 learning_rate : 1.0000000000000002e-07]\n",
      "392 SGD [max_df:0.8 min_df: 3 learning_rate : 1.0000000000000002e-07]\n",
      "393 SGD [max_df:0.8 min_df: 4 learning_rate : 1.0000000000000002e-07]\n",
      "394 SGD [max_df:0.8 min_df: 5 learning_rate : 1.0000000000000002e-07]\n",
      "395 SGD [max_df:0.8 min_df: 6 learning_rate : 1.0000000000000002e-07]\n",
      "396 SGD [max_df:0.8 min_df: 7 learning_rate : 1.0000000000000002e-07]\n",
      "397 SGD [max_df:0.8 min_df: 8 learning_rate : 1.0000000000000002e-07]\n",
      "398 SGD [max_df:0.8 min_df: 9 learning_rate : 1.0000000000000002e-07]\n",
      "399 SGD [max_df:0.8 min_df: 10 learning_rate : 1.0000000000000002e-07]\n",
      "400 SGD [max_df:0.9 min_df: 1 learning_rate : 1.0000000000000002e-07]\n",
      "401 SGD [max_df:0.9 min_df: 2 learning_rate : 1.0000000000000002e-07]\n",
      "402 SGD [max_df:0.9 min_df: 3 learning_rate : 1.0000000000000002e-07]\n",
      "403 SGD [max_df:0.9 min_df: 4 learning_rate : 1.0000000000000002e-07]\n",
      "404 SGD [max_df:0.9 min_df: 5 learning_rate : 1.0000000000000002e-07]\n",
      "405 SGD [max_df:0.9 min_df: 6 learning_rate : 1.0000000000000002e-07]\n",
      "406 SGD [max_df:0.9 min_df: 7 learning_rate : 1.0000000000000002e-07]\n",
      "407 SGD [max_df:0.9 min_df: 8 learning_rate : 1.0000000000000002e-07]\n",
      "408 SGD [max_df:0.9 min_df: 9 learning_rate : 1.0000000000000002e-07]\n",
      "409 SGD [max_df:0.9 min_df: 10 learning_rate : 1.0000000000000002e-07]\n",
      "410 SGD [max_df:1.0 min_df: 1 learning_rate : 1.0000000000000002e-07]\n",
      "411 SGD [max_df:1.0 min_df: 2 learning_rate : 1.0000000000000002e-07]\n",
      "412 SGD [max_df:1.0 min_df: 3 learning_rate : 1.0000000000000002e-07]\n",
      "413 SGD [max_df:1.0 min_df: 4 learning_rate : 1.0000000000000002e-07]\n",
      "414 SGD [max_df:1.0 min_df: 5 learning_rate : 1.0000000000000002e-07]\n",
      "415 SGD [max_df:1.0 min_df: 6 learning_rate : 1.0000000000000002e-07]\n",
      "416 SGD [max_df:1.0 min_df: 7 learning_rate : 1.0000000000000002e-07]\n",
      "417 SGD [max_df:1.0 min_df: 8 learning_rate : 1.0000000000000002e-07]\n",
      "418 SGD [max_df:1.0 min_df: 9 learning_rate : 1.0000000000000002e-07]\n",
      "419 SGD [max_df:1.0 min_df: 10 learning_rate : 1.0000000000000002e-07]\n",
      "420 SGD [max_df:0.5 min_df: 1 learning_rate : 1.0000000000000002e-08]\n",
      "421 SGD [max_df:0.5 min_df: 2 learning_rate : 1.0000000000000002e-08]\n",
      "422 SGD [max_df:0.5 min_df: 3 learning_rate : 1.0000000000000002e-08]\n",
      "423 SGD [max_df:0.5 min_df: 4 learning_rate : 1.0000000000000002e-08]\n",
      "424 SGD [max_df:0.5 min_df: 5 learning_rate : 1.0000000000000002e-08]\n",
      "425 SGD [max_df:0.5 min_df: 6 learning_rate : 1.0000000000000002e-08]\n",
      "426 SGD [max_df:0.5 min_df: 7 learning_rate : 1.0000000000000002e-08]\n",
      "427 SGD [max_df:0.5 min_df: 8 learning_rate : 1.0000000000000002e-08]\n",
      "428 SGD [max_df:0.5 min_df: 9 learning_rate : 1.0000000000000002e-08]\n",
      "429 SGD [max_df:0.5 min_df: 10 learning_rate : 1.0000000000000002e-08]\n",
      "430 SGD [max_df:0.6 min_df: 1 learning_rate : 1.0000000000000002e-08]\n",
      "431 SGD [max_df:0.6 min_df: 2 learning_rate : 1.0000000000000002e-08]\n",
      "432 SGD [max_df:0.6 min_df: 3 learning_rate : 1.0000000000000002e-08]\n",
      "433 SGD [max_df:0.6 min_df: 4 learning_rate : 1.0000000000000002e-08]\n",
      "434 SGD [max_df:0.6 min_df: 5 learning_rate : 1.0000000000000002e-08]\n",
      "435 SGD [max_df:0.6 min_df: 6 learning_rate : 1.0000000000000002e-08]\n",
      "436 SGD [max_df:0.6 min_df: 7 learning_rate : 1.0000000000000002e-08]\n",
      "437 SGD [max_df:0.6 min_df: 8 learning_rate : 1.0000000000000002e-08]\n",
      "438 SGD [max_df:0.6 min_df: 9 learning_rate : 1.0000000000000002e-08]\n",
      "439 SGD [max_df:0.6 min_df: 10 learning_rate : 1.0000000000000002e-08]\n",
      "440 SGD [max_df:0.7 min_df: 1 learning_rate : 1.0000000000000002e-08]\n",
      "441 SGD [max_df:0.7 min_df: 2 learning_rate : 1.0000000000000002e-08]\n",
      "442 SGD [max_df:0.7 min_df: 3 learning_rate : 1.0000000000000002e-08]\n",
      "443 SGD [max_df:0.7 min_df: 4 learning_rate : 1.0000000000000002e-08]\n",
      "444 SGD [max_df:0.7 min_df: 5 learning_rate : 1.0000000000000002e-08]\n",
      "445 SGD [max_df:0.7 min_df: 6 learning_rate : 1.0000000000000002e-08]\n",
      "446 SGD [max_df:0.7 min_df: 7 learning_rate : 1.0000000000000002e-08]\n",
      "447 SGD [max_df:0.7 min_df: 8 learning_rate : 1.0000000000000002e-08]\n",
      "448 SGD [max_df:0.7 min_df: 9 learning_rate : 1.0000000000000002e-08]\n",
      "449 SGD [max_df:0.7 min_df: 10 learning_rate : 1.0000000000000002e-08]\n",
      "450 SGD [max_df:0.8 min_df: 1 learning_rate : 1.0000000000000002e-08]\n",
      "451 SGD [max_df:0.8 min_df: 2 learning_rate : 1.0000000000000002e-08]\n",
      "452 SGD [max_df:0.8 min_df: 3 learning_rate : 1.0000000000000002e-08]\n",
      "453 SGD [max_df:0.8 min_df: 4 learning_rate : 1.0000000000000002e-08]\n",
      "454 SGD [max_df:0.8 min_df: 5 learning_rate : 1.0000000000000002e-08]\n",
      "455 SGD [max_df:0.8 min_df: 6 learning_rate : 1.0000000000000002e-08]\n",
      "456 SGD [max_df:0.8 min_df: 7 learning_rate : 1.0000000000000002e-08]\n",
      "457 SGD [max_df:0.8 min_df: 8 learning_rate : 1.0000000000000002e-08]\n",
      "458 SGD [max_df:0.8 min_df: 9 learning_rate : 1.0000000000000002e-08]\n",
      "459 SGD [max_df:0.8 min_df: 10 learning_rate : 1.0000000000000002e-08]\n",
      "460 SGD [max_df:0.9 min_df: 1 learning_rate : 1.0000000000000002e-08]\n",
      "461 SGD [max_df:0.9 min_df: 2 learning_rate : 1.0000000000000002e-08]\n",
      "462 SGD [max_df:0.9 min_df: 3 learning_rate : 1.0000000000000002e-08]\n",
      "463 SGD [max_df:0.9 min_df: 4 learning_rate : 1.0000000000000002e-08]\n",
      "464 SGD [max_df:0.9 min_df: 5 learning_rate : 1.0000000000000002e-08]\n",
      "465 SGD [max_df:0.9 min_df: 6 learning_rate : 1.0000000000000002e-08]\n",
      "466 SGD [max_df:0.9 min_df: 7 learning_rate : 1.0000000000000002e-08]\n",
      "467 SGD [max_df:0.9 min_df: 8 learning_rate : 1.0000000000000002e-08]\n",
      "468 SGD [max_df:0.9 min_df: 9 learning_rate : 1.0000000000000002e-08]\n",
      "469 SGD [max_df:0.9 min_df: 10 learning_rate : 1.0000000000000002e-08]\n",
      "470 SGD [max_df:1.0 min_df: 1 learning_rate : 1.0000000000000002e-08]\n",
      "471 SGD [max_df:1.0 min_df: 2 learning_rate : 1.0000000000000002e-08]\n",
      "472 SGD [max_df:1.0 min_df: 3 learning_rate : 1.0000000000000002e-08]\n",
      "473 SGD [max_df:1.0 min_df: 4 learning_rate : 1.0000000000000002e-08]\n",
      "474 SGD [max_df:1.0 min_df: 5 learning_rate : 1.0000000000000002e-08]\n",
      "475 SGD [max_df:1.0 min_df: 6 learning_rate : 1.0000000000000002e-08]\n",
      "476 SGD [max_df:1.0 min_df: 7 learning_rate : 1.0000000000000002e-08]\n",
      "477 SGD [max_df:1.0 min_df: 8 learning_rate : 1.0000000000000002e-08]\n",
      "478 SGD [max_df:1.0 min_df: 9 learning_rate : 1.0000000000000002e-08]\n",
      "479 SGD [max_df:1.0 min_df: 10 learning_rate : 1.0000000000000002e-08]\n",
      "480 SGD [max_df:0.5 min_df: 1 learning_rate : 1.0000000000000003e-09]\n",
      "481 SGD [max_df:0.5 min_df: 2 learning_rate : 1.0000000000000003e-09]\n",
      "482 SGD [max_df:0.5 min_df: 3 learning_rate : 1.0000000000000003e-09]\n",
      "483 SGD [max_df:0.5 min_df: 4 learning_rate : 1.0000000000000003e-09]\n",
      "484 SGD [max_df:0.5 min_df: 5 learning_rate : 1.0000000000000003e-09]\n",
      "485 SGD [max_df:0.5 min_df: 6 learning_rate : 1.0000000000000003e-09]\n",
      "486 SGD [max_df:0.5 min_df: 7 learning_rate : 1.0000000000000003e-09]\n",
      "487 SGD [max_df:0.5 min_df: 8 learning_rate : 1.0000000000000003e-09]\n",
      "488 SGD [max_df:0.5 min_df: 9 learning_rate : 1.0000000000000003e-09]\n",
      "489 SGD [max_df:0.5 min_df: 10 learning_rate : 1.0000000000000003e-09]\n",
      "490 SGD [max_df:0.6 min_df: 1 learning_rate : 1.0000000000000003e-09]\n",
      "491 SGD [max_df:0.6 min_df: 2 learning_rate : 1.0000000000000003e-09]\n",
      "492 SGD [max_df:0.6 min_df: 3 learning_rate : 1.0000000000000003e-09]\n",
      "493 SGD [max_df:0.6 min_df: 4 learning_rate : 1.0000000000000003e-09]\n",
      "494 SGD [max_df:0.6 min_df: 5 learning_rate : 1.0000000000000003e-09]\n",
      "495 SGD [max_df:0.6 min_df: 6 learning_rate : 1.0000000000000003e-09]\n",
      "496 SGD [max_df:0.6 min_df: 7 learning_rate : 1.0000000000000003e-09]\n",
      "497 SGD [max_df:0.6 min_df: 8 learning_rate : 1.0000000000000003e-09]\n",
      "498 SGD [max_df:0.6 min_df: 9 learning_rate : 1.0000000000000003e-09]\n",
      "499 SGD [max_df:0.6 min_df: 10 learning_rate : 1.0000000000000003e-09]\n",
      "500 SGD [max_df:0.7 min_df: 1 learning_rate : 1.0000000000000003e-09]\n",
      "501 SGD [max_df:0.7 min_df: 2 learning_rate : 1.0000000000000003e-09]\n",
      "502 SGD [max_df:0.7 min_df: 3 learning_rate : 1.0000000000000003e-09]\n",
      "503 SGD [max_df:0.7 min_df: 4 learning_rate : 1.0000000000000003e-09]\n",
      "504 SGD [max_df:0.7 min_df: 5 learning_rate : 1.0000000000000003e-09]\n",
      "505 SGD [max_df:0.7 min_df: 6 learning_rate : 1.0000000000000003e-09]\n",
      "506 SGD [max_df:0.7 min_df: 7 learning_rate : 1.0000000000000003e-09]\n",
      "507 SGD [max_df:0.7 min_df: 8 learning_rate : 1.0000000000000003e-09]\n",
      "508 SGD [max_df:0.7 min_df: 9 learning_rate : 1.0000000000000003e-09]\n",
      "509 SGD [max_df:0.7 min_df: 10 learning_rate : 1.0000000000000003e-09]\n",
      "510 SGD [max_df:0.8 min_df: 1 learning_rate : 1.0000000000000003e-09]\n",
      "511 SGD [max_df:0.8 min_df: 2 learning_rate : 1.0000000000000003e-09]\n",
      "512 SGD [max_df:0.8 min_df: 3 learning_rate : 1.0000000000000003e-09]\n",
      "513 SGD [max_df:0.8 min_df: 4 learning_rate : 1.0000000000000003e-09]\n",
      "514 SGD [max_df:0.8 min_df: 5 learning_rate : 1.0000000000000003e-09]\n",
      "515 SGD [max_df:0.8 min_df: 6 learning_rate : 1.0000000000000003e-09]\n",
      "516 SGD [max_df:0.8 min_df: 7 learning_rate : 1.0000000000000003e-09]\n",
      "517 SGD [max_df:0.8 min_df: 8 learning_rate : 1.0000000000000003e-09]\n",
      "518 SGD [max_df:0.8 min_df: 9 learning_rate : 1.0000000000000003e-09]\n",
      "519 SGD [max_df:0.8 min_df: 10 learning_rate : 1.0000000000000003e-09]\n",
      "520 SGD [max_df:0.9 min_df: 1 learning_rate : 1.0000000000000003e-09]\n",
      "521 SGD [max_df:0.9 min_df: 2 learning_rate : 1.0000000000000003e-09]\n",
      "522 SGD [max_df:0.9 min_df: 3 learning_rate : 1.0000000000000003e-09]\n",
      "523 SGD [max_df:0.9 min_df: 4 learning_rate : 1.0000000000000003e-09]\n",
      "524 SGD [max_df:0.9 min_df: 5 learning_rate : 1.0000000000000003e-09]\n",
      "525 SGD [max_df:0.9 min_df: 6 learning_rate : 1.0000000000000003e-09]\n",
      "526 SGD [max_df:0.9 min_df: 7 learning_rate : 1.0000000000000003e-09]\n",
      "527 SGD [max_df:0.9 min_df: 8 learning_rate : 1.0000000000000003e-09]\n",
      "528 SGD [max_df:0.9 min_df: 9 learning_rate : 1.0000000000000003e-09]\n",
      "529 SGD [max_df:0.9 min_df: 10 learning_rate : 1.0000000000000003e-09]\n",
      "530 SGD [max_df:1.0 min_df: 1 learning_rate : 1.0000000000000003e-09]\n",
      "531 SGD [max_df:1.0 min_df: 2 learning_rate : 1.0000000000000003e-09]\n",
      "532 SGD [max_df:1.0 min_df: 3 learning_rate : 1.0000000000000003e-09]\n",
      "533 SGD [max_df:1.0 min_df: 4 learning_rate : 1.0000000000000003e-09]\n",
      "534 SGD [max_df:1.0 min_df: 5 learning_rate : 1.0000000000000003e-09]\n",
      "535 SGD [max_df:1.0 min_df: 6 learning_rate : 1.0000000000000003e-09]\n",
      "536 SGD [max_df:1.0 min_df: 7 learning_rate : 1.0000000000000003e-09]\n",
      "537 SGD [max_df:1.0 min_df: 8 learning_rate : 1.0000000000000003e-09]\n",
      "538 SGD [max_df:1.0 min_df: 9 learning_rate : 1.0000000000000003e-09]\n",
      "539 SGD [max_df:1.0 min_df: 10 learning_rate : 1.0000000000000003e-09]\n",
      "540 SGD [max_df:0.5 min_df: 1]\n",
      "541 SGD [max_df:0.5 min_df: 2]\n",
      "542 SGD [max_df:0.5 min_df: 3]\n",
      "543 SGD [max_df:0.5 min_df: 4]\n",
      "544 SGD [max_df:0.5 min_df: 5]\n",
      "545 SGD [max_df:0.5 min_df: 6]\n",
      "546 SGD [max_df:0.5 min_df: 7]\n",
      "547 SGD [max_df:0.5 min_df: 8]\n",
      "548 SGD [max_df:0.5 min_df: 9]\n",
      "549 SGD [max_df:0.5 min_df: 10]\n",
      "550 SGD [max_df:0.6 min_df: 1]\n",
      "551 SGD [max_df:0.6 min_df: 2]\n",
      "552 SGD [max_df:0.6 min_df: 3]\n",
      "553 SGD [max_df:0.6 min_df: 4]\n",
      "554 SGD [max_df:0.6 min_df: 5]\n",
      "555 SGD [max_df:0.6 min_df: 6]\n",
      "556 SGD [max_df:0.6 min_df: 7]\n",
      "557 SGD [max_df:0.6 min_df: 8]\n",
      "558 SGD [max_df:0.6 min_df: 9]\n",
      "559 SGD [max_df:0.6 min_df: 10]\n",
      "560 SGD [max_df:0.7 min_df: 1]\n",
      "561 SGD [max_df:0.7 min_df: 2]\n",
      "562 SGD [max_df:0.7 min_df: 3]\n",
      "563 SGD [max_df:0.7 min_df: 4]\n",
      "564 SGD [max_df:0.7 min_df: 5]\n",
      "565 SGD [max_df:0.7 min_df: 6]\n",
      "566 SGD [max_df:0.7 min_df: 7]\n",
      "567 SGD [max_df:0.7 min_df: 8]\n",
      "568 SGD [max_df:0.7 min_df: 9]\n",
      "569 SGD [max_df:0.7 min_df: 10]\n",
      "570 SGD [max_df:0.8 min_df: 1]\n",
      "571 SGD [max_df:0.8 min_df: 2]\n",
      "572 SGD [max_df:0.8 min_df: 3]\n",
      "573 SGD [max_df:0.8 min_df: 4]\n",
      "574 SGD [max_df:0.8 min_df: 5]\n",
      "575 SGD [max_df:0.8 min_df: 6]\n",
      "576 SGD [max_df:0.8 min_df: 7]\n",
      "577 SGD [max_df:0.8 min_df: 8]\n",
      "578 SGD [max_df:0.8 min_df: 9]\n",
      "579 SGD [max_df:0.8 min_df: 10]\n",
      "580 SGD [max_df:0.9 min_df: 1]\n",
      "581 SGD [max_df:0.9 min_df: 2]\n",
      "582 SGD [max_df:0.9 min_df: 3]\n",
      "583 SGD [max_df:0.9 min_df: 4]\n",
      "584 SGD [max_df:0.9 min_df: 5]\n",
      "585 SGD [max_df:0.9 min_df: 6]\n",
      "586 SGD [max_df:0.9 min_df: 7]\n",
      "587 SGD [max_df:0.9 min_df: 8]\n",
      "588 SGD [max_df:0.9 min_df: 9]\n",
      "589 SGD [max_df:0.9 min_df: 10]\n",
      "590 SGD [max_df:1.0 min_df: 1]\n",
      "591 SGD [max_df:1.0 min_df: 2]\n",
      "592 SGD [max_df:1.0 min_df: 3]\n",
      "593 SGD [max_df:1.0 min_df: 4]\n",
      "594 SGD [max_df:1.0 min_df: 5]\n",
      "595 SGD [max_df:1.0 min_df: 6]\n",
      "596 SGD [max_df:1.0 min_df: 7]\n",
      "597 SGD [max_df:1.0 min_df: 8]\n",
      "598 SGD [max_df:1.0 min_df: 9]\n",
      "599 SGD [max_df:1.0 min_df: 10]\n",
      "600\n",
      "--\n",
      "SGD [max_df:0.5 min_df: 3 learning_rate : 0.1]\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier\n",
    "SGDtrials = []\n",
    "cont = 0\n",
    "lr = 1\n",
    "for i in range(1,10):\n",
    "    lr = lr / 10\n",
    "\n",
    "    for j in range(5, 11, 1):\n",
    "        max_df = j/10\n",
    "        for k in range(1, 11, 1):\n",
    "            min_df = k\n",
    "\n",
    "            # SGD\n",
    "            trial = Pipeline([\n",
    "                ('vectorizer', TfidfVectorizer(min_df=min_df, max_df=max_df)),\n",
    "                ('classifier', SGDClassifier(random_state=10)),\n",
    "                ])\n",
    "            desc = 'SGD [max_df:' + str(max_df) + ' min_df: ' + str(min_df) + ' learning_rate : ' +str(lr) + ']'\n",
    "            SGDtrials.append((cont,desc,trial))\n",
    "            cont+=1\n",
    "            allTrials.append((contAll,desc,trial))\n",
    "            contAll+=1            \n",
    "            \n",
    "for j in range(5, 11, 1):\n",
    "    max_df = j/10\n",
    "    for k in range(1, 11, 1):\n",
    "        min_df = k\n",
    "\n",
    "        # SGD\n",
    "        trial = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(min_df=min_df, max_df=max_df)),\n",
    "            ('classifier', SGDClassifier(random_state=10)),\n",
    "            ])\n",
    "        desc = 'SGD [max_df:' + str(max_df) + ' min_df: ' + str(min_df) + ']'\n",
    "        SGDtrials.append((cont,desc,trial))\n",
    "        cont+=1\n",
    "        allTrials.append((contAll,desc,trial))\n",
    "        contAll+=1            \n",
    "            \n",
    "            \n",
    "            \n",
    "for t in SGDtrials:\n",
    "    print(t[0],t[1])\n",
    "print(len(SGDtrials))\n",
    "print('--')\n",
    "print(SGDtrials[2][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 SGD [max_df:0.5 min_df: 1]\n",
      "1 SGD [max_df:0.5 min_df: 2]\n",
      "2 SGD [max_df:0.5 min_df: 3]\n",
      "3 SGD [max_df:0.5 min_df: 4]\n",
      "4 SGD [max_df:0.5 min_df: 5]\n",
      "5 SGD [max_df:0.5 min_df: 6]\n",
      "6 SGD [max_df:0.5 min_df: 7]\n",
      "7 SGD [max_df:0.5 min_df: 8]\n",
      "8 SGD [max_df:0.5 min_df: 9]\n",
      "9 SGD [max_df:0.5 min_df: 10]\n",
      "60\n",
      "--\n",
      "SGD [max_df:0.5 min_df: 3]\n",
      "\n",
      "\n",
      "\n",
      "0 LinearSVC [max_df:0.5 min_df: 1]\n",
      "1 LinearSVC [max_df:0.5 min_df: 2]\n",
      "2 LinearSVC [max_df:0.5 min_df: 3]\n",
      "3 LinearSVC [max_df:0.5 min_df: 4]\n",
      "4 LinearSVC [max_df:0.5 min_df: 5]\n",
      "5 LinearSVC [max_df:0.5 min_df: 6]\n",
      "6 LinearSVC [max_df:0.5 min_df: 7]\n",
      "7 LinearSVC [max_df:0.5 min_df: 8]\n",
      "8 LinearSVC [max_df:0.5 min_df: 9]\n",
      "9 LinearSVC [max_df:0.5 min_df: 10]\n",
      "60\n",
      "--\n",
      "LinearSVC [max_df:0.5 min_df: 3]\n",
      "\n",
      "\n",
      "\n",
      "0 SVC [max_df:0.5 min_df: 1]\n",
      "1 SVC [max_df:0.5 min_df: 2]\n",
      "2 SVC [max_df:0.5 min_df: 3]\n",
      "3 SVC [max_df:0.5 min_df: 4]\n",
      "4 SVC [max_df:0.5 min_df: 5]\n",
      "5 SVC [max_df:0.5 min_df: 6]\n",
      "6 SVC [max_df:0.5 min_df: 7]\n",
      "7 SVC [max_df:0.5 min_df: 8]\n",
      "8 SVC [max_df:0.5 min_df: 9]\n",
      "9 SVC [max_df:0.5 min_df: 10]\n",
      "60\n",
      "--\n",
      "SVC [max_df:0.5 min_df: 3]\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier\n",
    "SGDtrials = []\n",
    "LinearSVCtrials = []\n",
    "SVCtrials = []\n",
    "cont = 0\n",
    "\n",
    "for j in range(5, 11, 1):\n",
    "    max_df = j/10\n",
    "    for k in range(1, 11, 1):\n",
    "        min_df = k\n",
    "        \n",
    "        # SGD\n",
    "        trial = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(min_df=min_df, max_df=max_df)),\n",
    "            ('classifier', SGDClassifier(random_state=10)),\n",
    "            ])\n",
    "        desc = 'SGD [max_df:' + str(max_df) + ' min_df: ' + str(min_df) + ']'\n",
    "        SGDtrials.append((cont,desc,trial))\n",
    "        allTrials.append((contAll,desc,trial))\n",
    "        contAll+=1            \n",
    "        \n",
    "        \n",
    "        # Linear SVC\n",
    "        trial = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(min_df=min_df, max_df=max_df)),\n",
    "            ('classifier', LinearSVC()),\n",
    "            ])\n",
    "        desc = 'LinearSVC [max_df:' + str(max_df) + ' min_df: ' + str(min_df) + ']'\n",
    "        LinearSVCtrials.append((cont,desc,trial))\n",
    "        allTrials.append((contAll,desc,trial))\n",
    "        contAll+=1            \n",
    "        \n",
    "        # SVC\n",
    "        trial = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(min_df=min_df, max_df=max_df)),\n",
    "            ('classifier', SVC()),\n",
    "            ])\n",
    "        desc = 'SVC [max_df:' + str(max_df) + ' min_df: ' + str(min_df) + ']'\n",
    "        SVCtrials.append((cont,desc,trial))\n",
    "        allTrials.append((contAll,desc,trial))\n",
    "        contAll+=1            \n",
    "        \n",
    "        \n",
    "        cont+=1\n",
    "\n",
    "\n",
    "for t in SGDtrials[:10]:\n",
    "    print(t[0],t[1])\n",
    "print(len(SGDtrials))\n",
    "print('--')\n",
    "print(SGDtrials[2][1])\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "for t in LinearSVCtrials[:10]:\n",
    "    print(t[0],t[1])\n",
    "print(len(LinearSVCtrials))\n",
    "print('--')\n",
    "print(LinearSVCtrials[2][1])\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "for t in SVCtrials[:10]:\n",
    "    print(t[0],t[1])\n",
    "print(len(SVCtrials))\n",
    "print('--')\n",
    "print(SVCtrials[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "60\n",
      "660\n",
      "2400\n",
      "3240\n"
     ]
    }
   ],
   "source": [
    "print(len(SGDtrials))\n",
    "print(len(LinearSVCtrials))\n",
    "print(len(SVCtrials))\n",
    "print(len(MNBtrials))\n",
    "print(len(RFtrials))\n",
    "\n",
    "allTrials = SGDtrials + LinearSVCtrials + SVCtrials + MNBtrials + RFtrials\n",
    "print(len(allTrials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import  make_scorer\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=42)\n",
    "model=allTrials[928][2]\n",
    "\n",
    "#results = model_selection.cross_val_score(estimator=model,\n",
    "#                                          X=features,\n",
    "#                                          y=labels,\n",
    "#                                          cv=kfold,\n",
    "#                                          scoring='f1_micro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile functions/classificacao.py\n",
    "def classifierScore(classifier, X_train, y_train, X, y, metrica, idx):\n",
    "    \n",
    "    scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "               'precision' : make_scorer(precision_score),\n",
    "               'recall' : make_scorer(recall_score), \n",
    "               'f1' : make_scorer(f1_score)}\n",
    "    scoring = ['accuracy', 'precision']\n",
    "    scoring = 'f1_micro'\n",
    "\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=42)\n",
    "    \n",
    "    #print(classifier[1])\n",
    "    classifier[2].fit(X_train, y_train)\n",
    "    scores = model_selection.cross_val_score(classifier[2], X, y, cv=kfold, scoring=metrica)\n",
    "    return (idx, scores)\n",
    "\n",
    "def bestClassifier(X, y, classifiers, metrica):\n",
    "    target_names = list(set(y))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=50)\n",
    "    maxScore = 0\n",
    "    ic = 0\n",
    "    \n",
    "    \n",
    "    #for idx, val in enumerate(allTrials):\n",
    "        \n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    resultados = Parallel(n_jobs=num_cores)(delayed(classifierScore)(classifier, X_train, y_train, X, y, metrica, idx) \n",
    "                                            for idx, classifier in enumerate(classifiers))\n",
    "                                            #for classifier in classifiers)\n",
    "    #print(resultados)\n",
    "\n",
    "    for result in resultados:\n",
    "        classifier = classifiers[result[0]]\n",
    "\n",
    "        scores = result[1]\n",
    "        #print(scores)\n",
    "\n",
    "        score = scores.mean()\n",
    "        std = scores.std() * 2\n",
    "\n",
    "        if score > maxScore:\n",
    "            maxScore = score\n",
    "            maxStd = std\n",
    "            maxIc = ic\n",
    "            bClassifier = classifier\n",
    "        ic += 1\n",
    "\n",
    "    print(bClassifier[1], '[', bClassifier[0], ']')\n",
    "    print(\"max: \", maxScore)\n",
    "    print(\"%s: %0.2f (+/- %0.2f)\" % (metrica, maxScore, maxStd))\n",
    "    \n",
    "    print('----')\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    y_true = y_test\n",
    "    classifiers[bClassifier[0]][2].fit(X_train, y_train)\n",
    "    y_pred = classifiers[bClassifier[0]][2].predict(X_test)\n",
    "    report2 = sklearn.metrics.classification_report(y_true, y_pred)\n",
    "    print(report2)\n",
    "    \n",
    "    \n",
    "    return bClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melhor Classificador Dental Implants (tweets pr√©-processados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3240\n"
     ]
    }
   ],
   "source": [
    "print(len(allTrials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 'LinearSVC [max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "          steps=[('vectorizer',\n",
       "                  TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.float64'>,\n",
       "                                  encoding='utf-8', input='content',\n",
       "                                  lowercase=True, max_df=1.0, max_features=None,\n",
       "                                  min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                  preprocessor=None, smooth_idf=True,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  sublinear_tf=False,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, use_idf=True,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                            fit_intercept=True, intercept_scaling=1,\n",
       "                            loss='squared_hinge', max_iter=1000,\n",
       "                            multi_class='ovr', penalty='l2', random_state=None,\n",
       "                            tol=0.0001, verbose=0))],\n",
       "          verbose=False))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrials[111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC [max_df:0.9 min_df: 1] [ 40 ]\n",
      "max:  0.5289331688157526\n",
      "f1_weighted: 0.53 (+/- 0.06)\n",
      "----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    clinical       0.39      0.43      0.41        53\n",
      " information       0.46      0.44      0.45        25\n",
      "    personal       0.55      0.52      0.53        44\n",
      "     product       0.19      0.20      0.19        30\n",
      "        sale       0.40      0.21      0.28        19\n",
      "     service       0.63      0.65      0.64       129\n",
      "\n",
      "    accuracy                           0.50       300\n",
      "   macro avg       0.44      0.41      0.42       300\n",
      "weighted avg       0.50      0.50      0.50       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = bestClassifier(tweetsPP, topics, allTrials, 'f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC [max_df:0.9 min_df: 1] [ 40 ]\n",
      "max:  0.5360868507294835\n",
      "f1_weighted: 0.54 (+/- 0.07)\n",
      "----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    clinical       0.40      0.53      0.46        53\n",
      " information       0.46      0.48      0.47        25\n",
      "    personal       0.67      0.50      0.57        44\n",
      "     product       0.25      0.20      0.22        30\n",
      "        sale       0.33      0.21      0.26        19\n",
      "     service       0.64      0.67      0.66       129\n",
      "\n",
      "    accuracy                           0.53       300\n",
      "   macro avg       0.46      0.43      0.44       300\n",
      "weighted avg       0.53      0.53      0.53       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = bestClassifier(tweets, topics, allTrials, 'f1_weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melhor Classificador Dental Implants (tweets pr√©-processados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC [max_df:1.0 min_df: 1] [ 50 ]\n",
      "max:  0.5445151515151515\n",
      "accuracy: 0.54 (+/- 0.06)\n",
      "----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    clinical       0.40      0.47      0.43        53\n",
      " information       0.42      0.44      0.43        25\n",
      "    personal       0.61      0.45      0.52        44\n",
      "     product       0.19      0.23      0.21        30\n",
      "        sale       0.25      0.16      0.19        19\n",
      "     service       0.63      0.64      0.64       129\n",
      "\n",
      "    accuracy                           0.50       300\n",
      "   macro avg       0.42      0.40      0.40       300\n",
      "weighted avg       0.50      0.50      0.50       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = bestClassifier(tweetsPP, topics, allTrials, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC [max_df:0.9 min_df: 1] [ 40 ]\n",
      "max:  0.5546363636363637\n",
      "accuracy: 0.55 (+/- 0.06)\n",
      "----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    clinical       0.40      0.53      0.46        53\n",
      " information       0.46      0.48      0.47        25\n",
      "    personal       0.67      0.50      0.57        44\n",
      "     product       0.25      0.20      0.22        30\n",
      "        sale       0.33      0.21      0.26        19\n",
      "     service       0.64      0.67      0.66       129\n",
      "\n",
      "    accuracy                           0.53       300\n",
      "   macro avg       0.46      0.43      0.44       300\n",
      "weighted avg       0.53      0.53      0.53       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = bestClassifier(tweets, topics, allTrials, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 'LinearSVC [max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "          steps=[('vectorizer',\n",
       "                  TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.float64'>,\n",
       "                                  encoding='utf-8', input='content',\n",
       "                                  lowercase=True, max_df=0.9, max_features=None,\n",
       "                                  min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                  preprocessor=None, smooth_idf=True,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  sublinear_tf=False,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, use_idf=True,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                            fit_intercept=True, intercept_scaling=1,\n",
       "                            loss='squared_hinge', max_iter=1000,\n",
       "                            multi_class='ovr', penalty='l2', random_state=None,\n",
       "                            tol=0.0001, verbose=0))],\n",
       "          verbose=False))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrials[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "(40, 'SGD [max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.9, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_patt...\n",
      "                ('classifier',\n",
      "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
      "                               fit_intercept=True, l1_ratio=0.15,\n",
      "                               learning_rate='optimal', loss='hinge',\n",
      "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
      "                               penalty='l2', power_t=0.5, random_state=10,\n",
      "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
      "                               verbose=0, warm_start=False))],\n",
      "         verbose=False))\n",
      "100\n",
      "(40, 'LinearSVC [max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.9, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('classifier',\n",
      "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
      "                           fit_intercept=True, intercept_scaling=1,\n",
      "                           loss='squared_hinge', max_iter=1000,\n",
      "                           multi_class='ovr', penalty='l2', random_state=None,\n",
      "                           tol=0.0001, verbose=0))],\n",
      "         verbose=False))\n",
      "160\n",
      "(40, 'SVC [max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.9, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('classifier',\n",
      "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "                     decision_function_shape='ovr', degree=3,\n",
      "                     gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
      "                     probability=False, random_state=None, shrinking=True,\n",
      "                     tol=0.001, verbose=False))],\n",
      "         verbose=False))\n",
      "220\n",
      "(40, 'MNB [alpha: 0.0 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.9, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('classifier',\n",
      "                 MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
      "         verbose=False))\n",
      "880\n",
      "(40, 'RF [val: 1.0 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.9, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_patt...\n",
      "                ('classifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=None,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=1, min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=1, n_jobs=None,\n",
      "                                        oob_score=False, random_state=10,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False))\n"
     ]
    }
   ],
   "source": [
    "for idx, t in enumerate('allTrials'):\n",
    "    if (t[0] == 40):\n",
    "        print(idx)\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'SGD [max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (1, 'SGD [max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (2, 'SGD [max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (3, 'SGD [max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (4, 'SGD [max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (5, 'SGD [max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (6, 'SGD [max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (7, 'SGD [max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (8, 'SGD [max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (9, 'SGD [max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (10, 'SGD [max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (11, 'SGD [max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (12, 'SGD [max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (13, 'SGD [max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (14, 'SGD [max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (15, 'SGD [max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (16, 'SGD [max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (17, 'SGD [max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (18, 'SGD [max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (19, 'SGD [max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (20, 'SGD [max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (21, 'SGD [max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (22, 'SGD [max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (23, 'SGD [max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (24, 'SGD [max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (25, 'SGD [max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (26, 'SGD [max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (27, 'SGD [max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (28, 'SGD [max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (29, 'SGD [max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (30, 'SGD [max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (31, 'SGD [max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (32, 'SGD [max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (33, 'SGD [max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (34, 'SGD [max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (35, 'SGD [max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (36, 'SGD [max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (37, 'SGD [max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (38, 'SGD [max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (39, 'SGD [max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (40, 'SGD [max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (41, 'SGD [max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (42, 'SGD [max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (43, 'SGD [max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (44, 'SGD [max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (45, 'SGD [max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (46, 'SGD [max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (47, 'SGD [max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (48, 'SGD [max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (49, 'SGD [max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (50, 'SGD [max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (51, 'SGD [max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (52, 'SGD [max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (53, 'SGD [max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (54, 'SGD [max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (55, 'SGD [max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (56, 'SGD [max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (57, 'SGD [max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (58, 'SGD [max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (59, 'SGD [max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                                 early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                                 fit_intercept=True, l1_ratio=0.15,\n",
       "                                 learning_rate='optimal', loss='hinge',\n",
       "                                 max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                                 penalty='l2', power_t=0.5, random_state=10,\n",
       "                                 shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                                 verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (0, 'LinearSVC [max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (1, 'LinearSVC [max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (2, 'LinearSVC [max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (3, 'LinearSVC [max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (4, 'LinearSVC [max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (5, 'LinearSVC [max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (6, 'LinearSVC [max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (7, 'LinearSVC [max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (8, 'LinearSVC [max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (9, 'LinearSVC [max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (10, 'LinearSVC [max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (11, 'LinearSVC [max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (12, 'LinearSVC [max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (13, 'LinearSVC [max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (14, 'LinearSVC [max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (15, 'LinearSVC [max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (16, 'LinearSVC [max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (17, 'LinearSVC [max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (18, 'LinearSVC [max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (19, 'LinearSVC [max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (20, 'LinearSVC [max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (21, 'LinearSVC [max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (22, 'LinearSVC [max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (23, 'LinearSVC [max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (24, 'LinearSVC [max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (25, 'LinearSVC [max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (26, 'LinearSVC [max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (27, 'LinearSVC [max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (28, 'LinearSVC [max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (29, 'LinearSVC [max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (30, 'LinearSVC [max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (31, 'LinearSVC [max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (32, 'LinearSVC [max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (33, 'LinearSVC [max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (34, 'LinearSVC [max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (35, 'LinearSVC [max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (36, 'LinearSVC [max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (37, 'LinearSVC [max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (38, 'LinearSVC [max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (39, 'LinearSVC [max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (40, 'LinearSVC [max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (41, 'LinearSVC [max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (42, 'LinearSVC [max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (43, 'LinearSVC [max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (44, 'LinearSVC [max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (45, 'LinearSVC [max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (46, 'LinearSVC [max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (47, 'LinearSVC [max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (48, 'LinearSVC [max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (49, 'LinearSVC [max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (50, 'LinearSVC [max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (51, 'LinearSVC [max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (52, 'LinearSVC [max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (53, 'LinearSVC [max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (54, 'LinearSVC [max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (55, 'LinearSVC [max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (56, 'LinearSVC [max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (57, 'LinearSVC [max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (58, 'LinearSVC [max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (59, 'LinearSVC [max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                             fit_intercept=True, intercept_scaling=1,\n",
       "                             loss='squared_hinge', max_iter=1000,\n",
       "                             multi_class='ovr', penalty='l2', random_state=None,\n",
       "                             tol=0.0001, verbose=0))],\n",
       "           verbose=False)),\n",
       " (0, 'SVC [max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (1, 'SVC [max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (2, 'SVC [max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (3, 'SVC [max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (4, 'SVC [max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (5, 'SVC [max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (6, 'SVC [max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (7, 'SVC [max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (8, 'SVC [max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (9, 'SVC [max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (10, 'SVC [max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (11, 'SVC [max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (12, 'SVC [max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (13, 'SVC [max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (14, 'SVC [max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (15, 'SVC [max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (16, 'SVC [max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (17, 'SVC [max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (18, 'SVC [max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (19, 'SVC [max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (20, 'SVC [max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (21, 'SVC [max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (22, 'SVC [max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (23, 'SVC [max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (24, 'SVC [max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (25, 'SVC [max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (26, 'SVC [max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (27, 'SVC [max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (28, 'SVC [max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (29, 'SVC [max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (30, 'SVC [max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (31, 'SVC [max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (32, 'SVC [max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (33, 'SVC [max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (34, 'SVC [max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (35, 'SVC [max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (36, 'SVC [max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (37, 'SVC [max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (38, 'SVC [max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (39, 'SVC [max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (40, 'SVC [max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (41, 'SVC [max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (42, 'SVC [max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (43, 'SVC [max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (44, 'SVC [max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (45, 'SVC [max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (46, 'SVC [max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (47, 'SVC [max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (48, 'SVC [max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (49, 'SVC [max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (50, 'SVC [max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (51, 'SVC [max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (52, 'SVC [max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (53, 'SVC [max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (54, 'SVC [max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (55, 'SVC [max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (56, 'SVC [max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (57, 'SVC [max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (58, 'SVC [max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (59, 'SVC [max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                       decision_function_shape='ovr', degree=3,\n",
       "                       gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                       probability=False, random_state=None, shrinking=True,\n",
       "                       tol=0.001, verbose=False))],\n",
       "           verbose=False)),\n",
       " (0, 'MNB [alpha: 0.0 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (1, 'MNB [alpha: 0.0 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (2, 'MNB [alpha: 0.0 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (3, 'MNB [alpha: 0.0 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (4, 'MNB [alpha: 0.0 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (5, 'MNB [alpha: 0.0 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (6, 'MNB [alpha: 0.0 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (7, 'MNB [alpha: 0.0 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (8, 'MNB [alpha: 0.0 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (9, 'MNB [alpha: 0.0 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (10, 'MNB [alpha: 0.0 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (11, 'MNB [alpha: 0.0 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (12, 'MNB [alpha: 0.0 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (13, 'MNB [alpha: 0.0 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (14, 'MNB [alpha: 0.0 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (15, 'MNB [alpha: 0.0 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (16, 'MNB [alpha: 0.0 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (17, 'MNB [alpha: 0.0 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (18, 'MNB [alpha: 0.0 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (19, 'MNB [alpha: 0.0 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (20, 'MNB [alpha: 0.0 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (21, 'MNB [alpha: 0.0 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (22, 'MNB [alpha: 0.0 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (23, 'MNB [alpha: 0.0 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (24, 'MNB [alpha: 0.0 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (25, 'MNB [alpha: 0.0 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (26, 'MNB [alpha: 0.0 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (27, 'MNB [alpha: 0.0 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (28, 'MNB [alpha: 0.0 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (29, 'MNB [alpha: 0.0 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (30, 'MNB [alpha: 0.0 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (31, 'MNB [alpha: 0.0 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (32, 'MNB [alpha: 0.0 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (33, 'MNB [alpha: 0.0 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (34, 'MNB [alpha: 0.0 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (35, 'MNB [alpha: 0.0 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (36, 'MNB [alpha: 0.0 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (37, 'MNB [alpha: 0.0 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (38, 'MNB [alpha: 0.0 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (39, 'MNB [alpha: 0.0 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (40, 'MNB [alpha: 0.0 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (41, 'MNB [alpha: 0.0 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (42, 'MNB [alpha: 0.0 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (43, 'MNB [alpha: 0.0 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (44, 'MNB [alpha: 0.0 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (45, 'MNB [alpha: 0.0 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (46, 'MNB [alpha: 0.0 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (47, 'MNB [alpha: 0.0 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (48, 'MNB [alpha: 0.0 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (49, 'MNB [alpha: 0.0 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (50, 'MNB [alpha: 0.0 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (51, 'MNB [alpha: 0.0 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (52, 'MNB [alpha: 0.0 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (53, 'MNB [alpha: 0.0 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (54, 'MNB [alpha: 0.0 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (55, 'MNB [alpha: 0.0 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (56, 'MNB [alpha: 0.0 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (57, 'MNB [alpha: 0.0 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (58, 'MNB [alpha: 0.0 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (59, 'MNB [alpha: 0.0 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (60, 'MNB [alpha: 0.1 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (61, 'MNB [alpha: 0.1 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (62, 'MNB [alpha: 0.1 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (63, 'MNB [alpha: 0.1 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (64, 'MNB [alpha: 0.1 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (65, 'MNB [alpha: 0.1 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (66, 'MNB [alpha: 0.1 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (67, 'MNB [alpha: 0.1 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (68, 'MNB [alpha: 0.1 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (69, 'MNB [alpha: 0.1 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (70, 'MNB [alpha: 0.1 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (71, 'MNB [alpha: 0.1 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (72, 'MNB [alpha: 0.1 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (73, 'MNB [alpha: 0.1 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (74, 'MNB [alpha: 0.1 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (75, 'MNB [alpha: 0.1 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (76, 'MNB [alpha: 0.1 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (77, 'MNB [alpha: 0.1 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (78, 'MNB [alpha: 0.1 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (79, 'MNB [alpha: 0.1 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (80, 'MNB [alpha: 0.1 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (81, 'MNB [alpha: 0.1 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (82, 'MNB [alpha: 0.1 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (83, 'MNB [alpha: 0.1 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (84, 'MNB [alpha: 0.1 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (85, 'MNB [alpha: 0.1 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (86, 'MNB [alpha: 0.1 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (87, 'MNB [alpha: 0.1 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (88, 'MNB [alpha: 0.1 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (89, 'MNB [alpha: 0.1 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (90, 'MNB [alpha: 0.1 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (91, 'MNB [alpha: 0.1 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (92, 'MNB [alpha: 0.1 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (93, 'MNB [alpha: 0.1 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (94, 'MNB [alpha: 0.1 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (95, 'MNB [alpha: 0.1 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (96, 'MNB [alpha: 0.1 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (97, 'MNB [alpha: 0.1 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (98, 'MNB [alpha: 0.1 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (99, 'MNB [alpha: 0.1 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (100, 'MNB [alpha: 0.1 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (101, 'MNB [alpha: 0.1 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (102, 'MNB [alpha: 0.1 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (103, 'MNB [alpha: 0.1 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (104, 'MNB [alpha: 0.1 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (105, 'MNB [alpha: 0.1 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (106, 'MNB [alpha: 0.1 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (107, 'MNB [alpha: 0.1 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (108, 'MNB [alpha: 0.1 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (109, 'MNB [alpha: 0.1 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (110, 'MNB [alpha: 0.1 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (111, 'MNB [alpha: 0.1 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (112, 'MNB [alpha: 0.1 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (113, 'MNB [alpha: 0.1 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (114, 'MNB [alpha: 0.1 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (115, 'MNB [alpha: 0.1 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (116, 'MNB [alpha: 0.1 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (117, 'MNB [alpha: 0.1 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (118, 'MNB [alpha: 0.1 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (119, 'MNB [alpha: 0.1 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (120, 'MNB [alpha: 0.2 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (121, 'MNB [alpha: 0.2 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (122, 'MNB [alpha: 0.2 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (123, 'MNB [alpha: 0.2 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (124, 'MNB [alpha: 0.2 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (125, 'MNB [alpha: 0.2 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (126, 'MNB [alpha: 0.2 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (127, 'MNB [alpha: 0.2 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (128, 'MNB [alpha: 0.2 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (129, 'MNB [alpha: 0.2 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (130, 'MNB [alpha: 0.2 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (131, 'MNB [alpha: 0.2 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (132, 'MNB [alpha: 0.2 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (133, 'MNB [alpha: 0.2 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (134, 'MNB [alpha: 0.2 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (135, 'MNB [alpha: 0.2 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (136, 'MNB [alpha: 0.2 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (137, 'MNB [alpha: 0.2 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (138, 'MNB [alpha: 0.2 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (139, 'MNB [alpha: 0.2 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (140, 'MNB [alpha: 0.2 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (141, 'MNB [alpha: 0.2 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (142, 'MNB [alpha: 0.2 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (143, 'MNB [alpha: 0.2 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (144, 'MNB [alpha: 0.2 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (145, 'MNB [alpha: 0.2 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (146, 'MNB [alpha: 0.2 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (147, 'MNB [alpha: 0.2 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (148, 'MNB [alpha: 0.2 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (149, 'MNB [alpha: 0.2 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (150, 'MNB [alpha: 0.2 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (151, 'MNB [alpha: 0.2 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (152, 'MNB [alpha: 0.2 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (153, 'MNB [alpha: 0.2 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (154, 'MNB [alpha: 0.2 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (155, 'MNB [alpha: 0.2 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (156, 'MNB [alpha: 0.2 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (157, 'MNB [alpha: 0.2 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (158, 'MNB [alpha: 0.2 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (159, 'MNB [alpha: 0.2 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (160, 'MNB [alpha: 0.2 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (161, 'MNB [alpha: 0.2 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (162, 'MNB [alpha: 0.2 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (163, 'MNB [alpha: 0.2 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (164, 'MNB [alpha: 0.2 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (165, 'MNB [alpha: 0.2 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (166, 'MNB [alpha: 0.2 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (167, 'MNB [alpha: 0.2 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (168, 'MNB [alpha: 0.2 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (169, 'MNB [alpha: 0.2 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (170, 'MNB [alpha: 0.2 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (171, 'MNB [alpha: 0.2 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (172, 'MNB [alpha: 0.2 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (173, 'MNB [alpha: 0.2 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (174, 'MNB [alpha: 0.2 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (175, 'MNB [alpha: 0.2 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (176, 'MNB [alpha: 0.2 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (177, 'MNB [alpha: 0.2 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (178, 'MNB [alpha: 0.2 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (179, 'MNB [alpha: 0.2 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (180, 'MNB [alpha: 0.3 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (181, 'MNB [alpha: 0.3 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (182, 'MNB [alpha: 0.3 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (183, 'MNB [alpha: 0.3 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (184, 'MNB [alpha: 0.3 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (185, 'MNB [alpha: 0.3 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (186, 'MNB [alpha: 0.3 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (187, 'MNB [alpha: 0.3 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (188, 'MNB [alpha: 0.3 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (189, 'MNB [alpha: 0.3 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (190, 'MNB [alpha: 0.3 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (191, 'MNB [alpha: 0.3 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (192, 'MNB [alpha: 0.3 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (193, 'MNB [alpha: 0.3 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (194, 'MNB [alpha: 0.3 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (195, 'MNB [alpha: 0.3 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (196, 'MNB [alpha: 0.3 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (197, 'MNB [alpha: 0.3 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (198, 'MNB [alpha: 0.3 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (199, 'MNB [alpha: 0.3 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (200, 'MNB [alpha: 0.3 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (201, 'MNB [alpha: 0.3 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (202, 'MNB [alpha: 0.3 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (203, 'MNB [alpha: 0.3 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (204, 'MNB [alpha: 0.3 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (205, 'MNB [alpha: 0.3 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (206, 'MNB [alpha: 0.3 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (207, 'MNB [alpha: 0.3 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (208, 'MNB [alpha: 0.3 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (209, 'MNB [alpha: 0.3 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (210, 'MNB [alpha: 0.3 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (211, 'MNB [alpha: 0.3 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (212, 'MNB [alpha: 0.3 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (213, 'MNB [alpha: 0.3 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (214, 'MNB [alpha: 0.3 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (215, 'MNB [alpha: 0.3 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (216, 'MNB [alpha: 0.3 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (217, 'MNB [alpha: 0.3 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (218, 'MNB [alpha: 0.3 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (219, 'MNB [alpha: 0.3 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (220, 'MNB [alpha: 0.3 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (221, 'MNB [alpha: 0.3 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (222, 'MNB [alpha: 0.3 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (223, 'MNB [alpha: 0.3 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (224, 'MNB [alpha: 0.3 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (225, 'MNB [alpha: 0.3 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (226, 'MNB [alpha: 0.3 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (227, 'MNB [alpha: 0.3 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (228, 'MNB [alpha: 0.3 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (229, 'MNB [alpha: 0.3 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (230, 'MNB [alpha: 0.3 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (231, 'MNB [alpha: 0.3 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (232, 'MNB [alpha: 0.3 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (233, 'MNB [alpha: 0.3 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (234, 'MNB [alpha: 0.3 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (235, 'MNB [alpha: 0.3 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (236, 'MNB [alpha: 0.3 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (237, 'MNB [alpha: 0.3 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (238, 'MNB [alpha: 0.3 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (239, 'MNB [alpha: 0.3 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (240, 'MNB [alpha: 0.4 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (241, 'MNB [alpha: 0.4 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (242, 'MNB [alpha: 0.4 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (243, 'MNB [alpha: 0.4 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (244, 'MNB [alpha: 0.4 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (245, 'MNB [alpha: 0.4 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (246, 'MNB [alpha: 0.4 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (247, 'MNB [alpha: 0.4 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (248, 'MNB [alpha: 0.4 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (249, 'MNB [alpha: 0.4 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (250, 'MNB [alpha: 0.4 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (251, 'MNB [alpha: 0.4 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (252, 'MNB [alpha: 0.4 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (253, 'MNB [alpha: 0.4 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (254, 'MNB [alpha: 0.4 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (255, 'MNB [alpha: 0.4 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (256, 'MNB [alpha: 0.4 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (257, 'MNB [alpha: 0.4 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (258, 'MNB [alpha: 0.4 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (259, 'MNB [alpha: 0.4 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (260, 'MNB [alpha: 0.4 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (261, 'MNB [alpha: 0.4 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (262, 'MNB [alpha: 0.4 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (263, 'MNB [alpha: 0.4 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (264, 'MNB [alpha: 0.4 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (265, 'MNB [alpha: 0.4 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (266, 'MNB [alpha: 0.4 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (267, 'MNB [alpha: 0.4 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (268, 'MNB [alpha: 0.4 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (269, 'MNB [alpha: 0.4 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (270, 'MNB [alpha: 0.4 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (271, 'MNB [alpha: 0.4 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (272, 'MNB [alpha: 0.4 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (273, 'MNB [alpha: 0.4 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (274, 'MNB [alpha: 0.4 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (275, 'MNB [alpha: 0.4 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (276, 'MNB [alpha: 0.4 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (277, 'MNB [alpha: 0.4 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (278, 'MNB [alpha: 0.4 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (279, 'MNB [alpha: 0.4 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (280, 'MNB [alpha: 0.4 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (281, 'MNB [alpha: 0.4 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (282, 'MNB [alpha: 0.4 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (283, 'MNB [alpha: 0.4 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (284, 'MNB [alpha: 0.4 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (285, 'MNB [alpha: 0.4 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (286, 'MNB [alpha: 0.4 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (287, 'MNB [alpha: 0.4 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (288, 'MNB [alpha: 0.4 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (289, 'MNB [alpha: 0.4 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (290, 'MNB [alpha: 0.4 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (291, 'MNB [alpha: 0.4 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (292, 'MNB [alpha: 0.4 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (293, 'MNB [alpha: 0.4 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (294, 'MNB [alpha: 0.4 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (295, 'MNB [alpha: 0.4 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (296, 'MNB [alpha: 0.4 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (297, 'MNB [alpha: 0.4 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (298, 'MNB [alpha: 0.4 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (299, 'MNB [alpha: 0.4 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (300, 'MNB [alpha: 0.5 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (301, 'MNB [alpha: 0.5 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (302, 'MNB [alpha: 0.5 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (303, 'MNB [alpha: 0.5 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (304, 'MNB [alpha: 0.5 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (305, 'MNB [alpha: 0.5 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (306, 'MNB [alpha: 0.5 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (307, 'MNB [alpha: 0.5 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (308, 'MNB [alpha: 0.5 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (309, 'MNB [alpha: 0.5 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (310, 'MNB [alpha: 0.5 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (311, 'MNB [alpha: 0.5 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (312, 'MNB [alpha: 0.5 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (313, 'MNB [alpha: 0.5 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (314, 'MNB [alpha: 0.5 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (315, 'MNB [alpha: 0.5 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (316, 'MNB [alpha: 0.5 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (317, 'MNB [alpha: 0.5 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (318, 'MNB [alpha: 0.5 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (319, 'MNB [alpha: 0.5 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (320, 'MNB [alpha: 0.5 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (321, 'MNB [alpha: 0.5 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (322, 'MNB [alpha: 0.5 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (323, 'MNB [alpha: 0.5 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (324, 'MNB [alpha: 0.5 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (325, 'MNB [alpha: 0.5 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (326, 'MNB [alpha: 0.5 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (327, 'MNB [alpha: 0.5 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (328, 'MNB [alpha: 0.5 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (329, 'MNB [alpha: 0.5 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (330, 'MNB [alpha: 0.5 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (331, 'MNB [alpha: 0.5 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (332, 'MNB [alpha: 0.5 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (333, 'MNB [alpha: 0.5 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (334, 'MNB [alpha: 0.5 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (335, 'MNB [alpha: 0.5 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (336, 'MNB [alpha: 0.5 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (337, 'MNB [alpha: 0.5 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (338, 'MNB [alpha: 0.5 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (339, 'MNB [alpha: 0.5 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (340, 'MNB [alpha: 0.5 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (341, 'MNB [alpha: 0.5 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (342, 'MNB [alpha: 0.5 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (343, 'MNB [alpha: 0.5 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (344, 'MNB [alpha: 0.5 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (345, 'MNB [alpha: 0.5 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (346, 'MNB [alpha: 0.5 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (347, 'MNB [alpha: 0.5 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (348, 'MNB [alpha: 0.5 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (349, 'MNB [alpha: 0.5 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (350, 'MNB [alpha: 0.5 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (351, 'MNB [alpha: 0.5 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (352, 'MNB [alpha: 0.5 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (353, 'MNB [alpha: 0.5 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (354, 'MNB [alpha: 0.5 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (355, 'MNB [alpha: 0.5 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (356, 'MNB [alpha: 0.5 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (357, 'MNB [alpha: 0.5 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (358, 'MNB [alpha: 0.5 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (359, 'MNB [alpha: 0.5 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (360, 'MNB [alpha: 0.6 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (361, 'MNB [alpha: 0.6 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (362, 'MNB [alpha: 0.6 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (363, 'MNB [alpha: 0.6 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (364, 'MNB [alpha: 0.6 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (365, 'MNB [alpha: 0.6 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (366, 'MNB [alpha: 0.6 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (367, 'MNB [alpha: 0.6 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (368, 'MNB [alpha: 0.6 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (369, 'MNB [alpha: 0.6 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (370, 'MNB [alpha: 0.6 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (371, 'MNB [alpha: 0.6 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (372, 'MNB [alpha: 0.6 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (373, 'MNB [alpha: 0.6 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (374, 'MNB [alpha: 0.6 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (375, 'MNB [alpha: 0.6 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (376, 'MNB [alpha: 0.6 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (377, 'MNB [alpha: 0.6 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (378, 'MNB [alpha: 0.6 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (379, 'MNB [alpha: 0.6 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (380, 'MNB [alpha: 0.6 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (381, 'MNB [alpha: 0.6 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (382, 'MNB [alpha: 0.6 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (383, 'MNB [alpha: 0.6 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (384, 'MNB [alpha: 0.6 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (385, 'MNB [alpha: 0.6 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (386, 'MNB [alpha: 0.6 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (387, 'MNB [alpha: 0.6 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (388, 'MNB [alpha: 0.6 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (389, 'MNB [alpha: 0.6 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (390, 'MNB [alpha: 0.6 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (391, 'MNB [alpha: 0.6 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (392, 'MNB [alpha: 0.6 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (393, 'MNB [alpha: 0.6 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (394, 'MNB [alpha: 0.6 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (395, 'MNB [alpha: 0.6 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (396, 'MNB [alpha: 0.6 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (397, 'MNB [alpha: 0.6 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (398, 'MNB [alpha: 0.6 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (399, 'MNB [alpha: 0.6 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (400, 'MNB [alpha: 0.6 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (401, 'MNB [alpha: 0.6 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (402, 'MNB [alpha: 0.6 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (403, 'MNB [alpha: 0.6 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (404, 'MNB [alpha: 0.6 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (405, 'MNB [alpha: 0.6 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (406, 'MNB [alpha: 0.6 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (407, 'MNB [alpha: 0.6 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (408, 'MNB [alpha: 0.6 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (409, 'MNB [alpha: 0.6 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (410, 'MNB [alpha: 0.6 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (411, 'MNB [alpha: 0.6 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (412, 'MNB [alpha: 0.6 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (413, 'MNB [alpha: 0.6 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (414, 'MNB [alpha: 0.6 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (415, 'MNB [alpha: 0.6 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (416, 'MNB [alpha: 0.6 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (417, 'MNB [alpha: 0.6 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (418, 'MNB [alpha: 0.6 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (419, 'MNB [alpha: 0.6 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (420, 'MNB [alpha: 0.7 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (421, 'MNB [alpha: 0.7 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (422, 'MNB [alpha: 0.7 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (423, 'MNB [alpha: 0.7 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (424, 'MNB [alpha: 0.7 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (425, 'MNB [alpha: 0.7 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (426, 'MNB [alpha: 0.7 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (427, 'MNB [alpha: 0.7 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (428, 'MNB [alpha: 0.7 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (429, 'MNB [alpha: 0.7 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (430, 'MNB [alpha: 0.7 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (431, 'MNB [alpha: 0.7 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (432, 'MNB [alpha: 0.7 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (433, 'MNB [alpha: 0.7 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (434, 'MNB [alpha: 0.7 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (435, 'MNB [alpha: 0.7 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (436, 'MNB [alpha: 0.7 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (437, 'MNB [alpha: 0.7 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (438, 'MNB [alpha: 0.7 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (439, 'MNB [alpha: 0.7 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (440, 'MNB [alpha: 0.7 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (441, 'MNB [alpha: 0.7 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (442, 'MNB [alpha: 0.7 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (443, 'MNB [alpha: 0.7 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (444, 'MNB [alpha: 0.7 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (445, 'MNB [alpha: 0.7 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (446, 'MNB [alpha: 0.7 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (447, 'MNB [alpha: 0.7 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (448, 'MNB [alpha: 0.7 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (449, 'MNB [alpha: 0.7 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (450, 'MNB [alpha: 0.7 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (451, 'MNB [alpha: 0.7 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (452, 'MNB [alpha: 0.7 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (453, 'MNB [alpha: 0.7 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (454, 'MNB [alpha: 0.7 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (455, 'MNB [alpha: 0.7 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (456, 'MNB [alpha: 0.7 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (457, 'MNB [alpha: 0.7 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (458, 'MNB [alpha: 0.7 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (459, 'MNB [alpha: 0.7 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (460, 'MNB [alpha: 0.7 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (461, 'MNB [alpha: 0.7 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (462, 'MNB [alpha: 0.7 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (463, 'MNB [alpha: 0.7 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (464, 'MNB [alpha: 0.7 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (465, 'MNB [alpha: 0.7 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (466, 'MNB [alpha: 0.7 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (467, 'MNB [alpha: 0.7 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (468, 'MNB [alpha: 0.7 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (469, 'MNB [alpha: 0.7 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (470, 'MNB [alpha: 0.7 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (471, 'MNB [alpha: 0.7 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (472, 'MNB [alpha: 0.7 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (473, 'MNB [alpha: 0.7 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (474, 'MNB [alpha: 0.7 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (475, 'MNB [alpha: 0.7 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (476, 'MNB [alpha: 0.7 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (477, 'MNB [alpha: 0.7 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (478, 'MNB [alpha: 0.7 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (479, 'MNB [alpha: 0.7 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.7, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (480, 'MNB [alpha: 0.8 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (481, 'MNB [alpha: 0.8 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (482, 'MNB [alpha: 0.8 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (483, 'MNB [alpha: 0.8 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (484, 'MNB [alpha: 0.8 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (485, 'MNB [alpha: 0.8 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (486, 'MNB [alpha: 0.8 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (487, 'MNB [alpha: 0.8 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (488, 'MNB [alpha: 0.8 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (489, 'MNB [alpha: 0.8 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (490, 'MNB [alpha: 0.8 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (491, 'MNB [alpha: 0.8 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (492, 'MNB [alpha: 0.8 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (493, 'MNB [alpha: 0.8 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (494, 'MNB [alpha: 0.8 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (495, 'MNB [alpha: 0.8 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (496, 'MNB [alpha: 0.8 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (497, 'MNB [alpha: 0.8 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (498, 'MNB [alpha: 0.8 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (499, 'MNB [alpha: 0.8 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (500, 'MNB [alpha: 0.8 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (501, 'MNB [alpha: 0.8 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (502, 'MNB [alpha: 0.8 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (503, 'MNB [alpha: 0.8 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (504, 'MNB [alpha: 0.8 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (505, 'MNB [alpha: 0.8 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (506, 'MNB [alpha: 0.8 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (507, 'MNB [alpha: 0.8 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (508, 'MNB [alpha: 0.8 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (509, 'MNB [alpha: 0.8 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (510, 'MNB [alpha: 0.8 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (511, 'MNB [alpha: 0.8 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (512, 'MNB [alpha: 0.8 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (513, 'MNB [alpha: 0.8 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (514, 'MNB [alpha: 0.8 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (515, 'MNB [alpha: 0.8 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (516, 'MNB [alpha: 0.8 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (517, 'MNB [alpha: 0.8 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (518, 'MNB [alpha: 0.8 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (519, 'MNB [alpha: 0.8 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (520, 'MNB [alpha: 0.8 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (521, 'MNB [alpha: 0.8 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (522, 'MNB [alpha: 0.8 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (523, 'MNB [alpha: 0.8 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (524, 'MNB [alpha: 0.8 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (525, 'MNB [alpha: 0.8 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (526, 'MNB [alpha: 0.8 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (527, 'MNB [alpha: 0.8 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (528, 'MNB [alpha: 0.8 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (529, 'MNB [alpha: 0.8 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (530, 'MNB [alpha: 0.8 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (531, 'MNB [alpha: 0.8 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (532, 'MNB [alpha: 0.8 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (533, 'MNB [alpha: 0.8 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (534, 'MNB [alpha: 0.8 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (535, 'MNB [alpha: 0.8 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (536, 'MNB [alpha: 0.8 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (537, 'MNB [alpha: 0.8 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (538, 'MNB [alpha: 0.8 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (539, 'MNB [alpha: 0.8 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (540, 'MNB [alpha: 0.9 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (541, 'MNB [alpha: 0.9 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (542, 'MNB [alpha: 0.9 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (543, 'MNB [alpha: 0.9 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (544, 'MNB [alpha: 0.9 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (545, 'MNB [alpha: 0.9 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (546, 'MNB [alpha: 0.9 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (547, 'MNB [alpha: 0.9 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (548, 'MNB [alpha: 0.9 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (549, 'MNB [alpha: 0.9 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (550, 'MNB [alpha: 0.9 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (551, 'MNB [alpha: 0.9 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (552, 'MNB [alpha: 0.9 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (553, 'MNB [alpha: 0.9 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (554, 'MNB [alpha: 0.9 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (555, 'MNB [alpha: 0.9 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (556, 'MNB [alpha: 0.9 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (557, 'MNB [alpha: 0.9 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (558, 'MNB [alpha: 0.9 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (559, 'MNB [alpha: 0.9 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (560, 'MNB [alpha: 0.9 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (561, 'MNB [alpha: 0.9 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (562, 'MNB [alpha: 0.9 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (563, 'MNB [alpha: 0.9 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (564, 'MNB [alpha: 0.9 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (565, 'MNB [alpha: 0.9 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (566, 'MNB [alpha: 0.9 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (567, 'MNB [alpha: 0.9 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (568, 'MNB [alpha: 0.9 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (569, 'MNB [alpha: 0.9 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (570, 'MNB [alpha: 0.9 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (571, 'MNB [alpha: 0.9 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (572, 'MNB [alpha: 0.9 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (573, 'MNB [alpha: 0.9 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (574, 'MNB [alpha: 0.9 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (575, 'MNB [alpha: 0.9 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (576, 'MNB [alpha: 0.9 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (577, 'MNB [alpha: 0.9 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (578, 'MNB [alpha: 0.9 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (579, 'MNB [alpha: 0.9 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (580, 'MNB [alpha: 0.9 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (581, 'MNB [alpha: 0.9 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (582, 'MNB [alpha: 0.9 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (583, 'MNB [alpha: 0.9 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (584, 'MNB [alpha: 0.9 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (585, 'MNB [alpha: 0.9 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (586, 'MNB [alpha: 0.9 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (587, 'MNB [alpha: 0.9 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (588, 'MNB [alpha: 0.9 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (589, 'MNB [alpha: 0.9 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (590, 'MNB [alpha: 0.9 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (591, 'MNB [alpha: 0.9 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (592, 'MNB [alpha: 0.9 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (593, 'MNB [alpha: 0.9 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (594, 'MNB [alpha: 0.9 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (595, 'MNB [alpha: 0.9 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (596, 'MNB [alpha: 0.9 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (597, 'MNB [alpha: 0.9 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (598, 'MNB [alpha: 0.9 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (599, 'MNB [alpha: 0.9 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (600, 'MNB [alpha: 1.0 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (601, 'MNB [alpha: 1.0 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (602, 'MNB [alpha: 1.0 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (603, 'MNB [alpha: 1.0 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (604, 'MNB [alpha: 1.0 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (605, 'MNB [alpha: 1.0 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (606, 'MNB [alpha: 1.0 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (607, 'MNB [alpha: 1.0 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (608, 'MNB [alpha: 1.0 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (609, 'MNB [alpha: 1.0 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (610, 'MNB [alpha: 1.0 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (611, 'MNB [alpha: 1.0 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (612, 'MNB [alpha: 1.0 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (613, 'MNB [alpha: 1.0 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (614, 'MNB [alpha: 1.0 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (615, 'MNB [alpha: 1.0 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (616, 'MNB [alpha: 1.0 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (617, 'MNB [alpha: 1.0 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (618, 'MNB [alpha: 1.0 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (619, 'MNB [alpha: 1.0 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (620, 'MNB [alpha: 1.0 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (621, 'MNB [alpha: 1.0 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (622, 'MNB [alpha: 1.0 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (623, 'MNB [alpha: 1.0 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (624, 'MNB [alpha: 1.0 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (625, 'MNB [alpha: 1.0 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (626, 'MNB [alpha: 1.0 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (627, 'MNB [alpha: 1.0 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (628, 'MNB [alpha: 1.0 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (629, 'MNB [alpha: 1.0 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (630, 'MNB [alpha: 1.0 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (631, 'MNB [alpha: 1.0 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (632, 'MNB [alpha: 1.0 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (633, 'MNB [alpha: 1.0 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (634, 'MNB [alpha: 1.0 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (635, 'MNB [alpha: 1.0 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (636, 'MNB [alpha: 1.0 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (637, 'MNB [alpha: 1.0 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (638, 'MNB [alpha: 1.0 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (639, 'MNB [alpha: 1.0 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (640, 'MNB [alpha: 1.0 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (641, 'MNB [alpha: 1.0 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (642, 'MNB [alpha: 1.0 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (643, 'MNB [alpha: 1.0 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (644, 'MNB [alpha: 1.0 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (645, 'MNB [alpha: 1.0 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (646, 'MNB [alpha: 1.0 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (647, 'MNB [alpha: 1.0 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (648, 'MNB [alpha: 1.0 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (649, 'MNB [alpha: 1.0 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (650, 'MNB [alpha: 1.0 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (651, 'MNB [alpha: 1.0 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (652, 'MNB [alpha: 1.0 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (653, 'MNB [alpha: 1.0 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (654, 'MNB [alpha: 1.0 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (655, 'MNB [alpha: 1.0 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (656, 'MNB [alpha: 1.0 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (657, 'MNB [alpha: 1.0 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (658, 'MNB [alpha: 1.0 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (659, 'MNB [alpha: 1.0 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, use_idf=True,\n",
       "                                   vocabulary=None)),\n",
       "                  ('classifier',\n",
       "                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "           verbose=False)),\n",
       " (0, 'RF [val: 1.0 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (1, 'RF [val: 1.0 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (2, 'RF [val: 1.0 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (3, 'RF [val: 1.0 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (4, 'RF [val: 1.0 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (5, 'RF [val: 1.0 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (6, 'RF [val: 1.0 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (7, 'RF [val: 1.0 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (8, 'RF [val: 1.0 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (9, 'RF [val: 1.0 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (10, 'RF [val: 1.0 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (11, 'RF [val: 1.0 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (12, 'RF [val: 1.0 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (13, 'RF [val: 1.0 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (14, 'RF [val: 1.0 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (15, 'RF [val: 1.0 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (16, 'RF [val: 1.0 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (17, 'RF [val: 1.0 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (18, 'RF [val: 1.0 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (19, 'RF [val: 1.0 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (20, 'RF [val: 1.0 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (21, 'RF [val: 1.0 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (22, 'RF [val: 1.0 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (23, 'RF [val: 1.0 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (24, 'RF [val: 1.0 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (25, 'RF [val: 1.0 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (26, 'RF [val: 1.0 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (27, 'RF [val: 1.0 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (28, 'RF [val: 1.0 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (29, 'RF [val: 1.0 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (30, 'RF [val: 1.0 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (31, 'RF [val: 1.0 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (32, 'RF [val: 1.0 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (33, 'RF [val: 1.0 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (34, 'RF [val: 1.0 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (35, 'RF [val: 1.0 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (36, 'RF [val: 1.0 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (37, 'RF [val: 1.0 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (38, 'RF [val: 1.0 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (39, 'RF [val: 1.0 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (40, 'RF [val: 1.0 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (41, 'RF [val: 1.0 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (42, 'RF [val: 1.0 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (43, 'RF [val: 1.0 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (44, 'RF [val: 1.0 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (45, 'RF [val: 1.0 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (46, 'RF [val: 1.0 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (47, 'RF [val: 1.0 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (48, 'RF [val: 1.0 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (49, 'RF [val: 1.0 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (50, 'RF [val: 1.0 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (51, 'RF [val: 1.0 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (52, 'RF [val: 1.0 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (53, 'RF [val: 1.0 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (54, 'RF [val: 1.0 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (55, 'RF [val: 1.0 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (56, 'RF [val: 1.0 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (57, 'RF [val: 1.0 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (58, 'RF [val: 1.0 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (59, 'RF [val: 1.0 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=1, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (60, 'RF [val: 1.0 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (61, 'RF [val: 1.0 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (62, 'RF [val: 1.0 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (63, 'RF [val: 1.0 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (64, 'RF [val: 1.0 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (65, 'RF [val: 1.0 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (66, 'RF [val: 1.0 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (67, 'RF [val: 1.0 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (68, 'RF [val: 1.0 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (69, 'RF [val: 1.0 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (70, 'RF [val: 1.0 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (71, 'RF [val: 1.0 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (72, 'RF [val: 1.0 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (73, 'RF [val: 1.0 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (74, 'RF [val: 1.0 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (75, 'RF [val: 1.0 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (76, 'RF [val: 1.0 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (77, 'RF [val: 1.0 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (78, 'RF [val: 1.0 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (79, 'RF [val: 1.0 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (80, 'RF [val: 1.0 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (81, 'RF [val: 1.0 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (82, 'RF [val: 1.0 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (83, 'RF [val: 1.0 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (84, 'RF [val: 1.0 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (85, 'RF [val: 1.0 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (86, 'RF [val: 1.0 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (87, 'RF [val: 1.0 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (88, 'RF [val: 1.0 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (89, 'RF [val: 1.0 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (90, 'RF [val: 1.0 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (91, 'RF [val: 1.0 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (92, 'RF [val: 1.0 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (93, 'RF [val: 1.0 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (94, 'RF [val: 1.0 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (95, 'RF [val: 1.0 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (96, 'RF [val: 1.0 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (97, 'RF [val: 1.0 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (98, 'RF [val: 1.0 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (99, 'RF [val: 1.0 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (100, 'RF [val: 1.0 max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (101, 'RF [val: 1.0 max_df:0.9 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (102, 'RF [val: 1.0 max_df:0.9 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (103, 'RF [val: 1.0 max_df:0.9 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (104, 'RF [val: 1.0 max_df:0.9 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (105, 'RF [val: 1.0 max_df:0.9 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (106, 'RF [val: 1.0 max_df:0.9 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (107, 'RF [val: 1.0 max_df:0.9 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (108, 'RF [val: 1.0 max_df:0.9 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (109, 'RF [val: 1.0 max_df:0.9 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.9, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (110, 'RF [val: 1.0 max_df:1.0 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (111, 'RF [val: 1.0 max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (112, 'RF [val: 1.0 max_df:1.0 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (113, 'RF [val: 1.0 max_df:1.0 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (114, 'RF [val: 1.0 max_df:1.0 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (115, 'RF [val: 1.0 max_df:1.0 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (116, 'RF [val: 1.0 max_df:1.0 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (117, 'RF [val: 1.0 max_df:1.0 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (118, 'RF [val: 1.0 max_df:1.0 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (119, 'RF [val: 1.0 max_df:1.0 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=1.0, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=2, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (120, 'RF [val: 1.0 max_df:0.5 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (121, 'RF [val: 1.0 max_df:0.5 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (122, 'RF [val: 1.0 max_df:0.5 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (123, 'RF [val: 1.0 max_df:0.5 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (124, 'RF [val: 1.0 max_df:0.5 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (125, 'RF [val: 1.0 max_df:0.5 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (126, 'RF [val: 1.0 max_df:0.5 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (127, 'RF [val: 1.0 max_df:0.5 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (128, 'RF [val: 1.0 max_df:0.5 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (129, 'RF [val: 1.0 max_df:0.5 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.5, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (130, 'RF [val: 1.0 max_df:0.6 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (131, 'RF [val: 1.0 max_df:0.6 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (132, 'RF [val: 1.0 max_df:0.6 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (133, 'RF [val: 1.0 max_df:0.6 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (134, 'RF [val: 1.0 max_df:0.6 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (135, 'RF [val: 1.0 max_df:0.6 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (136, 'RF [val: 1.0 max_df:0.6 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (137, 'RF [val: 1.0 max_df:0.6 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (138, 'RF [val: 1.0 max_df:0.6 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (139, 'RF [val: 1.0 max_df:0.6 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.6, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (140, 'RF [val: 1.0 max_df:0.7 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (141, 'RF [val: 1.0 max_df:0.7 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (142, 'RF [val: 1.0 max_df:0.7 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (143, 'RF [val: 1.0 max_df:0.7 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (144, 'RF [val: 1.0 max_df:0.7 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (145, 'RF [val: 1.0 max_df:0.7 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (146, 'RF [val: 1.0 max_df:0.7 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (147, 'RF [val: 1.0 max_df:0.7 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (148, 'RF [val: 1.0 max_df:0.7 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (149, 'RF [val: 1.0 max_df:0.7 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.7, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (150, 'RF [val: 1.0 max_df:0.8 min_df: 1]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (151, 'RF [val: 1.0 max_df:0.8 min_df: 2]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (152, 'RF [val: 1.0 max_df:0.8 min_df: 3]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=3, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (153, 'RF [val: 1.0 max_df:0.8 min_df: 4]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=4, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (154, 'RF [val: 1.0 max_df:0.8 min_df: 5]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=5, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (155, 'RF [val: 1.0 max_df:0.8 min_df: 6]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=6, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (156, 'RF [val: 1.0 max_df:0.8 min_df: 7]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=7, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (157, 'RF [val: 1.0 max_df:0.8 min_df: 8]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=8, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (158, 'RF [val: 1.0 max_df:0.8 min_df: 9]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=9, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_patt...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " (159, 'RF [val: 1.0 max_df:0.8 min_df: 10]', Pipeline(memory=None,\n",
       "           steps=[('vectorizer',\n",
       "                   TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                   decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, max_df=0.8, max_features=None,\n",
       "                                   min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, smooth_idf=True,\n",
       "                                   stop_words=None, strip_accents=None,\n",
       "                                   sublinear_tf=False,\n",
       "                                   token_pat...\n",
       "                  ('classifier',\n",
       "                   RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=None,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1, min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=3, n_jobs=None,\n",
       "                                          oob_score=False, random_state=10,\n",
       "                                          verbose=0, warm_start=False))],\n",
       "           verbose=False)),\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "yga = [ 0,1,2,3,4,5,6,7,8,9]\n",
    "print(yga[:3])\n",
    "print(yga[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "697"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweetsPP[:697])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 'LinearSVC [max_df:1.0 min_df: 2]', Pipeline(memory=None,\n",
       "      steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tr...ax_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0))]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=6,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tr...ty='l2', power_t=0.5, random_state=10, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestTrial = allTrials[55]\n",
    "bestTrial[2].fit(tweetsPP[:697], topics[:697])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('service', 103)\n",
      "('personal', 72)\n",
      "('product', 58)\n",
      "('information', 37)\n",
      "('clinical', 16)\n",
      "('sale', 14)\n"
     ]
    }
   ],
   "source": [
    "y_pred = bestTrial[2].predict(tweetsPP[697:])\n",
    "for a in Counter(y_pred).most_common():\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('service', 83)\n",
      "('service clinical', 50)\n",
      "('personal', 44)\n",
      "('clinical', 39)\n",
      "('product', 31)\n",
      "('information', 31)\n",
      "('sale', 22)\n"
     ]
    }
   ],
   "source": [
    "y_true = topics[697:]\n",
    "for a in Counter(y_true).most_common():\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Op√ß√µes:\n",
    "\n",
    "- remover de vez as categorias\n",
    "- buscar no total de tweets, mensagens que sejam classificadas nessas categorias\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executando classificador em todos os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(293306,)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all tweets\n",
    "cmd = (\"select count(*) \"\n",
    "       \"from amostratweet at \"\n",
    "       \"inner join tweet t on at.codtweet = t.codtweet \"\n",
    "       \"where codamostra = 219\")\n",
    "query(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(299987,)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all tweets\n",
    "cmd = (\"select count(*) \"\n",
    "       \"from amostratweet at \"\n",
    "       \"inner join tweet t on at.codtweet = t.codtweet \"\n",
    "       \"where codamostra = 219\")\n",
    "query(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = (\"select t.codtweet, t.text, 0 \"\n",
    "       \"from amostratweet at \"\n",
    "       \"inner join tweet t on at.codtweet = t.codtweet \"\n",
    "       \"where codamostra = 219\")\n",
    "clTweets = query(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(77431828,\n",
       "  'Immediate dental implant - Can you really do that? is a new blog on our website. It explains the procedure in... https://t.co/aD8ZWwryEc',\n",
       "  0),\n",
       " (77431881, 'dental implants', 0),\n",
       " (77431997,\n",
       "  'RT @TomMeyerDDS: Dental implants - what they are and why you may want to consider this procedure. https://t.co/66WhABA2s5 #DesPlaines #dent‚Ä¶',\n",
       "  0),\n",
       " (77432011, 'The Versatility Of Dental Implants https://t.co/pXYknPe8M3', 0),\n",
       " (77432169,\n",
       "  'Are you considering dental Implants, and have questions about the procedure?  Well this informative video may... https://t.co/woRCdat9UK',\n",
       "  0),\n",
       " (77432237,\n",
       "  'Dental Implants or Dentures &amp; Partials: Which Is Right for You? https://t.co/iWf6ZjpyWW',\n",
       "  0),\n",
       " (77432239,\n",
       "  'Dental Implants or Dentures &amp; Partials: Which Is Right for You? https://t.co/iWf6ZjpyWW https://t.co/4vK2HCMuMJ',\n",
       "  0),\n",
       " (77432412,\n",
       "  \"Don't forget to register for the next Dental Implant Seminar. July 13 in East Aurora; July 14 in Amherst.... https://t.co/4N4qeo9ygw\",\n",
       "  0),\n",
       " (77432524,\n",
       "  'Dental implants are the long term solution to tooth loss ‚Äì for one tooth or all teeth! #Tonbridge',\n",
       "  0),\n",
       " (77432579,\n",
       "  'Dental Implants - Considering a dental implant? Things you should know! - Dr. Gary Coatroom\\nhttps://t.co/GtZ0302yn3',\n",
       "  0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clTweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTweets = []\n",
    "for t in clTweets:\n",
    "    allTweets.append(t[1])\n",
    "allTweetsPP = pre_proc(allTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in allTweetsPP] \n",
    "allTweetsClean = []\n",
    "for t in doc_clean:\n",
    "    allTweetsClean.append(\" \".join([i for i in t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299987\n",
      "299987\n",
      "299987\n",
      "299987\n",
      "299987\n",
      "[77431828]\n",
      "[Immediate dental implant - Can you really do that? is a new blog on our website. It explains the procedure in... https://t.co/aD8ZWwryEc]\n",
      "[Immediate dental implant - Can you really do that? is a new blog on our website. It explains the procedure in... https://t.co/aD8ZWwryEc]\n",
      "[immediate dental implant can you really do that questionmark is a new blog on our website it explains the procedure in urlurl ]\n",
      "[['immedi', 'realli', 'questionmark', 'blog', 'websit', 'explain', 'procedur', 'urlurl']]\n",
      "[immedi realli questionmark blog websit explain procedur urlurl]\n",
      "---\n",
      "\n",
      "[77431881]\n",
      "[dental implants]\n",
      "[dental implants]\n",
      "[dental implants]\n",
      "[[]]\n",
      "[]\n",
      "---\n",
      "\n",
      "[77431997]\n",
      "[RT @TomMeyerDDS: Dental implants - what they are and why you may want to consider this procedure. https://t.co/66WhABA2s5 #DesPlaines #dent‚Ä¶]\n",
      "[RT @TomMeyerDDS: Dental implants - what they are and why you may want to consider this procedure. https://t.co/66WhABA2s5 #DesPlaines #dent‚Ä¶]\n",
      "[rt mentionname dental implants what they are and why you may want to consider this procedure urlurl ]\n",
      "[['mentionnam', 'want', 'consid', 'procedur', 'urlurl']]\n",
      "[mentionnam want consid procedur urlurl]\n",
      "---\n",
      "\n",
      "[77432011]\n",
      "[The Versatility Of Dental Implants https://t.co/pXYknPe8M3]\n",
      "[The Versatility Of Dental Implants https://t.co/pXYknPe8M3]\n",
      "[the versatility of dental implants urlurl ]\n",
      "[['versatil', 'urlurl']]\n",
      "[versatil urlurl]\n",
      "---\n",
      "\n",
      "[77432169]\n",
      "[Are you considering dental Implants, and have questions about the procedure?  Well this informative video may... https://t.co/woRCdat9UK]\n",
      "[Are you considering dental Implants, and have questions about the procedure?  Well this informative video may... https://t.co/woRCdat9UK]\n",
      "[are you considering dental implants and have questions about the procedure questionmark well this informative video may urlurl ]\n",
      "[['consid', 'question', 'procedur', 'questionmark', 'well', 'inform', 'video', 'urlurl']]\n",
      "[consid question procedur questionmark well inform video urlurl]\n",
      "---\n",
      "\n",
      "[77432237]\n",
      "[Dental Implants or Dentures &amp; Partials: Which Is Right for You? https://t.co/iWf6ZjpyWW]\n",
      "[Dental Implants or Dentures &amp; Partials: Which Is Right for You? https://t.co/iWf6ZjpyWW]\n",
      "[dental implants or dentures partials which is right for you questionmark urlurl ]\n",
      "[['dentur', 'partial', 'right', 'questionmark', 'urlurl']]\n",
      "[dentur partial right questionmark urlurl]\n",
      "---\n",
      "\n",
      "[77432239]\n",
      "[Dental Implants or Dentures &amp; Partials: Which Is Right for You? https://t.co/iWf6ZjpyWW https://t.co/4vK2HCMuMJ]\n",
      "[Dental Implants or Dentures &amp; Partials: Which Is Right for You? https://t.co/iWf6ZjpyWW https://t.co/4vK2HCMuMJ]\n",
      "[dental implants or dentures partials which is right for you questionmark urlurl ]\n",
      "[['dentur', 'partial', 'right', 'questionmark', 'urlurl']]\n",
      "[dentur partial right questionmark urlurl]\n",
      "---\n",
      "\n",
      "[77432412]\n",
      "[Don't forget to register for the next Dental Implant Seminar. July 13 in East Aurora; July 14 in Amherst.... https://t.co/4N4qeo9ygw]\n",
      "[Don't forget to register for the next Dental Implant Seminar. July 13 in East Aurora; July 14 in Amherst.... https://t.co/4N4qeo9ygw]\n",
      "[don t forget to register for the next dental implant seminar july numbers in east aurora july numbers in amherst urlurl ]\n",
      "[['forget', 'regist', 'next', 'seminar', 'juli', 'number', 'east', 'aurora', 'juli', 'number', 'amherst', 'urlurl']]\n",
      "[forget regist next seminar juli number east aurora juli number amherst urlurl]\n",
      "---\n",
      "\n",
      "[77432524]\n",
      "[Dental implants are the long term solution to tooth loss ‚Äì for one tooth or all teeth! #Tonbridge]\n",
      "[Dental implants are the long term solution to tooth loss ‚Äì for one tooth or all teeth! #Tonbridge]\n",
      "[dental implants are the long term solution to tooth loss for one tooth or all teeth tonbridge]\n",
      "[['long', 'term', 'solut', 'tooth', 'loss', 'tooth', 'teeth', 'tonbridg']]\n",
      "[long term solut tooth loss tooth teeth tonbridg]\n",
      "---\n",
      "\n",
      "[77432579]\n",
      "[Dental Implants - Considering a dental implant? Things you should know! - Dr. Gary Coatroom\n",
      "https://t.co/GtZ0302yn3]\n",
      "[Dental Implants - Considering a dental implant? Things you should know! - Dr. Gary Coatroom\n",
      "https://t.co/GtZ0302yn3]\n",
      "[dental implants considering a dental implant questionmark things you should know dr gary coatroom urlurl ]\n",
      "[['consid', 'questionmark', 'thing', 'know', 'gari', 'coatroom', 'urlurl']]\n",
      "[consid questionmark thing know gari coatroom urlurl]\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(clTweets))\n",
    "print(len(allTweets))\n",
    "print(len(allTweetsPP))\n",
    "print(len(doc_clean))\n",
    "print(len(allTweetsClean))\n",
    "\n",
    "\n",
    "for cl, t, p, dc, tc in zip(clTweets[:10], allTweets[:10], allTweetsPP[:10], doc_clean[:100], allTweetsClean[:100]):\n",
    "    print(\"[%s]\" % cl[0])\n",
    "    print(\"[%s]\" % cl[1])\n",
    "    print(\"[%s]\" % t)\n",
    "    print(\"[%s]\" % p)\n",
    "    print(\"[%s]\" % dc)\n",
    "    print(\"[%s]\" % tc)\n",
    "    print('---\\n')\n",
    "#print(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin motor reduct number number push contra angl handpiec urlurl',\n",
       " 'urlurl',\n",
       " 'lose teeth creat potenti oral health declin quick consid urlurl tcmi urlurl',\n",
       " 'pinterest ever wonder look like questionmark urlurl',\n",
       " 'prefer technolog first compani number print solid custom urlurl',\n",
       " 'mentionnam know posit featur medaka doctor urlurl',\n",
       " 'mentionnam orang counti dentist provid patient qualiti general cosmet dentistri invisalig',\n",
       " 'enhanc best smile post cnipo urlurl',\n",
       " 'tooth loss happen smile hold back urlurl',\n",
       " 'market project wit signific growth number radiant insight urlurl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsClean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dental Implant machine w motor w reduction 16:1 push contra angle handpiece SET https://t.co/afHj5VEuga https://t.co/ZJOWzvs3dl',\n",
       " 'RT https://t.co/rvhOgYkKw2 Dental Implants are More Efficient than Traditional Dentures and Bridges. Explore details on https://t.co/PXAAhAgdDc #teeth #hornsbydentist #dentisthornsby #Oral‚Ä¶ https://t.co/4GAfBK8nTi',\n",
       " 'Losing #teeth creates the potential for your oral #health to decline quickly‚Ä¶\\n\\nConsider a #dental implants: https://t.co/FCtOAjdmJP\\n\\n#tcmi https://t.co/vUapu9seeq',\n",
       " \"We're on Pinterest!! \\n\\nDid you ever wonder what a dental implant looks like? ...or how Inv... https://t.co/LBqQ6fKLYA https://t.co/9Y8CQpQ538\",\n",
       " 'Preferred Dental Technologies is the First Dental Implant Company to 3D Print Solid Custom ... https://t.co/Um9otgSCSS',\n",
       " 'RT @lintenaju7: Know More about Dental Implants and its Positive Features ‚Äì Medaka Doctor https://t.co/hQhYkBsIQs',\n",
       " 'RT @olivesanders15: Orange County Dentist  provides patients with quality general and cosmetic dentistry such as dental implants, Invisalig‚Ä¶',\n",
       " 'How to Enhance Your Best Smile With Dental Implants,see this post on Cnipo https://t.co/LmcQMV1xLr',\n",
       " \"Tooth loss can happen at any age! Don't let your smile hold you back, ask us about dental implants! https://t.co/1VuY6HTUTt\",\n",
       " 'Dental Implant Market Is Projected To Witness Significant Growth By 2021 | Radiant Insights,Inc https://t.co/LdrY0VCVqB https://t.co/eyiMLaDGtR']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 'LinearSVC [max_df:0.9 min_df: 1]', Pipeline(memory=None,\n",
       "          steps=[('vectorizer',\n",
       "                  TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.float64'>,\n",
       "                                  encoding='utf-8', input='content',\n",
       "                                  lowercase=True, max_df=0.9, max_features=None,\n",
       "                                  min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                  preprocessor=None, smooth_idf=True,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  sublinear_tf=False,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, use_idf=True,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                            fit_intercept=True, intercept_scaling=1,\n",
       "                            loss='squared_hinge', max_iter=1000,\n",
       "                            multi_class='ovr', penalty='l2', random_state=None,\n",
       "                            tol=0.0001, verbose=0))],\n",
       "          verbose=False))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrials[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.9, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestTrial = allTrials[100]\n",
    "bestTrial[2].fit(tweets, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('service', 154619)\n",
      "('clinical', 60886)\n",
      "('personal', 34535)\n",
      "('product', 22278)\n",
      "('information', 18290)\n",
      "('sale', 9379)\n"
     ]
    }
   ],
   "source": [
    "y_pred = bestTrial[2].predict(allTweets)\n",
    "for a in Counter(y_pred).most_common():\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service ; 154619\n",
      "clinical ; 60886\n",
      "personal ; 34535\n",
      "product ; 22278\n",
      "information ; 18290\n",
      "sale ; 9379\n"
     ]
    }
   ],
   "source": [
    "for a in Counter(y_pred).most_common():\n",
    "    print(a[0],';',a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299987"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl, allt, yp in zip(clTweets, allTweetsClean, y_pred):\n",
    "    #print(\"[%s]\" % cl[0])\n",
    "    #print(\"[%s]\" % cl[1])\n",
    "    #print(\"[%s]\" % allt)\n",
    "    #print(\"[%s]\" % yp)\n",
    "    \n",
    "    codcl = 39\n",
    "    if yp == 'information': codcl = 223\n",
    "    if yp == 'product': codcl = 224\n",
    "    if yp == 'service': codcl = 226\n",
    "    if yp == 'personal': codcl = 227\n",
    "    if yp == 'sale': codcl = 228\n",
    "    if yp == 'clinical': codcl = 229\n",
    "    if yp == 'service clinical' : codcl = 230\n",
    "    \n",
    "    #if codcl != 39: continue\n",
    "    #print(\"[%s]\" % dc)\n",
    "    #print(\"[%s]\" % tc)\n",
    "    cmd = (\"INSERT INTO resultadoanaliseprojetotweet \"\n",
    "           \"(codresultadoanaliseprojeto, codclassificacao, codtweet) VALUES \" \n",
    "           \"(20, \" + str(codcl) + \" , \" + str(cl[0]) + \")\");\n",
    "    #print(cmd)\n",
    "    execSQL(cmd)\n",
    "    \n",
    "    #print('---\\n')\n",
    "#print(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 19, 45, 'filtro 1'),\n",
       " (3, 19, 47, 'filtro 1 '),\n",
       " (4, 2, 20, 'india filtro 2'),\n",
       " (5, 3, 20, 'uk filtro 2'),\n",
       " (6, 4, 20, 'usa filtro 2'),\n",
       " (1, 1, 40, 'Filtro 1'),\n",
       " (7, 1, 40, 'Filtro 6'),\n",
       " (8, 26, 78, None),\n",
       " (9, 26, 79, None),\n",
       " (19, 28, 124, 'ML Classification 2019 update'),\n",
       " (11, 31, 116, 'Original concilia√ß√£o Debora / Dayane / Geraldine'),\n",
       " (10, 31, 108, 'Revis√£o Geraldine p√≥s concilia√ß√£o'),\n",
       " (12, 28, 124, None),\n",
       " (14, 28, 124, 'LDA: Service 30 topics'),\n",
       " (13, 28, 124, 'LDA: Information 30 topics'),\n",
       " (15, 28, 124, 'LDA: Clinical'),\n",
       " (17, 28, 124, 'LDA: Product'),\n",
       " (18, 28, 124, 'LDA: Sale'),\n",
       " (16, 28, 124, 'LDA: Personal'),\n",
       " (20, None, None, 'Dental Health December 2020')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from resultadoanaliseprojeto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical ; 2016-07 ; 1265\n",
      "clinical ; 2016-08 ; 2055\n",
      "clinical ; 2016-09 ; 1625\n",
      "clinical ; 2016-10 ; 2834\n",
      "clinical ; 2016-11 ; 1351\n",
      "clinical ; 2016-12 ; 1710\n",
      "clinical ; 2017-01 ; 4012\n",
      "clinical ; 2017-02 ; 2315\n",
      "clinical ; 2017-03 ; 1831\n",
      "clinical ; 2017-04 ; 1353\n",
      "clinical ; 2017-05 ; 1543\n",
      "clinical ; 2017-06 ; 1816\n",
      "clinical ; 2017-07 ; 1887\n",
      "clinical ; 2017-08 ; 1564\n",
      "clinical ; 2017-09 ; 1660\n",
      "clinical ; 2017-10 ; 1772\n",
      "clinical ; 2017-11 ; 1534\n",
      "clinical ; 2017-12 ; 1702\n",
      "clinical ; 2018-01 ; 1544\n",
      "clinical ; 2018-02 ; 1443\n",
      "clinical ; 2018-03 ; 1310\n",
      "clinical ; 2018-04 ; 1010\n",
      "clinical ; 2018-05 ; 210\n",
      "clinical ; 2018-07 ; 257\n",
      "clinical ; 2018-08 ; 638\n",
      "clinical ; 2018-09 ; 1383\n",
      "clinical ; 2018-10 ; 1731\n",
      "clinical ; 2018-11 ; 1110\n",
      "clinical ; 2018-12 ; 1241\n",
      "clinical ; 2019-01 ; 1964\n",
      "clinical ; 2019-02 ; 3011\n",
      "clinical ; 2019-03 ; 1799\n",
      "clinical ; 2019-04 ; 1398\n",
      "clinical ; 2019-05 ; 1252\n",
      "clinical ; 2019-06 ; 1260\n",
      "clinical ; 2019-07 ; 1313\n",
      "clinical ; 2019-08 ; 860\n",
      "clinical ; 2019-09 ; 934\n",
      "clinical ; 2019-10 ; 647\n",
      "clinical ; 2019-11 ; 719\n",
      "clinical ; 2019-12 ; 23\n",
      "information ; 2016-07 ; 174\n",
      "information ; 2016-08 ; 441\n",
      "information ; 2016-09 ; 582\n",
      "information ; 2016-10 ; 338\n",
      "information ; 2016-11 ; 810\n",
      "information ; 2016-12 ; 842\n",
      "information ; 2017-01 ; 1035\n",
      "information ; 2017-02 ; 438\n",
      "information ; 2017-03 ; 636\n",
      "information ; 2017-04 ; 458\n",
      "information ; 2017-05 ; 400\n",
      "information ; 2017-06 ; 644\n",
      "information ; 2017-07 ; 453\n",
      "information ; 2017-08 ; 403\n",
      "information ; 2017-09 ; 1149\n",
      "information ; 2017-10 ; 269\n",
      "information ; 2017-11 ; 1382\n",
      "information ; 2017-12 ; 279\n",
      "information ; 2018-01 ; 510\n",
      "information ; 2018-02 ; 261\n",
      "information ; 2018-03 ; 857\n",
      "information ; 2018-04 ; 728\n",
      "information ; 2018-05 ; 195\n",
      "information ; 2018-07 ; 137\n",
      "information ; 2018-08 ; 83\n",
      "information ; 2018-09 ; 332\n",
      "information ; 2018-10 ; 1137\n",
      "information ; 2018-11 ; 343\n",
      "information ; 2018-12 ; 233\n",
      "information ; 2019-01 ; 304\n",
      "information ; 2019-02 ; 440\n",
      "information ; 2019-03 ; 360\n",
      "information ; 2019-04 ; 377\n",
      "information ; 2019-05 ; 149\n",
      "information ; 2019-06 ; 248\n",
      "information ; 2019-07 ; 214\n",
      "information ; 2019-08 ; 155\n",
      "information ; 2019-09 ; 168\n",
      "information ; 2019-10 ; 172\n",
      "information ; 2019-11 ; 141\n",
      "information ; 2019-12 ; 13\n",
      "personal ; 2016-07 ; 332\n",
      "personal ; 2016-08 ; 1199\n",
      "personal ; 2016-09 ; 589\n",
      "personal ; 2016-10 ; 595\n",
      "personal ; 2016-11 ; 468\n",
      "personal ; 2016-12 ; 2260\n",
      "personal ; 2017-01 ; 2095\n",
      "personal ; 2017-02 ; 988\n",
      "personal ; 2017-03 ; 1068\n",
      "personal ; 2017-04 ; 603\n",
      "personal ; 2017-05 ; 670\n",
      "personal ; 2017-06 ; 519\n",
      "personal ; 2017-07 ; 563\n",
      "personal ; 2017-08 ; 396\n",
      "personal ; 2017-09 ; 709\n",
      "personal ; 2017-10 ; 740\n",
      "personal ; 2017-11 ; 862\n",
      "personal ; 2017-12 ; 1724\n",
      "personal ; 2018-01 ; 686\n",
      "personal ; 2018-02 ; 488\n",
      "personal ; 2018-03 ; 662\n",
      "personal ; 2018-04 ; 814\n",
      "personal ; 2018-05 ; 136\n",
      "personal ; 2018-07 ; 188\n",
      "personal ; 2018-08 ; 339\n",
      "personal ; 2018-09 ; 762\n",
      "personal ; 2018-10 ; 799\n",
      "personal ; 2018-11 ; 809\n",
      "personal ; 2018-12 ; 774\n",
      "personal ; 2019-01 ; 1028\n",
      "personal ; 2019-02 ; 1619\n",
      "personal ; 2019-03 ; 1481\n",
      "personal ; 2019-04 ; 923\n",
      "personal ; 2019-05 ; 914\n",
      "personal ; 2019-06 ; 921\n",
      "personal ; 2019-07 ; 892\n",
      "personal ; 2019-08 ; 759\n",
      "personal ; 2019-09 ; 1007\n",
      "personal ; 2019-10 ; 1265\n",
      "personal ; 2019-11 ; 812\n",
      "personal ; 2019-12 ; 77\n",
      "product ; 2016-07 ; 439\n",
      "product ; 2016-08 ; 653\n",
      "product ; 2016-09 ; 619\n",
      "product ; 2016-10 ; 773\n",
      "product ; 2016-11 ; 610\n",
      "product ; 2016-12 ; 1488\n",
      "product ; 2017-01 ; 1370\n",
      "product ; 2017-02 ; 471\n",
      "product ; 2017-03 ; 808\n",
      "product ; 2017-04 ; 458\n",
      "product ; 2017-05 ; 922\n",
      "product ; 2017-06 ; 1102\n",
      "product ; 2017-07 ; 886\n",
      "product ; 2017-08 ; 672\n",
      "product ; 2017-09 ; 800\n",
      "product ; 2017-10 ; 1133\n",
      "product ; 2017-11 ; 1150\n",
      "product ; 2017-12 ; 865\n",
      "product ; 2018-01 ; 911\n",
      "product ; 2018-02 ; 1064\n",
      "product ; 2018-03 ; 898\n",
      "product ; 2018-04 ; 492\n",
      "product ; 2018-05 ; 95\n",
      "product ; 2018-07 ; 71\n",
      "product ; 2018-08 ; 67\n",
      "product ; 2018-09 ; 188\n",
      "product ; 2018-10 ; 243\n",
      "product ; 2018-11 ; 170\n",
      "product ; 2018-12 ; 571\n",
      "product ; 2019-01 ; 190\n",
      "product ; 2019-02 ; 251\n",
      "product ; 2019-03 ; 126\n",
      "product ; 2019-04 ; 260\n",
      "product ; 2019-05 ; 190\n",
      "product ; 2019-06 ; 170\n",
      "product ; 2019-07 ; 167\n",
      "product ; 2019-08 ; 288\n",
      "product ; 2019-09 ; 207\n",
      "product ; 2019-10 ; 257\n",
      "product ; 2019-11 ; 177\n",
      "product ; 2019-12 ; 6\n",
      "sale ; 2016-07 ; 763\n",
      "sale ; 2016-08 ; 286\n",
      "sale ; 2016-09 ; 361\n",
      "sale ; 2016-10 ; 374\n",
      "sale ; 2016-11 ; 543\n",
      "sale ; 2016-12 ; 1132\n",
      "sale ; 2017-01 ; 618\n",
      "sale ; 2017-02 ; 377\n",
      "sale ; 2017-03 ; 364\n",
      "sale ; 2017-04 ; 126\n",
      "sale ; 2017-05 ; 169\n",
      "sale ; 2017-06 ; 265\n",
      "sale ; 2017-07 ; 224\n",
      "sale ; 2017-08 ; 178\n",
      "sale ; 2017-09 ; 260\n",
      "sale ; 2017-10 ; 268\n",
      "sale ; 2017-11 ; 405\n",
      "sale ; 2017-12 ; 147\n",
      "sale ; 2018-01 ; 89\n",
      "sale ; 2018-02 ; 64\n",
      "sale ; 2018-03 ; 53\n",
      "sale ; 2018-04 ; 35\n",
      "sale ; 2018-05 ; 5\n",
      "sale ; 2018-07 ; 48\n",
      "sale ; 2018-08 ; 75\n",
      "sale ; 2018-09 ; 130\n",
      "sale ; 2018-10 ; 135\n",
      "sale ; 2018-11 ; 107\n",
      "sale ; 2018-12 ; 415\n",
      "sale ; 2019-01 ; 186\n",
      "sale ; 2019-02 ; 114\n",
      "sale ; 2019-03 ; 55\n",
      "sale ; 2019-04 ; 136\n",
      "sale ; 2019-05 ; 100\n",
      "sale ; 2019-06 ; 127\n",
      "sale ; 2019-07 ; 189\n",
      "sale ; 2019-08 ; 190\n",
      "sale ; 2019-09 ; 104\n",
      "sale ; 2019-10 ; 88\n",
      "sale ; 2019-11 ; 71\n",
      "sale ; 2019-12 ; 3\n",
      "service ; 2016-07 ; 3493\n",
      "service ; 2016-08 ; 4900\n",
      "service ; 2016-09 ; 4079\n",
      "service ; 2016-10 ; 4245\n",
      "service ; 2016-11 ; 4635\n",
      "service ; 2016-12 ; 7284\n",
      "service ; 2017-01 ; 5957\n",
      "service ; 2017-02 ; 4044\n",
      "service ; 2017-03 ; 6736\n",
      "service ; 2017-04 ; 5032\n",
      "service ; 2017-05 ; 4888\n",
      "service ; 2017-06 ; 4744\n",
      "service ; 2017-07 ; 4505\n",
      "service ; 2017-08 ; 4559\n",
      "service ; 2017-09 ; 4045\n",
      "service ; 2017-10 ; 5415\n",
      "service ; 2017-11 ; 4238\n",
      "service ; 2017-12 ; 3972\n",
      "service ; 2018-01 ; 3427\n",
      "service ; 2018-02 ; 3606\n",
      "service ; 2018-03 ; 3690\n",
      "service ; 2018-04 ; 3392\n",
      "service ; 2018-05 ; 1091\n",
      "service ; 2018-07 ; 976\n",
      "service ; 2018-08 ; 1961\n",
      "service ; 2018-09 ; 3550\n",
      "service ; 2018-10 ; 4277\n",
      "service ; 2018-11 ; 3693\n",
      "service ; 2018-12 ; 3921\n",
      "service ; 2019-01 ; 4634\n",
      "service ; 2019-02 ; 4906\n",
      "service ; 2019-03 ; 2386\n",
      "service ; 2019-04 ; 3214\n",
      "service ; 2019-05 ; 3265\n",
      "service ; 2019-06 ; 4141\n",
      "service ; 2019-07 ; 2971\n",
      "service ; 2019-08 ; 2273\n",
      "service ; 2019-09 ; 2426\n",
      "service ; 2019-10 ; 1793\n",
      "service ; 2019-11 ; 2162\n",
      "service ; 2019-12 ; 93\n"
     ]
    }
   ],
   "source": [
    "cmd = (\"SELECT \"\n",
    "       \"cl.descricao, \"\n",
    "       \"to_char(datatweet, 'YYYY-MM') as data, \"\n",
    "       \"count(r.codtweet) \"\n",
    "       \"from resultadoanaliseprojetotweet r \"\n",
    "       \"inner join classificacao cl on cl.codclassificacao = r.codclassificacao \"\n",
    "       \"INNER JOIN tweet t ON r.codtweet = t.codtweet \"\n",
    "       \"where codresultadoanaliseprojeto = 20 \"\n",
    "       \"GROUP BY 1, 2 \"\n",
    "       \"ORDER BY 1, 2\"\n",
    "      )\n",
    "res = query(cmd)\n",
    "for r in res:\n",
    "    print(r[0],\";\",r[1],\";\",r[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
